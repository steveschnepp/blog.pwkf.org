///DOTCLEAR|2.1.5|single

[category cat_id,blog_id,cat_title,cat_url,cat_desc,cat_position,cat_lft,cat_rgt]
"38366","d0cea1ba0b96cc1f1d745b224ff8099a","programming","programming","","0","12","19"
"38368","d0cea1ba0b96cc1f1d745b224ff8099a","munin","sysadmin/munin","","0","3","4"
"33233","d0cea1ba0b96cc1f1d745b224ff8099a","database","sysadmin/database","","1","5","6"
"12386","d0cea1ba0b96cc1f1d745b224ff8099a","java","programming/java","<p>Some random thoughts collected meanwhile working on an\n<em>Enterprise-grade</em> J2EE stack.</p>","2","13","14"
"6848","d0cea1ba0b96cc1f1d745b224ff8099a","general","general","<p>Articles about eclectic things that don't fit in more specific\ncategories.</p>","5","10","11"
"49436","d0cea1ba0b96cc1f1d745b224ff8099a","perl","programming/perl","","0","17","18"
"38367","d0cea1ba0b96cc1f1d745b224ff8099a","c++","programming/c-plus-plus","","0","15","16"
"6847","d0cea1ba0b96cc1f1d745b224ff8099a","pwkf","pwkf","","4","8","9"
"28205","d0cea1ba0b96cc1f1d745b224ff8099a","sysadmin","sysadmin","","3","2","7"

[link link_id,blog_id,link_href,link_title,link_desc,link_lang,link_xfn,link_position]
"52061","d0cea1ba0b96cc1f1d745b224ff8099a","http://glooby-wedding.blogspot.com/","Glooby's Wedding Blog","Our Wedding's Blog","en","me","0"
"52060","d0cea1ba0b96cc1f1d745b224ff8099a","http://glooby.blogspot.com/","Glooby's Blog","My wife's blog","en","friend met co-resident spouse","0"
"48770","d0cea1ba0b96cc1f1d745b224ff8099a","http://www.courtois.cc/blogeclectique/","Blog Eclectique [fr]","","fr","friend met","0"

[setting setting_id,blog_id,setting_ns,setting_value,setting_type,setting_label]
"unix_id","d0cea1ba0b96cc1f1d745b224ff8099a","system","1125580","string","Setuid to this id when acting as the owner"
"antispam_dnsbls","d0cea1ba0b96cc1f1d745b224ff8099a","antispam","sbl-xbl.spamhaus.org , bsb.spamlookup.net","string","Antispam DNSBL servers"
"antispam_filters","d0cea1ba0b96cc1f1d745b224ff8099a","antispam","a:5:{s:10:\"dcFilterIP\";a:2:{i:0;b:1;i:1;i:0;}s:13:\"dcFilterWords\";a:2:{i:0;b:1;i:1;i:1;}s:16:\"dcFilterIpLookup\";a:2:{i:0;b:1;i:1;i:2;}s:19:\"dcFilterLinksLookup\";a:2:{i:0;b:1;i:1;i:3;}s:15:\"dcFilterAkismet\";a:2:{i:0;b:1;i:1;i:4;}}","string","Antispam Filters"
"lightbox_enabled","d0cea1ba0b96cc1f1d745b224ff8099a","lightbox","","string",""
"theme","d0cea1ba0b96cc1f1d745b224ff8099a","system","chaudeJournee","string","Blog theme"
"antispam_moderation_ttl","d0cea1ba0b96cc1f1d745b224ff8099a","antispam","7","integer","Antispam Moderation TTL (days)"
"wiki_comments","d0cea1ba0b96cc1f1d745b224ff8099a","system","1","boolean","Allow commenters to use a subset of wiki syntax"
"enable_xmlrpc","d0cea1ba0b96cc1f1d745b224ff8099a","system","1","boolean","Enable XML/RPC interface"
"date_format","d0cea1ba0b96cc1f1d745b224ff8099a","system","%A, %e %B %Y","string","Date format. See PHP strftime function for patterns"
"nb_post_per_page","d0cea1ba0b96cc1f1d745b224ff8099a","system","5","integer","Number of entries on home page and category pages"
"editor","d0cea1ba0b96cc1f1d745b224ff8099a","system","Steve Schnepp","string","Person responsible of the content"
"copyright_notice","d0cea1ba0b96cc1f1d745b224ff8099a","system","(c) 2006-2011 - Steve Schnepp","string","Copyright notice (simple text)"
"blog_timezone","d0cea1ba0b96cc1f1d745b224ff8099a","system","Europe/Paris","string","Blog timezone"
"widgets_extra","d0cea1ba0b96cc1f1d745b224ff8099a","widgets","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","string",""
"gd_on_portal","d0cea1ba0b96cc1f1d745b224ff8099a","gandiblog","1","boolean","Blog can be displayed on portal page"
"allow_trackbacks","d0cea1ba0b96cc1f1d745b224ff8099a","system","0","boolean","Allow trackbacks on blog"
"trackbacks_pub","d0cea1ba0b96cc1f1d745b224ff8099a","system","0","boolean","Publish trackbacks immediatly"
"post_url_format","d0cea1ba0b96cc1f1d745b224ff8099a","system","{y}/{m}/{t}","string","Post URL format. {y}: year, {m}: month, {d}: day, {id}: post id, {t}: entry title"
"allow_comments","d0cea1ba0b96cc1f1d745b224ff8099a","system","0","boolean","Allow comments on blog"
"comments_pub","d0cea1ba0b96cc1f1d745b224ff8099a","system","0","boolean","Publish comments immediatly"
"widgets_nav","d0cea1ba0b96cc1f1d745b224ff8099a","widgets","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","string",""
"antispam_date_last_purge","d0cea1ba0b96cc1f1d745b224ff8099a","antispam","1590335492","integer","Antispam Date Last Purge (unix timestamp)"

[post post_id,blog_id,user_id,cat_id,post_dt,post_tz,post_creadt,post_upddt,post_password,post_type,post_format,post_url,post_lang,post_title,post_excerpt,post_excerpt_xhtml,post_content,post_content_xhtml,post_notes,post_words,post_meta,post_status,post_selected,post_open_comment,post_open_tb,nb_comment,nb_trackback,post_position]
"156496","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2007-09-19 06:33:00","Europe/Paris","2007-09-18 06:36:16","2010-08-23 16:45:30","","post","wiki","2007/09/19/Convert-your-Log-Files-into-Gold","en","Convert your Log Files into Gold","","","Log files are a necessary evil on a live system. A few rules can transform your log files from a useless heap of textfiles to a gold mine.\r\n\r\nI'll focus mostly on  [Log4J|http://logging.apache.org/log4j/|en] since it's available on Java, but is ported on many languages also as it sets some kind of logging standard.\r\n\r\nThe logger is configured per class with a : \r\n\r\n@@private static Logger logger = Logger.getLogger(MyClass.class);@@\r\n\r\nThe logger is private since each class should have its logger, especially the derived ones (that way you can very nicely debug the virtual function calls), and static since it's either thread-safe and that way you have it even in the <init> and <cinit> of your class (you did put it in the first line of the class, didn't you ?).\r\n\r\nThis is very useful since you can configure a per-package or a even per-class level of logging. It is very useful since all your classes do only one thing, don't they ?\r\n\r\nI usually use only 5 levels of logging : ERROR, WARN, INFO, DEBUG, TRACE\r\n\r\n* __TRACE__ is used to help with a dump of many internal variables (it's a last resort debug, since usually it's very verbose)\r\n* __DEBUG__ is used to debug this class, with keypoints inside the class, in order to see the inside flow of execution.\r\n* __INFO__ is used to debug other classes with the help of this one. Usually it emits only one line per public call, with the incoming parameters and the result displayed in a synthetised form.\r\n* __WARN__ is used when an exceptional situation happens (usually a catched Exception that is triggered by the caller data) and there is a known path to recover.\r\n* __ERROR__ is used when an exceptional situation happens, but there is no known path to recover. Usually this line is send by email to the administrator. If there is a problem, and an ERROR is logged, there should be no other ERROR logged for this problem : it will help you to keep a high signal/noise ratio.\r\n\r\nIn a live production system, I just log INFO for ''terminal'' business classes (the ones that represent actions), and WARN on technical ones (the one that actions class uses). I configure it to send an email on every ERROR.\r\n\r\nIt's also very important to have a reminder of a synthetised form of the arguments in the INFO, WARN & ERROR log messages (such as an ORDER_ID if it's an ordering action or the PRODUCT_ID if it's a deleting product action). It's also a good thing to put the exception that triggered the WARN/ERROR log.\r\nThat way you can just grep through your logs to see if, when and what happened to that famous product that everyone is so excited about\r\n\r\nAlways use a RollingFileAppender. __Always__. If you're scared about loosing some logs, just put an insane number of backup files since __nothing__ is worse than not enough space on the log filesystem : you won't have the logs anyway. Note that if you have a different kind of rolling mecanismed you can use it, the point is  that you should __never__ leave a growing log file without control.\r\n\r\nSo, here is a exemple of 2 classes (one business, one technical) that present the logging system I talked about : \r\n\r\n///\r\npublic class PublishProductAction extends Action {\r\n\r\nprivate static Logger logger = Logger.getLogger(AddProductAction.class); \r\nprivate int product_id;\r\npublic PublishProductAction(int product_id) { this.product_id = product_id; }\r\npublic void execute() {\r\n	logger.info(\"publishing product[product_id:\" \r\n		+ product_id + \"]\");\r\n	try {\r\n		// Do things...\r\n	} catch (Exception e) {\r\n		logger.error(\"Cannot publish product[\" \r\n			+ product_id + \"] : \", e);\r\n	}\r\n	logger.debug(\"done publishing product[product_id:\" \r\n		+ product_id + \"]\");\r\n}\r\n\r\n}\r\n\r\npublic class MyPreparedStatement {\r\n\r\nprivate static Logger logger = Logger.getLogger(MyPreparedStatement.class);\r\nprivate String sql;\r\npublic MyPreparedStatement(String sql) { this.sql_id = sql; }\r\npublic void execute(Collection params) {\r\n	startTimer();\r\n	try {\r\n	if (sql == null) {\r\n		// Log in warning, since it should not happen, \r\n		// but we can handle it gracefully\r\n		logger.warn(\"The SQL statement is null, we do nothing\");\r\n		return;\r\n	}\r\n		try {\r\n		// Do things...\r\n	} catch (Exception e) {\r\n		// log only in info since we don't catch\r\n		// the Exception.\r\n		logger.info(\"Cannot execute SQL[\" + sql + \"] : \", e);\r\n		thow e;\r\n	}\r\n	} finally {\r\n		stopTimer();\r\n		// help the outside class to see what happened\r\n		if (logger.isInfoEnabled()) {\r\n			logger.info(\"executed sql [\" \r\n				+ parseSQL(sql, params) + \"] in \" \r\n				+ getTime() + \" ms\");\r\n		}\r\n	}\r\n}\r\n\r\n}\r\n///","<p>Log files are a necessary evil on a live system. A few rules can transform\nyour log files from a useless heap of textfiles to a gold mine.</p>\n<p>I'll focus mostly on <a href=\"http://logging.apache.org/log4j/\" hreflang=\"en\">Log4J</a> since it's available on Java, but is ported on many languages\nalso as it sets some kind of logging standard.</p>\n<p>The logger is configured per class with a :</p>\n<p><code>private static Logger logger =\nLogger.getLogger(MyClass.class);</code></p>\n<p>The logger is private since each class should have its logger, especially\nthe derived ones (that way you can very nicely debug the virtual function\ncalls), and static since it's either thread-safe and that way you have it even\nin the &lt;init&gt; and &lt;cinit&gt; of your class (you did put it in the\nfirst line of the class, didn't you ?).</p>\n<p>This is very useful since you can configure a per-package or a even\nper-class level of logging. It is very useful since all your classes do only\none thing, don't they ?</p>\n<p>I usually use only 5 levels of logging : ERROR, WARN, INFO, DEBUG, TRACE</p>\n<ul>\n<li><strong>TRACE</strong> is used to help with a dump of many internal\nvariables (it's a last resort debug, since usually it's very verbose)</li>\n<li><strong>DEBUG</strong> is used to debug this class, with keypoints inside\nthe class, in order to see the inside flow of execution.</li>\n<li><strong>INFO</strong> is used to debug other classes with the help of this\none. Usually it emits only one line per public call, with the incoming\nparameters and the result displayed in a synthetised form.</li>\n<li><strong>WARN</strong> is used when an exceptional situation happens\n(usually a catched Exception that is triggered by the caller data) and there is\na known path to recover.</li>\n<li><strong>ERROR</strong> is used when an exceptional situation happens, but\nthere is no known path to recover. Usually this line is send by email to the\nadministrator. If there is a problem, and an ERROR is logged, there should be\nno other ERROR logged for this problem : it will help you to keep a high\nsignal/noise ratio.</li>\n</ul>\n<p>In a live production system, I just log INFO for <em>terminal</em> business\nclasses (the ones that represent actions), and WARN on technical ones (the one\nthat actions class uses). I configure it to send an email on every ERROR.</p>\n<p>It's also very important to have a reminder of a synthetised form of the\narguments in the INFO, WARN &amp; ERROR log messages (such as an ORDER_ID if\nit's an ordering action or the PRODUCT_ID if it's a deleting product action).\nIt's also a good thing to put the exception that triggered the WARN/ERROR log.\nThat way you can just grep through your logs to see if, when and what happened\nto that famous product that everyone is so excited about</p>\n<p>Always use a RollingFileAppender. <strong>Always</strong>. If you're scared\nabout loosing some logs, just put an insane number of backup files since\n<strong>nothing</strong> is worse than not enough space on the log filesystem :\nyou won't have the logs anyway. Note that if you have a different kind of\nrolling mecanismed you can use it, the point is that you should\n<strong>never</strong> leave a growing log file without control.</p>\n<p>So, here is a exemple of 2 classes (one business, one technical) that\npresent the logging system I talked about :</p>\n<pre>\npublic class PublishProductAction extends Action {\n\nprivate static Logger logger = Logger.getLogger(AddProductAction.class); \nprivate int product_id;\npublic PublishProductAction(int product_id) { this.product_id = product_id; }\npublic void execute() {\n        logger.info(&quot;publishing product[product_id:&quot; \n                + product_id + &quot;]&quot;);\n        try {\n                // Do things...\n        } catch (Exception e) {\n                logger.error(&quot;Cannot publish product[&quot; \n                        + product_id + &quot;] : &quot;, e);\n        }\n        logger.debug(&quot;done publishing product[product_id:&quot; \n                + product_id + &quot;]&quot;);\n}\n\n}\n\npublic class MyPreparedStatement {\n\nprivate static Logger logger = Logger.getLogger(MyPreparedStatement.class);\nprivate String sql;\npublic MyPreparedStatement(String sql) { this.sql_id = sql; }\npublic void execute(Collection params) {\n        startTimer();\n        try {\n        if (sql == null) {\n                // Log in warning, since it should not happen, \n                // but we can handle it gracefully\n                logger.warn(&quot;The SQL statement is null, we do nothing&quot;);\n                return;\n        }\n                try {\n                // Do things...\n        } catch (Exception e) {\n                // log only in info since we don't catch\n                // the Exception.\n                logger.info(&quot;Cannot execute SQL[&quot; + sql + &quot;] : &quot;, e);\n                thow e;\n        }\n        } finally {\n                stopTimer();\n                // help the outside class to see what happened\n                if (logger.isInfoEnabled()) {\n                        logger.info(&quot;executed sql [&quot; \n                                + parseSQL(sql, params) + &quot;] in &quot; \n                                + getTime() + &quot; ms&quot;);\n                }\n        }\n}\n\n}\n</pre>","Just put a commented example class","convert your log files into gold log files are necessary evil live system few rules can transform your log files from useless heap textfiles gold mine focus mostly log4j since available java but ported many languages also sets some kind logging standard the logger configured per class with private static logger logger logger getlogger myclass class the logger private since each class should have its logger especially the derived ones that way you can very nicely debug the virtual function calls and static since either thread safe and that way you have even the init and cinit your class you did put the first line the class didn you this very useful since you can configure per package even per class level logging very useful since all your classes only one thing don they usually use only levels logging error warn info debug trace trace used help with dump many internal variables last resort debug since usually very verbose debug used debug this class with keypoints inside the class order see the inside flow execution info used debug other classes with the help this one usually emits only one line per public call with the incoming parameters and the result displayed synthetised form warn used when exceptional situation happens usually catched exception that triggered the caller data and there known path recover error used when exceptional situation happens but there known path recover usually this line send email the administrator there problem and error logged there should other error logged for this problem will help you keep high signal noise ratio live production system just log info for terminal business classes the ones that represent actions and warn technical ones the one that actions class uses configure send email every error also very important have reminder synthetised form the arguments the info warn amp error log messages such order ordering action the product deleting product action also good thing put the exception that triggered the warn error log that way you can just grep through your logs see when and what happened that famous product that everyone excited about always use rollingfileappender always you scared about loosing some logs just put insane number backup files since nothing worse than not enough space the log filesystem you won have the logs anyway note that you have different kind rolling mecanismed you can use the point that you should never leave growing log file without control here exemple classes one business one technical that present the logging system talked about public class publishproductaction extends action private static logger logger logger getlogger addproductaction class private int product public publishproductaction int product this product product public void execute logger info quot publishing product product quot product quot quot try things catch exception logger error quot cannot publish product quot product quot quot logger debug quot done publishing product product quot product quot quot public class mypreparedstatement private static logger logger logger getlogger mypreparedstatement class private string sql public mypreparedstatement string sql this sql sql public void execute collection params starttimer try sql null log warning since should not happen but can handle gracefully logger warn quot the sql statement null nothing quot return try things catch exception log only info since don catch the exception logger info quot cannot execute sql quot sql quot quot thow finally stoptimer help the outside class see what happened logger isinfoenabled logger info quot executed sql quot parsesql sql params quot quot gettime quot quot","a:1:{s:3:\"tag\";a:1:{i:0;s:3:\"sql\";}}","1","0","1","0","0","0","0"
"161842","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38366","2007-10-11 22:32:00","Europe/Paris","2007-10-09 04:42:24","2009-12-10 09:51:04","","post","wiki","2007/10/11/Keep-your-caches-coherent-:-Scope-them","en","Keep your caches coherent : Scope them !","Everyone knows about [variable scoping|http://en.wikipedia.org/wiki/Variable#Scope_and_extent|en] in a bloc, a function or a class. \r\n\r\nYou can have general scoping in general by just extending to any context. For example, in a web-processing context, you may have request-scoped elements (the one usually put in the attributes of the current @@HttpServletRequest@@).","<p>Everyone knows about <a href=\"http://en.wikipedia.org/wiki/Variable#Scope_and_extent\" hreflang=\"en\">variable\nscoping</a> in a bloc, a function or a class.</p>\n<p>You can have general scoping in general by just extending to any context.\nFor example, in a web-processing context, you may have request-scoped elements\n(the one usually put in the attributes of the current\n<code>HttpServletRequest</code>).</p>","You can mostly have these kind of scoping : \r\n* Sub-routine (typically a function or a statement bloc)\r\n* Request (typically in a @@HttpServletRequest@@)\r\n* Session (typically in a @@HttpSession@@)\r\n* Node (typically in a JVM)\r\n* Application (typically on a specific node)\r\n* Time (typically a filenumber per day / most cached values)\r\n\r\nHere we encounter at last the ''Time'' scope. This one is the hardest to cope with when it come to cached values. Usually a quick and dirty caching is done with a combinaison of a Map that contains Keys linked to a TimedValue. The last part is composed of the value itself and a sort of \"expiration date\" (can also be implemented with a ''manufactured date''). \r\n\r\nThe biggest problem with these time-scoped values are that they are not easily updated to maintain coherency with the value they should be a copy of. It involves that for each update, we manage to find the cached value and either delete it (so that it can be recomputed later) or update it. It's not really acceptable since : \r\n# it is really coding-wise intrusive (you have to seek every place that you update an element of the calculation of the cached value) to put triggers everywhere, and therefore not really practical, not to say sometime not even possible (you don't own the whole code)\r\n# it is really performance-wise disastrous : for every write you do, you have to fire the trigger to update all your caches that impacted from this update.\r\n\r\nSo, usually this triggering is just avoided, and the cache is left in a non-coherent state. And in reality it ends up in a fighting of __speedup __ vs __accuracy__.\r\n\r\nBut this fight sometimes can be avoided if you scope correctly your caches by storing it in an appropriatly scoped variable and not as usually done in a static variable. It will have the same effect as a transaction in a database since it's basically a copy-on-read scheme. Putting it in an appropriatly scoped variable has the net effect of relieving of pruning the cache with data that are not valid anymore without any extra effort.\r\n\r\nThis lays the premice of [STM|http://en.wikipedia.org/wiki/Software_transactional_memory|en] that will be something that will really count in the era of massive-multi-core-computing (MMCC) that we are beginning to enter with processors like [Sun's Niagara T2|http://www.sun.com/processors/UltraSPARC-T2|en] and its 64 thread per CPU.","<p>You can mostly have these kind of scoping :</p>\n<ul>\n<li>Sub-routine (typically a function or a statement bloc)</li>\n<li>Request (typically in a <code>HttpServletRequest</code>)</li>\n<li>Session (typically in a <code>HttpSession</code>)</li>\n<li>Node (typically in a JVM)</li>\n<li>Application (typically on a specific node)</li>\n<li>Time (typically a filenumber per day / most cached values)</li>\n</ul>\n<p>Here we encounter at last the <em>Time</em> scope. This one is the hardest\nto cope with when it come to cached values. Usually a quick and dirty caching\nis done with a combinaison of a Map that contains Keys linked to a TimedValue.\nThe last part is composed of the value itself and a sort of &quot;expiration date&quot;\n(can also be implemented with a <em>manufactured date</em>).</p>\n<p>The biggest problem with these time-scoped values are that they are not\neasily updated to maintain coherency with the value they should be a copy of.\nIt involves that for each update, we manage to find the cached value and either\ndelete it (so that it can be recomputed later) or update it. It's not really\nacceptable since :</p>\n<ol>\n<li>it is really coding-wise intrusive (you have to seek every place that you\nupdate an element of the calculation of the cached value) to put triggers\neverywhere, and therefore not really practical, not to say sometime not even\npossible (you don't own the whole code)</li>\n<li>it is really performance-wise disastrous : for every write you do, you have\nto fire the trigger to update all your caches that impacted from this\nupdate.</li>\n</ol>\n<p>So, usually this triggering is just avoided, and the cache is left in a\nnon-coherent state. And in reality it ends up in a fighting of\n<strong>speedup</strong> vs <strong>accuracy</strong>.</p>\n<p>But this fight sometimes can be avoided if you scope correctly your caches\nby storing it in an appropriatly scoped variable and not as usually done in a\nstatic variable. It will have the same effect as a transaction in a database\nsince it's basically a copy-on-read scheme. Putting it in an appropriatly\nscoped variable has the net effect of relieving of pruning the cache with data\nthat are not valid anymore without any extra effort.</p>\n<p>This lays the premice of <a href=\"http://en.wikipedia.org/wiki/Software_transactional_memory\" hreflang=\"en\">STM</a> that will be something that will really count in the era of\nmassive-multi-core-computing (MMCC) that we are beginning to enter with\nprocessors like <a href=\"http://www.sun.com/processors/UltraSPARC-T2\" hreflang=\"en\">Sun's Niagara T2</a> and its 64 thread per CPU.</p>","","keep your caches coherent scope them everyone knows about variable scoping bloc function class you can have general scoping general just extending any context for example web processing context you may have request scoped elements the one usually put the attributes the current httpservletrequest you can mostly have these kind scoping sub routine typically function statement bloc request typically httpservletrequest session typically httpsession node typically jvm application typically specific node time typically filenumber per day most cached values here encounter last the time scope this one the hardest cope with when come cached values usually quick and dirty caching done with combinaison map that contains keys linked timedvalue the last part composed the value itself and sort quot expiration date quot can also implemented with manufactured date the biggest problem with these time scoped values are that they are not easily updated maintain coherency with the value they should copy involves that for each update manage find the cached value and either delete that can recomputed later update not really acceptable since really coding wise intrusive you have seek every place that you update element the calculation the cached value put triggers everywhere and therefore not really practical not say sometime not even possible you don own the whole code really performance wise disastrous for every write you you have fire the trigger update all your caches that impacted from this update usually this triggering just avoided and the cache left non coherent state and reality ends fighting speedup accuracy but this fight sometimes can avoided you scope correctly your caches storing appropriatly scoped variable and not usually done static variable will have the same effect transaction database since basically copy read scheme putting appropriatly scoped variable has the net effect relieving pruning the cache with data that are not valid anymore without any extra effort this lays the premice stm that will something that will really count the era massive multi core computing mmcc that are beginning enter with processors like sun niagara and its thread per cpu","a:0:{}","1","0","1","1","0","0","0"
"194474","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6847","2008-01-05 14:15:00","Europe/Paris","2008-01-05 13:15:06","2008-03-14 12:46:31","","post","wiki","2008/01/05/A-Little-History-of-PWKF","en","A Little History of PWKF","","","It all began with a scratching need as I just felt that workflows (mostly BPM engines to be more precise) were the [way to do many things in IT of the future|http://blog.pwkf.org/post/2007/09/10/Are-Workflows-the-Future-of-IT-Computing|en]. \r\nThere are even many workflows out here already. My main feeling was that they put too much emphasis on how much of the standard they support, mostly in order to be \"Entreprise Grade\". \r\n\r\nThat felt just plain wrong to me : WF where at first designed (at least that's was the usual original marketing scheme) to put back the design of the business rules in the hands of our beloved users, in order to mostly bypass the IT departement, and be able to change and adapt the process very rapidly as described on [ACM|http://acmqueue.com/modules.php?name=Content&pa=showpage&pid=369|en]. With the current implementations of WF, i just have to feeling that it involves a lot more of XML and/or BPEL files to be written than a non-IT worker can manage. \r\n\r\nI think that WFs are a very good concept, but the emphasis should be on the modeling, on the reporting, and on the operational part (in that order). The number of constructs that the implementation recognise isn't that important : it could support a much smaller ''instruction set'', but doing it in a user-friendly way.\r\n* The modeling is the most important part of the equation. \r\n\r\n	It's always über-fustrating that we have to attend to meetings, capturing the informations on how to model the business rules, tranlate it into something that the systems understand, and to try to explain it to the users. The most important part of the modeling is that the model is accurate enough. Modeling is usually the difficult part of the job. I don't advocate that the business users should be able to model it themselves : it's usually a recipe for a disaster since they mostly don't have the mind for this. But the opposite is also true : the user has to understand easily what is modeled.\r\n	Modeling a workflow is like writing a book : it takes a different skillset to write it and to read it to be able to spot inaccuracies.\r\n	Since users aren't very good at decyphering computer languages, having a good bijection between the modelling and a ''random-human''-readable form is very important. That enables the fact that you users don't have to model their business ''through'' you, but ''with'' you.\r\n	\r\n* Reporting is the second most important part\r\n	The debugging part is also very important. Since everyone makes mistakes, it's very important to be able to spot them. So we have to see what happened to a particular order. Usually that's the most difficult part of the system, but with workflow, it's quite easy to log everything that happened, and be able to show it later, even to show a snapshot of the current state and history of a particular workcase. If you also manage to show it with a graphical form that is the same than the modeling one, you'll have bug reports that would be much more accurate. The users will be able to tell you : {{This workcase has gone through here and there, but since it's the special case A, it should have gone here instead.}} and then bugfixing would be usually like a piece of cake.\r\n	\r\n* Operating the workflow is also important.\r\n	You don't __use__ the workflow for a living. you __design__ them. But that's not a reason to make the life of operators miserable :-). And then they should have a nice list of tasks to be done. Tasks forms should be standardized, and it should be possible to have a wizard-like approach so they don't need to fill submit, reopen the case on another task, fill and resubmit anbd so one. The current workcase could be left opened, with the forms \"advancing\" without need to reselect them again.\r\n	\r\nAnd as Alan Kay said : \"Simple things should be simple, complex things should be possible. \", once all those 3 priorities are done, it's very easy to divide task that's isn't defined in the core workflow to an external \"plugin\", such as \"signaling an other application to do something, via a webservice for example\".\r\n\r\nSo, that laid the initial  approach to PWKF, which was at first named \"Perl Workflow\" since I was planning to do it in Perl & wxWidgets. I migrated to Java since i wasn't that fluent in Perl anymore since I mostly use Java now at work and therefore rename PWKF in \"Personal Workflow\", in the way [PHP is for 'Personal Home Page'|http://en.wikipedia.org/wiki/PHP].","<p>It all began with a scratching need as I just felt that workflows (mostly\nBPM engines to be more precise) were the <a href=\"http://blog.pwkf.org/post/2007/09/10/Are-Workflows-the-Future-of-IT-Computing\" hreflang=\"en\">way to do many things in IT of the future</a>. There are even\nmany workflows out here already. My main feeling was that they put too much\nemphasis on how much of the standard they support, mostly in order to be\n&quot;Entreprise Grade&quot;.</p>\n<p>That felt just plain wrong to me : WF where at first designed (at least\nthat's was the usual original marketing scheme) to put back the design of the\nbusiness rules in the hands of our beloved users, in order to mostly bypass the\nIT departement, and be able to change and adapt the process very rapidly as\ndescribed on <a href=\"http://acmqueue.com/modules.php?name=Content&amp;pa=showpage&amp;pid=369\" hreflang=\"en\">ACM</a>. With the current implementations of WF, i just have to\nfeeling that it involves a lot more of XML and/or BPEL files to be written than\na non-IT worker can manage.</p>\n<p>I think that WFs are a very good concept, but the emphasis should be on the\nmodeling, on the reporting, and on the operational part (in that order). The\nnumber of constructs that the implementation recognise isn't that important :\nit could support a much smaller <em>instruction set</em>, but doing it in a\nuser-friendly way.</p>\n<ul>\n<li>The modeling is the most important part of the equation.</li>\n</ul>\n<p>It's always über-fustrating that we have to attend to meetings, capturing\nthe informations on how to model the business rules, tranlate it into something\nthat the systems understand, and to try to explain it to the users. The most\nimportant part of the modeling is that the model is accurate enough. Modeling\nis usually the difficult part of the job. I don't advocate that the business\nusers should be able to model it themselves : it's usually a recipe for a\ndisaster since they mostly don't have the mind for this. But the opposite is\nalso true : the user has to understand easily what is modeled. Modeling a\nworkflow is like writing a book : it takes a different skillset to write it and\nto read it to be able to spot inaccuracies. Since users aren't very good at\ndecyphering computer languages, having a good bijection between the modelling\nand a <em>random-human</em>-readable form is very important. That enables the\nfact that you users don't have to model their business <em>through</em> you,\nbut <em>with</em> you.</p>\n<ul>\n<li>Reporting is the second most important part</li>\n</ul>\n<p>The debugging part is also very important. Since everyone makes mistakes,\nit's very important to be able to spot them. So we have to see what happened to\na particular order. Usually that's the most difficult part of the system, but\nwith workflow, it's quite easy to log everything that happened, and be able to\nshow it later, even to show a snapshot of the current state and history of a\nparticular workcase. If you also manage to show it with a graphical form that\nis the same than the modeling one, you'll have bug reports that would be much\nmore accurate. The users will be able to tell you : <q>This workcase has gone\nthrough here and there, but since it's the special case A, it should have gone\nhere instead.</q> and then bugfixing would be usually like a piece of cake.</p>\n<ul>\n<li>Operating the workflow is also important.</li>\n</ul>\n<p>You don't <strong>use</strong> the workflow for a living. you\n<strong>design</strong> them. But that's not a reason to make the life of\noperators miserable :-). And then they should have a nice list of tasks to be\ndone. Tasks forms should be standardized, and it should be possible to have a\nwizard-like approach so they don't need to fill submit, reopen the case on\nanother task, fill and resubmit anbd so one. The current workcase could be left\nopened, with the forms &quot;advancing&quot; without need to reselect them again. And as\nAlan Kay said : &quot;Simple things should be simple, complex things should be\npossible. &quot;, once all those 3 priorities are done, it's very easy to divide\ntask that's isn't defined in the core workflow to an external &quot;plugin&quot;, such as\n&quot;signaling an other application to do something, via a webservice for\nexample&quot;.</p>\n<p>So, that laid the initial approach to PWKF, which was at first named &quot;Perl\nWorkflow&quot; since I was planning to do it in Perl &amp; wxWidgets. I migrated to\nJava since i wasn't that fluent in Perl anymore since I mostly use Java now at\nwork and therefore rename PWKF in &quot;Personal Workflow&quot;, in the way <a href=\"http://en.wikipedia.org/wiki/PHP\">PHP is for 'Personal Home Page'</a>.</p>","","little history pwkf all began with scratching need just felt that workflows mostly bpm engines more precise were the way many things the future there are even many workflows out here already main feeling was that they put too much emphasis how much the standard they support mostly order quot entreprise grade quot that felt just plain wrong where first designed least that was the usual original marketing scheme put back the design the business rules the hands our beloved users order mostly bypass the departement and able change and adapt the process very rapidly described acm with the current implementations just have feeling that involves lot more xml and bpel files written than non worker can manage think that wfs are very good concept but the emphasis should the modeling the reporting and the operational part that order the number constructs that the implementation recognise isn that important could support much smaller instruction set but doing user friendly way the modeling the most important part the equation always über fustrating that have attend meetings capturing the informations how model the business rules tranlate into something that the systems understand and try explain the users the most important part the modeling that the model accurate enough modeling usually the difficult part the job don advocate that the business users should able model themselves usually recipe for disaster since they mostly don have the mind for this but the opposite also true the user has understand easily what modeled modeling workflow like writing book takes different skillset write and read able spot inaccuracies since users aren very good decyphering computer languages having good bijection between the modelling and random human readable form very important that enables the fact that you users don have model their business through you but with you reporting the second most important part the debugging part also very important since everyone makes mistakes very important able spot them have see what happened particular order usually that the most difficult part the system but with workflow quite easy log everything that happened and able show later even show snapshot the current state and history particular workcase you also manage show with graphical form that the same than the modeling one you have bug reports that would much more accurate the users will able tell you this workcase has gone through here and there but since the special case should have gone here instead and then bugfixing would usually like piece cake operating the workflow also important you don use the workflow for living you design them but that not reason make the life operators miserable and then they should have nice list tasks done tasks forms should standardized and should possible have wizard like approach they don need fill submit reopen the case another task fill and resubmit anbd one the current workcase could left opened with the forms quot advancing quot without need reselect them again and alan kay said quot simple things should simple complex things should possible quot once all those priorities are done very easy divide task that isn defined the core workflow external quot plugin quot such quot signaling other application something via webservice for example quot that laid the initial approach pwkf which was first named quot perl workflow quot since was planning perl amp wxwidgets migrated java since wasn that fluent perl anymore since mostly use java now work and therefore rename pwkf quot personal workflow quot the way php for personal home page","a:0:{}","1","0","1","1","0","0","0"
"30617","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2006-09-15 10:55:05.947685","UTC","2006-09-15 10:55:05.947685","2007-03-17 14:41:45","","post","xhtml","2006/09/15/first","","Premier billet","","","<p>Je suis le premier billet. Modifiez moi.</p>","<p>Je suis le premier billet. Modifiez moi.</p>","","","","0","0","0","0","0","0","0"
"153057","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2007-09-10 20:58:00","Europe/Paris","2007-09-06 06:26:24","2007-09-10 18:52:32","","post","wiki","2007/09/10/Are-Workflows-the-Future-of-IT-Computing","en","Are Workflows the Future of IT Computing ?","","","We are about to change from a \"CPU power is cheap\"-paradigm to a \"CPU are cheap\"-one. Everyone knows now that the 10Ghz barrier will not be that easily broken like said Herb Sutter in his famous [The Free Lunch Is Over|http://www.gotw.ca/publications/concurrency-ddj.htm|en] DDJ article.\r\n\r\nI also think it will be much cheaper to produce a 3Ghz 128-core CPU than a 10Ghz 1-core one, and that's a huge problem in our current way to code. Current common code just isn't adapted to a many-cpu world as said Jeff Atwood in his [comparison of 4- and 2-way systems|http://www.codinghorror.com/blog/archives/000942.html|en]. It's because real parallel programming  is not easy. Even our current practices of locking are quite difficult to get right. \r\n\r\nA good way do it efficiently, is to delegate the plumbing of parallelizing the tasks to an automated system. It seems at first less efficient to do, but noone  really want to go back to ASM programming. We really have to define a new level of language that auto-parallelize the treatements, like the way we code in a high-level language that is either interpreted or even compiled.\r\n\r\nThis move in the programming area is of same order than the move from ASM to scripting or compiled language. And here you have 2 differents approaches : \r\n* the static (compiled) way \r\n* the dynamic (interpreted) way\r\n\r\nIn the static way, you have also mostly 2 choices :\r\n* a language (usually compiled) that handle the dividing & regrouping of tasks, like the model followed by [OpenMP|http://en.wikipedia.org/wiki/OpenMP|en] using mainly threads, shared memory and locking.\r\n* a language (usually fonctionnal) that handle the differents tasklets with a message-passing like interface ([Erlang|http://www.pragmaticprogrammer.com/articles/erlang.html|en] is a good example of this)\r\n\r\nThe dynamic road is less defined. But usually it involves a state-flow machine, that can coordinate all the tasks to do on all the processing units. That is usually done via a Workflow (also called BPM-engine in this particular area), like the Master-Worker pattern [Alex Miller|http://tech.puredanger.com/2007/08/30/workflow-terracotta/|en] speaks about.\r\n\r\nThis last item is, in my opinion the future. If you put everything in a database, and the BPM-engine interprets each item independantly, wich a high focus on a runtine resolution of the next thing to do.\r\nIt gives you a highly flexible design that you can change at runtime without much of trouble. Its flexibility is quite the same the one you would have with scripting languages (think Ruby) versus compiled one (think Java).","<p>We are about to change from a &quot;CPU power is cheap&quot;-paradigm to a &quot;CPU are\ncheap&quot;-one. Everyone knows now that the 10Ghz barrier will not be that easily\nbroken like said Herb Sutter in his famous <a href=\"http://www.gotw.ca/publications/concurrency-ddj.htm\" hreflang=\"en\">The Free\nLunch Is Over</a> DDJ article.</p>\n<p>I also think it will be much cheaper to produce a 3Ghz 128-core CPU than a\n10Ghz 1-core one, and that's a huge problem in our current way to code. Current\ncommon code just isn't adapted to a many-cpu world as said Jeff Atwood in his\n<a href=\"http://www.codinghorror.com/blog/archives/000942.html\" hreflang=\"en\">comparison of 4- and 2-way systems</a>. It's because real parallel\nprogramming is not easy. Even our current practices of locking are quite\ndifficult to get right.</p>\n<p>A good way do it efficiently, is to delegate the plumbing of parallelizing\nthe tasks to an automated system. It seems at first less efficient to do, but\nnoone really want to go back to ASM programming. We really have to define a new\nlevel of language that auto-parallelize the treatements, like the way we code\nin a high-level language that is either interpreted or even compiled.</p>\n<p>This move in the programming area is of same order than the move from ASM to\nscripting or compiled language. And here you have 2 differents approaches :</p>\n<ul>\n<li>the static (compiled) way</li>\n<li>the dynamic (interpreted) way</li>\n</ul>\n<p>In the static way, you have also mostly 2 choices :</p>\n<ul>\n<li>a language (usually compiled) that handle the dividing &amp; regrouping of\ntasks, like the model followed by <a href=\"http://en.wikipedia.org/wiki/OpenMP\" hreflang=\"en\">OpenMP</a> using mainly threads, shared memory and locking.</li>\n<li>a language (usually fonctionnal) that handle the differents tasklets with a\nmessage-passing like interface (<a href=\"http://www.pragmaticprogrammer.com/articles/erlang.html\" hreflang=\"en\">Erlang</a> is a good example of this)</li>\n</ul>\n<p>The dynamic road is less defined. But usually it involves a state-flow\nmachine, that can coordinate all the tasks to do on all the processing units.\nThat is usually done via a Workflow (also called BPM-engine in this particular\narea), like the Master-Worker pattern <a href=\"http://tech.puredanger.com/2007/08/30/workflow-terracotta/\" hreflang=\"en\">Alex\nMiller</a> speaks about.</p>\n<p>This last item is, in my opinion the future. If you put everything in a\ndatabase, and the BPM-engine interprets each item independantly, wich a high\nfocus on a runtine resolution of the next thing to do. It gives you a highly\nflexible design that you can change at runtime without much of trouble. Its\nflexibility is quite the same the one you would have with scripting languages\n(think Ruby) versus compiled one (think Java).</p>","Il faut ajouter les références aux blogs suivants: \r\n- Free lunch is over (Herb Sutter)\r\n- http://tech.puredanger.com/2007/08/30/workflow-terracotta/\r\n- 4-way CPU don't improve much (Coding horror)\r\n-","are workflows the future computing are about change from quot cpu power cheap quot paradigm quot cpu are cheap quot one everyone knows now that the 10ghz barrier will not that easily broken like said herb sutter his famous the free lunch over ddj article also think will much cheaper produce 3ghz 128 core cpu than 10ghz core one and that huge problem our current way code current common code just isn adapted many cpu world said jeff atwood his comparison and way systems because real parallel programming not easy even our current practices locking are quite difficult get right good way efficiently delegate the plumbing parallelizing the tasks automated system seems first less efficient but noone really want back asm programming really have define new level language that auto parallelize the treatements like the way code high level language that either interpreted even compiled this move the programming area same order than the move from asm scripting compiled language and here you have differents approaches the static compiled way the dynamic interpreted way the static way you have also mostly choices language usually compiled that handle the dividing amp regrouping tasks like the model followed openmp using mainly threads shared memory and locking language usually fonctionnal that handle the differents tasklets with message passing like interface erlang good example this the dynamic road less defined but usually involves state flow machine that can coordinate all the tasks all the processing units that usually done via workflow also called bpm engine this particular area like the master worker pattern alex miller speaks about this last item opinion the future you put everything database and the bpm engine interprets each item independantly wich high focus runtine resolution the next thing gives you highly flexible design that you can change runtime without much trouble its flexibility quite the same the one you would have with scripting languages think ruby versus compiled one think java","a:0:{}","1","0","1","1","0","0","0"
"721204","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2013-02-24 13:23:00","Europe/Paris","2013-02-24 12:23:24","2013-02-24 12:44:42","","post","wiki","2013/02/good-relationships-maintainers-curse","en","When having good relationships with package maintainers can also be a curse","","","I advise every user to __only__ use the __packaged version__ of [munin|http://munin-monitoring.org|en]. Here's a short article to explain the background of my reluctance to ask for users to directly use the official tarball.\r\n\r\nI have become upstream of [munin|http://munin-monitoring.org|en] a while ago now. As such, I'm in contact with package maintainers. They take the official releases and cram it into their own distribution of choice$$Be it linux-based like Gentoo, Redhat..., BSD-based as FreeBSD, OpenBSD..., or even multi-kernel based as Debian$$. \r\n\r\nI have to admit that the various epic war stories read throughout the web about ''upstream vs packagers'' are very far from the truth here. They are a ++charm++ to work with. Often challenging and demanding, but always because there's a real need. And that's quite a good thing, as I'm still a rookie in term of open source software management. Therefore I'm quite grateful when they gently pinpoint my mistakes$$Defaulting to CGI graphics was a move that was way too premature, end-user wise. So thanks to them, it defaults to cron again$$.\r\n\r\nYet, this nice team comes with a ''price''. Since we mostly hang out on IRC together, there is way much inter-distro communication than on other software. But I'm the ''sole'' owner of the ''tarball distro'' .\r\n\r\nYet, as I don't like to build everything from source, I obviously use a distro. There, since the packaging is very nicely done, I don't feel to take the hassle of using my own \"tarball\" to test them. I just build a package for my distro out of the release code. \r\n\r\nThat's also a curse, as I admit that I although I test the ''code'', I only seldom test the ''packaging''. This means that I cannot really advise someone on using the tarball, nor directly git code as even I don't do it.\r\n\r\nBut, that said, __I still think I'm the luckiest upstream around__. Thanks guys !","<p>I advise every user to <strong>only</strong> use the <strong>packaged\nversion</strong> of <a href=\"http://munin-monitoring.org\" hreflang=\"en\">munin</a>. Here's a short article to explain the background of my\nreluctance to ask for users to directly use the official tarball.</p>\n<p>I have become upstream of <a href=\"http://munin-monitoring.org\" hreflang=\"en\">munin</a> a while ago now. As such, I'm in contact with package\nmaintainers. They take the official releases and cram it into their own\ndistribution of choice<sup>[<a href=\"#pnote-721204-1\" id=\"rev-pnote-721204-1\" name=\"rev-pnote-721204-1\">1</a>]</sup>.</p>\n<p>I have to admit that the various epic war stories read throughout the web\nabout <em>upstream vs packagers</em> are very far from the truth here. They are\na <ins>charm</ins> to work with. Often challenging and demanding, but always\nbecause there's a real need. And that's quite a good thing, as I'm still a\nrookie in term of open source software management. Therefore I'm quite grateful\nwhen they gently pinpoint my mistakes<sup>[<a href=\"#pnote-721204-2\" id=\"rev-pnote-721204-2\" name=\"rev-pnote-721204-2\">2</a>]</sup>.</p>\n<p>Yet, this nice team comes with a <em>price</em>. Since we mostly hang out on\nIRC together, there is way much inter-distro communication than on other\nsoftware. But I'm the <em>sole</em> owner of the <em>tarball distro</em> .</p>\n<p>Yet, as I don't like to build everything from source, I obviously use a\ndistro. There, since the packaging is very nicely done, I don't feel to take\nthe hassle of using my own &quot;tarball&quot; to test them. I just build a package for\nmy distro out of the release code.</p>\n<p>That's also a curse, as I admit that I although I test the <em>code</em>, I\nonly seldom test the <em>packaging</em>. This means that I cannot really advise\nsomeone on using the tarball, nor directly git code as even I don't do it.</p>\n<p>But, that said, <strong>I still think I'm the luckiest upstream\naround</strong>. Thanks guys !</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-721204-1\" id=\"pnote-721204-1\" name=\"pnote-721204-1\">1</a>] Be it linux-based like Gentoo, Redhat..., BSD-based as\nFreeBSD, OpenBSD..., or even multi-kernel based as Debian</p>\n<p>[<a href=\"#rev-pnote-721204-2\" id=\"pnote-721204-2\" name=\"pnote-721204-2\">2</a>] Defaulting to CGI graphics was a move that was way too\npremature, end-user wise. So thanks to them, it defaults to cron again</p>\n</div>","","when having good relationships with package maintainers can also curse advise every user only use the packaged version munin here short article explain the background reluctance ask for users directly use the official tarball have become upstream munin while ago now such contact with package maintainers they take the official releases and cram into their own distribution choice have admit that the various epic war stories read throughout the web about upstream packagers are very far from the truth here they are charm work with often challenging and demanding but always because there real need and that quite good thing still rookie term open source software management therefore quite grateful when they gently pinpoint mistakes yet this nice team comes with price since mostly hang out irc together there way much inter distro communication than other software but the sole owner the tarball distro yet don like build everything from source obviously use distro there since the packaging very nicely done don feel take the hassle using own quot tarball quot test them just build package for distro out the release code that also curse admit that although test the code only seldom test the packaging this means that cannot really advise someone using the tarball nor directly git code even don but that said still think the luckiest upstream around thanks guys notes linux based like gentoo redhat bsd based freebsd openbsd even multi kernel based debian defaulting cgi graphics was move that was way too premature end user wise thanks them defaults cron again","a:1:{s:3:\"tag\";a:2:{i:0;s:5:\"munin\";i:1;s:9:\"packaging\";}}","1","0","1","0","1","0","0"
"115594","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2007-05-26 08:31:00","Europe/Paris","2007-05-26 06:29:21","2009-04-17 15:09:46","","post","wiki","2007/05/26/Avoid-the-runtime-penaly-of-Singletons","en","Avoid the runtime penaly of Singletons","","","A common way to do the Singleton pattern involves a @@getInstance()@@ method which does a runtime test like this :\r\n\r\n///\r\nprivate static Singleton instance;\r\nprivate static Object mutex;\r\n\r\npublic static Singleton getInstance() {\r\n	if (null == instance) {\r\n		// First call\r\n		synchronized (mutex) {\r\n			if (null == instance) {\r\n				instance = new Singleton ();\r\n			}\r\n		}\r\n	}\r\n	return instance;\r\n}\r\n\r\nprivate Singleton() {\r\n}\r\n///\r\n\r\nThis method works well, since it uses the ''double-null-check''. You always have to check it while beeing under a synchronized belt, and you check it just before in order to avoid the synchronized cost after its initialisation.\r\nSo we have a method that still makes a runtime check, and cannot be nicely inlined by the compilator.\r\n\r\nIf you can afford to  : \r\n* not be able to deregister the instance afterwards with a @@freeInstance()@@ method (We'll speak about this later)\r\n* not defering the creation of  the instance the first time you need it (also called lazy creation)\r\nyou can avoid that extra cost by creating the instance in class loading time with a static block.\r\n\r\nThe code then becomes : \r\n\r\n///\r\npublic static Singleton getInstance() {\r\n	return instance;\r\n}\r\n\r\nprivate Singleton() {\r\n}\r\n\r\nprivate static Singleton instance = new Singleton();\r\n///\r\n\r\nThe @@getInstance()@@ method can now be inline quite easily and there is no more extra check cost.\r\nNote that here the default constructor is empty. If it's not, I suggest to put the static initialisation at the end of the class, in order for the rest of the class to be initialised and to be ready for use in the constructor.\r\n\r\nIf you want to be able to deregister you have to use the first version, and write a freeInstance() like :\r\n\r\n///\r\npublic static void freeInstance() {\r\n	if (null != instance) {\r\n		// First call\r\n		synchronized (mutex) {\r\n			if (null != instance) {\r\n				// Do some closing work here if required like instance.close()\r\n				instance = null;\r\n			}\r\n		}\r\n	}\r\n}\r\n///\r\n\r\nAnd now you just wrote a primitive form of pooling (with only one instance)..But that's another story...","<p>A common way to do the Singleton pattern involves a\n<code>getInstance()</code> method which does a runtime test like this :</p>\n<pre>\nprivate static Singleton instance;\nprivate static Object mutex;\n\npublic static Singleton getInstance() {\n        if (null == instance) {\n                // First call\n                synchronized (mutex) {\n                        if (null == instance) {\n                                instance = new Singleton ();\n                        }\n                }\n        }\n        return instance;\n}\n\nprivate Singleton() {\n}\n</pre>\n<p>This method works well, since it uses the <em>double-null-check</em>. You\nalways have to check it while beeing under a synchronized belt, and you check\nit just before in order to avoid the synchronized cost after its\ninitialisation. So we have a method that still makes a runtime check, and\ncannot be nicely inlined by the compilator.</p>\n<p>If you can afford to :</p>\n<ul>\n<li>not be able to deregister the instance afterwards with a\n<code>freeInstance()</code> method (We'll speak about this later)</li>\n<li>not defering the creation of the instance the first time you need it (also\ncalled lazy creation)</li>\n</ul>\n<p>you can avoid that extra cost by creating the instance in class loading time\nwith a static block.</p>\n<p>The code then becomes :</p>\n<pre>\npublic static Singleton getInstance() {\n        return instance;\n}\n\nprivate Singleton() {\n}\n\nprivate static Singleton instance = new Singleton();\n</pre>\n<p>The <code>getInstance()</code> method can now be inline quite easily and\nthere is no more extra check cost. Note that here the default constructor is\nempty. If it's not, I suggest to put the static initialisation at the end of\nthe class, in order for the rest of the class to be initialised and to be ready\nfor use in the constructor.</p>\n<p>If you want to be able to deregister you have to use the first version, and\nwrite a freeInstance() like :</p>\n<pre>\npublic static void freeInstance() {\n        if (null != instance) {\n                // First call\n                synchronized (mutex) {\n                        if (null != instance) {\n                                // Do some closing work here if required like instance.close()\n                                instance = null;\n                        }\n                }\n        }\n}\n</pre>\n<p>And now you just wrote a primitive form of pooling (with only one\ninstance)..But that's another story...</p>","","avoid the runtime penaly singletons common way the singleton pattern involves getinstance method which does runtime test like this private static singleton instance private static object mutex public static singleton getinstance null instance first call synchronized mutex null instance instance new singleton return instance private singleton this method works well since uses the double null check you always have check while beeing under synchronized belt and you check just before order avoid the synchronized cost after its initialisation have method that still makes runtime check and cannot nicely inlined the compilator you can afford not able deregister the instance afterwards with freeinstance method speak about this later not defering the creation the instance the first time you need also called lazy creation you can avoid that extra cost creating the instance class loading time with static block the code then becomes public static singleton getinstance return instance private singleton private static singleton instance new singleton the getinstance method can now inline quite easily and there more extra check cost note that here the default constructor empty not suggest put the static initialisation the end the class order for the rest the class initialised and ready for use the constructor you want able deregister you have use the first version and write freeinstance like public static void freeinstance null instance first call synchronized mutex null instance some closing work here required like instance close instance null and now you just wrote primitive form pooling with only one instance but that another story","a:0:{}","1","0","1","0","0","0","0"
"436982","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2010-02-20 16:01:00","Europe/Paris","2009-09-03 08:23:23","2010-02-20 15:02:20","","post","wiki","2010/02/You-Shall-Declare-The-Unchecked-Exceptions-You-Might-Throw","en","Free Exception lunch : Use unchecked exceptions, but still announce which ones you might throw.","","","In a previous article I choosed my side : [Unchecked Exceptions are much simpler to use|/post/2009/07/Checked-or-Unchecked-Exceptions-for-Legacy-Code]. \r\n\r\nBut, on the other side of this great division, there is a very valid point : __You usually declare checked exceptions__. Sure it's possible to only declare to throw @@Exception@@, but that would defeat the whole purpose of using checked exceptions.\r\n\r\nThe nicest thing is that you can also have a custom exception hierarchy, but based on @@RuntimeException@@ instead of a plain @@Exception@@. This way it's like in C++. Everything might be thrown, and you don't __need__ to handle them. \r\n\r\n\r\n__Declaring them__, on the other side, is very interesting because you are __documenting your interface__ for almost __free__.\r\n\r\nSo, use unchecked exceptions to free yourself of the checked catch-slavery, but still declare the custom ones you might throw.","<p>In a previous article I choosed my side : <a href=\"/post/2009/07/Checked-or-Unchecked-Exceptions-for-Legacy-Code\">Unchecked\nExceptions are much simpler to use</a>.</p>\n<p>But, on the other side of this great division, there is a very valid point :\n<strong>You usually declare checked exceptions</strong>. Sure it's possible to\nonly declare to throw <code>Exception</code>, but that would defeat the whole\npurpose of using checked exceptions.</p>\n<p>The nicest thing is that you can also have a custom exception hierarchy, but\nbased on <code>RuntimeException</code> instead of a plain\n<code>Exception</code>. This way it's like in C++. Everything might be thrown,\nand you don't <strong>need</strong> to handle them.</p>\n<p><strong>Declaring them</strong>, on the other side, is very interesting\nbecause you are <strong>documenting your interface</strong> for almost\n<strong>free</strong>.</p>\n<p>So, use unchecked exceptions to free yourself of the checked catch-slavery,\nbut still declare the custom ones you might throw.</p>","","free exception lunch use unchecked exceptions but still announce which ones you might throw previous article choosed side unchecked exceptions are much simpler use but the other side this great division there very valid point you usually declare checked exceptions sure possible only declare throw exception but that would defeat the whole purpose using checked exceptions the nicest thing that you can also have custom exception hierarchy but based runtimeexception instead plain exception this way like everything might thrown and you don need handle them declaring them the other side very interesting because you are documenting your interface for almost free use unchecked exceptions free yourself the checked catch slavery but still declare the custom ones you might throw","","1","0","1","0","0","0","0"
"309272","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-03-30 13:05:00","Europe/Paris","2008-12-16 06:35:30","2009-05-18 10:06:35","","post","wiki","2009/03/Databases:-Meta-Data-ctime-mtime","en","Databases: Meta-Data (ctime & mtime)","A RDBMS is all about data. Actually also about the relationships between them, but let's focus mostly on the datas.\r\n\r\nWe can even view a RDBMS as a glorified file system, with the tables like directories and rows like files inside. Every filesystem has some meta-data attached to its elements such as the creation date and the last modification date. With this analogy it becomes quite clear that we could also have this meta-data on the table. The good part of the filesystem meta-data design is that it's handled ''outside'' the element. The last modification date is something that isn't stored in the file itself. (Actually it could, but then it had an application meaning, and is to be handled by the application). It's also completely automated, without any intervention required, or even possible, on the application part (except for specialized tools like when we restore a backup).","<p>A RDBMS is all about data. Actually also about the relationships between\nthem, but let's focus mostly on the datas.</p>\n<p>We can even view a RDBMS as a glorified file system, with the tables like\ndirectories and rows like files inside. Every filesystem has some meta-data\nattached to its elements such as the creation date and the last modification\ndate. With this analogy it becomes quite clear that we could also have this\nmeta-data on the table. The good part of the filesystem meta-data design is\nthat it's handled <em>outside</em> the element. The last modification date is\nsomething that isn't stored in the file itself. (Actually it could, but then it\nhad an application meaning, and is to be handled by the application). It's also\ncompletely automated, without any intervention required, or even possible, on\nthe application part (except for specialized tools like when we restore a\nbackup).</p>","Having this, knowing the last modification of an item is as easy as querying this extra information that is updated automatically by the system.\r\n\r\nAdaptation to RDBMS is quite easy. Either the RDBMS comes already with the necessary tools to records this kind of extra information, or it is just a matter of ''ghosting'' the schema with tables that will only contain the meta-data. The information inside is updated automatically via triggers on the main table.\r\n\r\nAn example would be for the @@ORDERS@@ and @@ORDER_ITEMS@@ tables (I don't use a specific SQL dialect, it's just for illustrate purposes. Just feel free to adapt the code to your favorite database) : \r\n\r\n///\r\nTABLE ORDERS ( \r\n   SERIAL ORDERS_ID PRIMARY KEY,\r\n   MONEY PRICE_ADJUSTEMENT\r\n)\r\n\r\nTABLE ORDER_ITEMS ( \r\n   SERIAL ORDER_ITEMS_ID PRIMARY KEY,\r\n   INTEGER ORDERS_ID FOREIGN_KEY ON ORDERS(ORDERS_ID), \r\n   INTEGER PRODUCT_ID, \r\n   INTEGER QUANTITY,\r\n   MONEY UNIT_PRICE\r\n)\r\n///\r\n\r\nWe have to create 2 ''ghost'' tables @@MD_ORDERS@@ and @@MD_ORDER_ITEMS@@ :\r\n\r\n///\r\nTABLE MD_ORDERS ( \r\n   INT ORDERS_ID FOREIGN_KEY ON ORDERS(ORDERS_ID),\r\n   TIMESTAMP CTIME DEFAULT NOW(),\r\n   TIMESTAMP MTIME DEFAULT NOW(),\r\n   VARCHAR CLOGIN DEFAULT CURRENT_USER(),\r\n   VARCHAR MLOGIN DEFAULT CURRENT_USER()\r\n)\r\n\r\nTABLE ORDER_ITEMS ( \r\n   INT ORDER_ITEMS_ID FOREIGN_KEY ON ORDER_ITEMS(ORDER_ITEMS_ID),\r\n   TIMESTAMP CTIME DEFAULT NOW(),\r\n   TIMESTAMP MTIME DEFAULT NOW(),\r\n   VARCHAR CLOGIN DEFAULT CURRENT_USER(),\r\n   VARCHAR MLOGIN DEFAULT CURRENT_USER()\r\n)\r\n///\r\n\r\nThe colums @@CTIME@@ and @@MTIME@@ are obviously like their corresponding part in file-systems. @@CLOGIN@@ and @@MLOGIN@@ are either the login used to connect to the database or for a web application that usually have only one DB login the current application user connected (To communicate this information to the database system, tt could be inserted in a special temporary table at the beginning of each request, scoped at the current connection/transaction/end-user-request/... in a way shared by all applications).\r\n\r\nUpdating theses 2 tables is as easy as adding several TRIGGERS. (The examples given below are only for ORDERS, for ORDER_ITEMS it's the same pattern)\r\n\r\n///\r\nCREATE INSERT TRIGGER ON ORDERS o\r\nBEGIN\r\n  INSERT INTO MD_ORDERS (ORDERS_ID) VALUES (o.ORDERS_ID)\r\nEND\r\n\r\nCREATE UPDATE TRIGGER ON ORDERS o\r\nBEGIN\r\n  UPDATE MD_ORDERS SET MTIME=NOW(), MLOGIN=CURRENT_USER() \r\n    WHERE ORDERS_ID = o.ORDERS_ID\r\nEND\r\n///\r\n\r\nThe obvious avantage to this that you can use this technique __right now__ on your application, since it doesn't require any applicative change. The new columns are completely transparent. Even the locking scheme is the same : if the shadow row has to be locked for updating, the underlaying base row is currently also locked anyway. The only side effect is that you will have effectively 2 times more updates in the database system, and therefore you have to be careful at where you put your new tables (in order not to be bitten by the I/O increase cost) if you have a clever table/tablespace layout.\r\n\r\nAnother advantage is that if you relax your foreign keys constraints you can even log the __deletion__ date of a row in the underlying base table.\r\n\r\n///\r\nTABLE MD_ORDERS ( \r\n   INT ORDERS_ID ON ORDERS(ORDERS_ID) INDEXED,\r\n   TIMESTAMP CTIME DEFAULT NOW(),\r\n   TIMESTAMP MTIME DEFAULT NOW(),\r\n   TIMESTAMP DTIME DEFAULT NULL,\r\n   VARCHAR CLOGIN DEFAULT CURRENT_USER(),\r\n   VARCHAR MLOGIN DEFAULT CURRENT_USER(),\r\n   VARCHAR DLOGIN DEFAULT NULL\r\n)\r\n///\r\n\r\nThe @@DELETE@@ trigger is obvious, but the @@INSERT@@ one has to be careful to handle the insertion of a deleted entry.\r\n\r\nThis ''delete'' feature leads us to the next entry on [Databases: Version History|/post/2009/03/Databases%3A-Version-History]\r\n\r\n__Edit(10/04/2009):__ Actually CTIME isn't the __C__reation Time in POSIX as I first thought, but __C__hange time. It's related to MTIME, as the MTIME takes also the ''content'' into account. So we always have a later CTIME than MTIME (or equal). And there seem to be [no Creation Time|http://www.faqs.org/faqs/unix-faq/faq/part3/section-1.html|en] in POSIX.","<p>Having this, knowing the last modification of an item is as easy as querying\nthis extra information that is updated automatically by the system.</p>\n<p>Adaptation to RDBMS is quite easy. Either the RDBMS comes already with the\nnecessary tools to records this kind of extra information, or it is just a\nmatter of <em>ghosting</em> the schema with tables that will only contain the\nmeta-data. The information inside is updated automatically via triggers on the\nmain table.</p>\n<p>An example would be for the <code>ORDERS</code> and <code>ORDER_ITEMS</code>\ntables (I don't use a specific SQL dialect, it's just for illustrate purposes.\nJust feel free to adapt the code to your favorite database) :</p>\n<pre>\nTABLE ORDERS ( \n   SERIAL ORDERS_ID PRIMARY KEY,\n   MONEY PRICE_ADJUSTEMENT\n)\n\nTABLE ORDER_ITEMS ( \n   SERIAL ORDER_ITEMS_ID PRIMARY KEY,\n   INTEGER ORDERS_ID FOREIGN_KEY ON ORDERS(ORDERS_ID), \n   INTEGER PRODUCT_ID, \n   INTEGER QUANTITY,\n   MONEY UNIT_PRICE\n)\n</pre>\n<p>We have to create 2 <em>ghost</em> tables <code>MD_ORDERS</code> and\n<code>MD_ORDER_ITEMS</code> :</p>\n<pre>\nTABLE MD_ORDERS ( \n   INT ORDERS_ID FOREIGN_KEY ON ORDERS(ORDERS_ID),\n   TIMESTAMP CTIME DEFAULT NOW(),\n   TIMESTAMP MTIME DEFAULT NOW(),\n   VARCHAR CLOGIN DEFAULT CURRENT_USER(),\n   VARCHAR MLOGIN DEFAULT CURRENT_USER()\n)\n\nTABLE ORDER_ITEMS ( \n   INT ORDER_ITEMS_ID FOREIGN_KEY ON ORDER_ITEMS(ORDER_ITEMS_ID),\n   TIMESTAMP CTIME DEFAULT NOW(),\n   TIMESTAMP MTIME DEFAULT NOW(),\n   VARCHAR CLOGIN DEFAULT CURRENT_USER(),\n   VARCHAR MLOGIN DEFAULT CURRENT_USER()\n)\n</pre>\n<p>The colums <code>CTIME</code> and <code>MTIME</code> are obviously like\ntheir corresponding part in file-systems. <code>CLOGIN</code> and\n<code>MLOGIN</code> are either the login used to connect to the database or for\na web application that usually have only one DB login the current application\nuser connected (To communicate this information to the database system, tt\ncould be inserted in a special temporary table at the beginning of each\nrequest, scoped at the current connection/transaction/end-user-request/... in a\nway shared by all applications).</p>\n<p>Updating theses 2 tables is as easy as adding several TRIGGERS. (The\nexamples given below are only for ORDERS, for ORDER_ITEMS it's the same\npattern)</p>\n<pre>\nCREATE INSERT TRIGGER ON ORDERS o\nBEGIN\n  INSERT INTO MD_ORDERS (ORDERS_ID) VALUES (o.ORDERS_ID)\nEND\n\nCREATE UPDATE TRIGGER ON ORDERS o\nBEGIN\n  UPDATE MD_ORDERS SET MTIME=NOW(), MLOGIN=CURRENT_USER() \n    WHERE ORDERS_ID = o.ORDERS_ID\nEND\n</pre>\n<p>The obvious avantage to this that you can use this technique <strong>right\nnow</strong> on your application, since it doesn't require any applicative\nchange. The new columns are completely transparent. Even the locking scheme is\nthe same : if the shadow row has to be locked for updating, the underlaying\nbase row is currently also locked anyway. The only side effect is that you will\nhave effectively 2 times more updates in the database system, and therefore you\nhave to be careful at where you put your new tables (in order not to be bitten\nby the I/O increase cost) if you have a clever table/tablespace layout.</p>\n<p>Another advantage is that if you relax your foreign keys constraints you can\neven log the <strong>deletion</strong> date of a row in the underlying base\ntable.</p>\n<pre>\nTABLE MD_ORDERS ( \n   INT ORDERS_ID ON ORDERS(ORDERS_ID) INDEXED,\n   TIMESTAMP CTIME DEFAULT NOW(),\n   TIMESTAMP MTIME DEFAULT NOW(),\n   TIMESTAMP DTIME DEFAULT NULL,\n   VARCHAR CLOGIN DEFAULT CURRENT_USER(),\n   VARCHAR MLOGIN DEFAULT CURRENT_USER(),\n   VARCHAR DLOGIN DEFAULT NULL\n)\n</pre>\n<p>The <code>DELETE</code> trigger is obvious, but the <code>INSERT</code> one\nhas to be careful to handle the insertion of a deleted entry.</p>\n<p>This <em>delete</em> feature leads us to the next entry on <a href=\"/post/2009/03/Databases%3A-Version-History\">Databases: Version History</a></p>\n<p><strong>Edit(10/04/2009):</strong> Actually CTIME isn't the\n<strong>C</strong>reation Time in POSIX as I first thought, but\n<strong>C</strong>hange time. It's related to MTIME, as the MTIME takes also\nthe <em>content</em> into account. So we always have a later CTIME than MTIME\n(or equal). And there seem to be <a href=\"http://www.faqs.org/faqs/unix-faq/faq/part3/section-1.html\" hreflang=\"en\">no\nCreation Time</a> in POSIX.</p>","","databases meta data ctime mtime rdbms all about data actually also about the relationships between them but let focus mostly the datas can even view rdbms glorified file system with the tables like directories and rows like files inside every filesystem has some meta data attached its elements such the creation date and the last modification date with this analogy becomes quite clear that could also have this meta data the table the good part the filesystem meta data design that handled outside the element the last modification date something that isn stored the file itself actually could but then had application meaning and handled the application also completely automated without any intervention required even possible the application part except for specialized tools like when restore backup having this knowing the last modification item easy querying this extra information that updated automatically the system adaptation rdbms quite easy either the rdbms comes already with the necessary tools records this kind extra information just matter ghosting the schema with tables that will only contain the meta data the information inside updated automatically via triggers the main table example would for the orders and order items tables don use specific sql dialect just for illustrate purposes just feel free adapt the code your favorite database table orders serial orders primary key money price adjustement table order items serial order items primary key integer orders foreign key orders orders integer product integer quantity money unit price have create ghost tables orders and order items table orders int orders foreign key orders orders timestamp ctime default now timestamp mtime default now varchar clogin default current user varchar mlogin default current user table order items int order items foreign key order items order items timestamp ctime default now timestamp mtime default now varchar clogin default current user varchar mlogin default current user the colums ctime and mtime are obviously like their corresponding part file systems clogin and mlogin are either the login used connect the database for web application that usually have only one login the current application user connected communicate this information the database system could inserted special temporary table the beginning each request scoped the current connection transaction end user request way shared all applications updating theses tables easy adding several triggers the examples given below are only for orders for order items the same pattern create insert trigger orders begin insert into orders orders values orders end create update trigger orders begin update orders set mtime now mlogin current user where orders orders end the obvious avantage this that you can use this technique right now your application since doesn require any applicative change the new columns are completely transparent even the locking scheme the same the shadow row has locked for updating the underlaying base row currently also locked anyway the only side effect that you will have effectively times more updates the database system and therefore you have careful where you put your new tables order not bitten the increase cost you have clever table tablespace layout another advantage that you relax your foreign keys constraints you can even log the deletion date row the underlying base table table orders int orders orders orders indexed timestamp ctime default now timestamp mtime default now timestamp dtime default null varchar clogin default current user varchar mlogin default current user varchar dlogin default null the delete trigger obvious but the insert one has careful handle the insertion deleted entry this delete feature leads the next entry databases version history edit 2009 actually ctime isn the creation time posix first thought but change time related mtime the mtime takes also the content into account always have later ctime than mtime equal and there seem creation time posix","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","1","0","1","0","0","0","0"
"727986","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38366","2013-04-04 20:48:00","Europe/Paris","2013-04-04 09:49:18","2013-04-06 11:01:10","","post","wiki","2013/04/Do-not-fear-git-rebase-:-make-snapshots-!","en","Do not fear git rebase : make snapshots !","","","Git is a nice version system, but some commands are destructrive, such as rebase. \r\n\r\nHere is a script to have a safety net, and free backups !\r\n\r\n///\r\n#! /bin/sh\r\n# Script to snaphot a git repo\r\nSNAP_VERSION=$(date +%s)\r\nBUNDLE_NAME=$(basename $( pwd )).${SNAP_VERSION}.git.bundle\r\ngit bundle create ../${BUNDLE_NAME} --all\r\ngit remote add snap-${SNAP_VERSION} ../${BUNDLE_NAME}\r\ngit fetch -p snap-${SNAP_VERSION}\r\n///\r\n\r\nUsage is very easy. If you want to restore your current branch to the master one you made earlier.\r\n\r\n git reset --hard snap-1365068411/master","<p>Git is a nice version system, but some commands are destructrive, such as\nrebase.</p>\n<p>Here is a script to have a safety net, and free backups !</p>\n<pre>\n#! /bin/sh\n# Script to snaphot a git repo\nSNAP_VERSION=$(date +%s)\nBUNDLE_NAME=$(basename $( pwd )).${SNAP_VERSION}.git.bundle\ngit bundle create ../${BUNDLE_NAME} --all\ngit remote add snap-${SNAP_VERSION} ../${BUNDLE_NAME}\ngit fetch -p snap-${SNAP_VERSION}\n</pre>\n<p>Usage is very easy. If you want to restore your current branch to the master\none you made earlier.</p>\n<pre>\ngit reset --hard snap-1365068411/master\n</pre>","","not fear git rebase make snapshots git nice version system but some commands are destructrive such rebase here script have safety net and free backups bin script snaphot git repo snap version date bundle name basename pwd snap version git bundle git bundle create bundle name all git remote add snap snap version bundle name git fetch snap snap version usage very easy you want restore your current branch the master one you made earlier git reset hard snap 1365068411 master","a:1:{s:3:\"tag\";a:3:{i:0;s:6:\"rebase\";i:1;s:6:\"backup\";i:2;s:3:\"git\";}}","1","0","1","0","0","0","0"
"243917","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2008-05-26 15:01:00","Europe/Paris","2008-05-26 13:01:00","2009-05-06 09:49:00","","post","wiki","2008/05/26/Why-Negative-comments-are-better-than-positive-ones","en","Why Negative comments are better than positive ones","","","Why Negative comments are better than positive ones If you are reading this blog and thinking {{Hey ! What a  He's just talking nonsense...}} please don't walk away in horror. It would be very much appreciated if you drop me a comment about what is wrong in my post instead. I actually __prefer__ to have negative comments than positive ones... at least when they are well argumented. \r\n\r\nAs a matter of fact, we mostly learn by our mistakes. If you do something and it works, you are really happy but you don't know __why__ it works. The next time you have to do something quite similar you are tempted to change the least possible in order not to be disappointed. This leads to the infamous [Cargo Cult Programming|http://en.wikipedia.org/wiki/Cargo_cult_programming] effect. Whereas if it doesn't work you spend some times, but just learned something that you can subsequently reuse.\r\n\r\nIt's actually by been challenged that you make the most interesting progress. I nevertheless agree that when in isolation your raw productivity is much bigger than when being part of a team. The reason if quite obvious : since you don't have to argue with others, you can spend all your time doing ''useful'' stuff. The main problem with this approach is that the ''real'' goal (where you really should go), isn't necessarily where you ''think'' it is. So you just might go very fast, but aiming the wrong goal. Sometimes the very fact explaining something to someone (that didn't even disagree) can show you the internal problems of your way of thinking. \r\n\r\nIt's actually the ''convincing-battle'' that you have to fight with your audience/co-workers/etc that leads to the most interesting solutions. We are all humans, and each had different experiences, hence different point of views. So the real cleverness is to be able to take the ladder of your opinions, climb with them on the shoulder of giants and give the feed back your fellow giants what your new point of view gives you to see. I really insist on the feed-back stage, knowledge is something you can even increase by giving it, since it usually makes you think ways that you would not have explored normally.\r\n\r\nSo I do write in this blog with my own convictions. It is certainly not the __universal truth__, but it's my very own vision of it. If you feel that I'm wrong, feel free to tell me : I don't say I'll agree with you, but it may be a very interesting ''battle'' that might even elevate both of us.","<p>Why Negative comments are better than positive ones If you are reading this\nblog and thinking <q>Hey ! What a He's just talking nonsense...</q> please\ndon't walk away in horror. It would be very much appreciated if you drop me a\ncomment about what is wrong in my post instead. I actually\n<strong>prefer</strong> to have negative comments than positive ones... at\nleast when they are well argumented.</p>\n<p>As a matter of fact, we mostly learn by our mistakes. If you do something\nand it works, you are really happy but you don't know <strong>why</strong> it\nworks. The next time you have to do something quite similar you are tempted to\nchange the least possible in order not to be disappointed. This leads to the\ninfamous <a href=\"http://en.wikipedia.org/wiki/Cargo_cult_programming\">Cargo\nCult Programming</a> effect. Whereas if it doesn't work you spend some times,\nbut just learned something that you can subsequently reuse.</p>\n<p>It's actually by been challenged that you make the most interesting\nprogress. I nevertheless agree that when in isolation your raw productivity is\nmuch bigger than when being part of a team. The reason if quite obvious : since\nyou don't have to argue with others, you can spend all your time doing\n<em>useful</em> stuff. The main problem with this approach is that the\n<em>real</em> goal (where you really should go), isn't necessarily where you\n<em>think</em> it is. So you just might go very fast, but aiming the wrong\ngoal. Sometimes the very fact explaining something to someone (that didn't even\ndisagree) can show you the internal problems of your way of thinking.</p>\n<p>It's actually the <em>convincing-battle</em> that you have to fight with\nyour audience/co-workers/etc that leads to the most interesting solutions. We\nare all humans, and each had different experiences, hence different point of\nviews. So the real cleverness is to be able to take the ladder of your\nopinions, climb with them on the shoulder of giants and give the feed back your\nfellow giants what your new point of view gives you to see. I really insist on\nthe feed-back stage, knowledge is something you can even increase by giving it,\nsince it usually makes you think ways that you would not have explored\nnormally.</p>\n<p>So I do write in this blog with my own convictions. It is certainly not the\n<strong>universal truth</strong>, but it's my very own vision of it. If you\nfeel that I'm wrong, feel free to tell me : I don't say I'll agree with you,\nbut it may be a very interesting <em>battle</em> that might even elevate both\nof us.</p>","","why negative comments are better than positive ones why negative comments are better than positive ones you are reading this blog and thinking hey what just talking nonsense please don walk away horror would very much appreciated you drop comment about what wrong post instead actually prefer have negative comments than positive ones least when they are well argumented matter fact mostly learn our mistakes you something and works you are really happy but you don know why works the next time you have something quite similar you are tempted change the least possible order not disappointed this leads the infamous cargo cult programming effect whereas doesn work you spend some times but just learned something that you can subsequently reuse actually been challenged that you make the most interesting progress nevertheless agree that when isolation your raw productivity much bigger than when being part team the reason quite obvious since you don have argue with others you can spend all your time doing useful stuff the main problem with this approach that the real goal where you really should isn necessarily where you think you just might very fast but aiming the wrong goal sometimes the very fact explaining something someone that didn even disagree can show you the internal problems your way thinking actually the convincing battle that you have fight with your audience workers etc that leads the most interesting solutions are all humans and each had different experiences hence different point views the real cleverness able take the ladder your opinions climb with them the shoulder giants and give the feed back your fellow giants what your new point view gives you see really insist the feed back stage knowledge something you can even increase giving since usually makes you think ways that you would not have explored normally write this blog with own convictions certainly not the universal truth but very own vision you feel that wrong feel free tell don say agree with you but may very interesting battle that might even elevate both","a:0:{}","1","0","1","0","2","0","0"
"459093","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2009-11-10 18:26:00","Europe/Paris","2009-11-10 17:26:21","2010-06-28 16:56:42","","post","wiki","2009/11/Per-user-rc.d-system-V-init-scripts","en","Per user rc.d system V init scripts","","","In the UNIX world, there is usually a ''sysadmin'' that does all the installing, launching, monitoring, etc. \r\n\r\nRegular users are just that, regular users. They are focused on __using__ the system, not __running__ it. \r\n\r\nIn a world of SOA, as every application becomes a service, the overhead of having a dedicated team is increasing rapidly. Usually every team could install\r\n\r\nSometimes the \r\n\r\nLike in http://serverfault.com/questions/83253/managing-per-user-rc-d-init-scripts","<p>In the UNIX world, there is usually a <em>sysadmin</em> that does all the\ninstalling, launching, monitoring, etc.</p>\n<p>Regular users are just that, regular users. They are focused on\n<strong>using</strong> the system, not <strong>running</strong> it.</p>\n<p>In a world of SOA, as every application becomes a service, the overhead of\nhaving a dedicated team is increasing rapidly. Usually every team could\ninstall</p>\n<p>Sometimes the</p>\n<p>Like in\nhttp://serverfault.com/questions/83253/managing-per-user-rc-d-init-scripts</p>","","per user system init scripts the unix world there usually sysadmin that does all the installing launching monitoring etc regular users are just that regular users they are focused using the system not running world soa every application becomes service the overhead having dedicated team increasing rapidly usually every team could install sometimes the like http serverfault com questions 83253 managing per user init scripts","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"sysadmin\";i:1;s:5:\"munin\";}}","-2","0","1","0","0","0","0"
"391836","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2009-04-10 12:46:00","Europe/Paris","2009-04-08 18:45:27","2009-05-18 09:48:27","","post","wiki","2009/04/Email-Ping-to-Comments-Reply-on-Blogs","en","Email Ping to Comments Reply on Blogs","","","As a blog writer I can receive the comments written on my blog via email. \r\n\r\nI'm wondering why this service isn't implemented for replies to my comments on foreign blogs. I don't really want to remember all the blogs I left a comment on in order to poll them to see if there is a reply to my comment.\r\n\r\nActually, a quick hack would be to subscribe to the RSS feed of this particular entry and let the RSS feeder handle all the remembrance and polling for us, but that seems so ''inneffective''. Just imagine all the wasted traffic induced by polling if this habit becomes mainstreams. \r\n\r\nI thought about turning it the other way around, since it seems that I'm forced to leave an email adress on each comment I leave, why don't the blogging engines just use this adress to send a notification (maybe opt-in) if there is a activity ?\r\n\r\nI wanted to activate this feature on my blog and it seems non-existant. I therefore searched  around quickly only to discover that noone seems to be having this particular need. \r\n\r\nStrange...\r\n\r\n__Edit(15/04/2009)__: I just found out that WordPress seems to support this functionality via a plugin : [Subscribe to Comments|http://txfx.net/code/wordpress/subscribe-to-comments/|en]","<p>As a blog writer I can receive the comments written on my blog via\nemail.</p>\n<p>I'm wondering why this service isn't implemented for replies to my comments\non foreign blogs. I don't really want to remember all the blogs I left a\ncomment on in order to poll them to see if there is a reply to my comment.</p>\n<p>Actually, a quick hack would be to subscribe to the RSS feed of this\nparticular entry and let the RSS feeder handle all the remembrance and polling\nfor us, but that seems so <em>inneffective</em>. Just imagine all the wasted\ntraffic induced by polling if this habit becomes mainstreams.</p>\n<p>I thought about turning it the other way around, since it seems that I'm\nforced to leave an email adress on each comment I leave, why don't the blogging\nengines just use this adress to send a notification (maybe opt-in) if there is\na activity ?</p>\n<p>I wanted to activate this feature on my blog and it seems non-existant. I\ntherefore searched around quickly only to discover that noone seems to be\nhaving this particular need.</p>\n<p>Strange...</p>\n<p><strong>Edit(15/04/2009)</strong>: I just found out that WordPress seems to\nsupport this functionality via a plugin : <a href=\"http://txfx.net/code/wordpress/subscribe-to-comments/\" hreflang=\"en\">Subscribe\nto Comments</a></p>","","email ping comments reply blogs blog writer can receive the comments written blog via email wondering why this service isn implemented for replies comments foreign blogs don really want remember all the blogs left comment order poll them see there reply comment actually quick hack would subscribe the rss feed this particular entry and let the rss feeder handle all the remembrance and polling for but that seems inneffective just imagine all the wasted traffic induced polling this habit becomes mainstreams thought about turning the other way around since seems that forced leave email adress each comment leave why don the blogging engines just use this adress send notification maybe opt there activity wanted activate this feature blog and seems non existant therefore searched around quickly only discover that noone seems having this particular need strange edit 2009 just found out that wordpress seems support this functionality via plugin subscribe comments","a:0:{}","1","0","1","0","0","0","0"
"399444","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","","2009-05-04 14:51:00","Europe/Paris","2009-05-04 12:51:40","2009-05-04 13:05:53","IgnoreMe","page","xhtml","Google-Analytics-IgnoreMe-Page","en","Google Analytics IgnoreMe Page","","","<br /><script>\r\nfunction ignoreMe() { pageTracker._setVar('IgnoreMe'); }\r\nfor each (body in document.getElementsByTagName('body')) {\r\nbody.onLoad = ignoreMe;\r\n}\r\n</script>","<br />\n\n//","","google analytics ignoreme page","","1","0","0","0","0","0","0"
"439371","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2009-09-07 22:00:00","Europe/Paris","2009-09-08 13:21:10","2009-09-08 15:46:55","","post","wiki","2009/09/Overloading-method-is-hard-:-a-common-pitfall","en","Overloading a method is hard : a common pitfall","","","As I said in my [equality article|/post/2009/06/Equality-in-Java-is-a-Hot-Topic-but-a-Hazardous-one], overloading in Java$$It's not really a ''Java''-ism, it's the same in other languages, such as C++ .$$ is resolved by the __static__ type of the argument, not the __run-time__ type. \r\n\r\nIt's a __generic__  problem of most compiled OO languages since usually __overloading resolution happens at compile-time and not at runtime__.\r\n\r\nNow, ''that'' militates for the well known idiom : \r\n\r\n> Never overload a method with one that has the same number of parameters.\r\n\r\nActually, it should be enough to overload a method with one that accept parameters that are not inheritance-related : @@String@@ and @@Number@@ would be OK, but @@MyClass@@ and @@Object@@ would not.","<p>As I said in my <a href=\"/post/2009/06/Equality-in-Java-is-a-Hot-Topic-but-a-Hazardous-one\">equality\narticle</a>, overloading in Java<sup>[<a href=\"#pnote-439371-1\" id=\"rev-pnote-439371-1\" name=\"rev-pnote-439371-1\">1</a>]</sup> is resolved by the\n<strong>static</strong> type of the argument, not the <strong>run-time</strong>\ntype.</p>\n<p>It's a <strong>generic</strong> problem of most compiled OO languages since\nusually <strong>overloading resolution happens at compile-time and not at\nruntime</strong>.</p>\n<p>Now, <em>that</em> militates for the well known idiom :</p>\n<blockquote>\n<p>Never overload a method with one that has the same number of parameters.</p>\n</blockquote>\n<p>Actually, it should be enough to overload a method with one that accept\nparameters that are not inheritance-related : <code>String</code> and\n<code>Number</code> would be OK, but <code>MyClass</code> and\n<code>Object</code> would not.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-439371-1\" id=\"pnote-439371-1\" name=\"pnote-439371-1\">1</a>] It's not really a <em>Java</em>-ism, it's the same in\nother languages, such as C++ .</p>\n</div>","","overloading method hard common pitfall said equality article overloading java resolved the static type the argument not the run time type generic problem most compiled languages since usually overloading resolution happens compile time and not runtime now that militates for the well known idiom never overload method with one that has the same number parameters actually should enough overload method with one that accept parameters that are not inheritance related string and number would but myclass and object would not notes not really java ism the same other languages such","","1","0","1","0","0","0","0"
"638286","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2011-09-22 14:17:00","Europe/Paris","2011-09-22 12:17:23","2011-09-25 14:18:30","","post","wiki","2011/09/Tutorial-Write-your-own-Munin-plugin-Part-1","en","Tutorial - Write your own Munin plugin - Part 1","","","Writing a [munin|http://munin-monitoring.org/|en] plugin is dead easy, let's see how.\r\n\r\n!!!! Context\r\n\r\nWe'll write a plugin for the apache log file. It is very simple at first, but will evolve to use progressively all the features of munin.\r\n\r\nWe assume the apache log file is @@/var/log/apache/access_log@@, and its format is a plain common log format with vhost prefixed to it.\r\n\r\n///\r\nLogFormat \"%v %l %u %t \\"%r\\" %>s %b\" commonvhost\r\nCustomLog /var/log/apache/access_log commonvhost \r\n///\r\n\r\n!!!! Simple one\r\n\r\nHere a very simple plugin, all in POSIX shell :\r\n\r\n///\r\n#! /bin/sh\r\n# Simple apache plugin\r\n\r\nif [ $1 = \"config\" ]\r\nthen\r\n  cat <<EOF\r\ngraph_title Simple apache plugin to show hits\r\nhits_2xx.label Hits 2xx\r\nhits_3xx.label Hits 3xx\r\nhits_4xx.label Hits 4xx\r\nhits_5xx.label Hits 5xx\r\nEOF\r\nfi\r\n\r\nHITS_2XX=$(grep -c '\" 2.. ' /var/log/apache/access_log )\r\nHITS_3XX=$(grep -c '\" 3.. ' /var/log/apache/access_log )\r\nHITS_4XX=$(grep -c '\" 4.. ' /var/log/apache/access_log )\r\nHITS_5XX=$(grep -c '\" 5.. ' /var/log/apache/access_log )\r\n\r\n# now we emit the values. \r\n# The '.value' part is important and should not be omitted.\r\necho \"hits_2xx.value $HITS_2XX\"\r\necho \"hits_3xx.value $HITS_3XX\"\r\necho \"hits_4xx.value $HITS_4XX\"\r\necho \"hits_5xx.value $HITS_5XX\"\r\n\r\n\r\n\r\n///","<p>Writing a <a href=\"http://munin-monitoring.org/\" hreflang=\"en\">munin</a>\nplugin is dead easy, let's see how.</p>\n<h2>Context</h2>\n<p>We'll write a plugin for the apache log file. It is very simple at first,\nbut will evolve to use progressively all the features of munin.</p>\n<p>We assume the apache log file is <code>/var/log/apache/access_log</code>,\nand its format is a plain common log format with vhost prefixed to it.</p>\n<pre>\nLogFormat &quot;%v %l %u %t \&quot;%r\&quot; %&gt;s %b&quot; commonvhost\nCustomLog /var/log/apache/access_log commonvhost \n</pre>\n<h2>Simple one</h2>\n<p>Here a very simple plugin, all in POSIX shell :</p>\n<pre>\n#! /bin/sh\n# Simple apache plugin\n\nif [ $1 = &quot;config&quot; ]\nthen\n  cat &lt;&lt;EOF\ngraph_title Simple apache plugin to show hits\nhits_2xx.label Hits 2xx\nhits_3xx.label Hits 3xx\nhits_4xx.label Hits 4xx\nhits_5xx.label Hits 5xx\nEOF\nfi\n\nHITS_2XX=$(grep -c '&quot; 2.. ' /var/log/apache/access_log )\nHITS_3XX=$(grep -c '&quot; 3.. ' /var/log/apache/access_log )\nHITS_4XX=$(grep -c '&quot; 4.. ' /var/log/apache/access_log )\nHITS_5XX=$(grep -c '&quot; 5.. ' /var/log/apache/access_log )\n\n# now we emit the values. \n# The '.value' part is important and should not be omitted.\necho &quot;hits_2xx.value $HITS_2XX&quot;\necho &quot;hits_3xx.value $HITS_3XX&quot;\necho &quot;hits_4xx.value $HITS_4XX&quot;\necho &quot;hits_5xx.value $HITS_5XX&quot;\n\n\n\n</pre>","","tutorial write your own munin plugin part writing munin plugin dead easy let see how context write plugin for the apache log file very simple first but will evolve use progressively all the features munin assume the apache log file var log apache access log and its format plain common log format with vhost prefixed logformat quot quot quot quot commonvhost customlog var log apache access log commonvhost simple one here very simple plugin all posix shell bin simple apache plugin quot config quot then cat eof graph title simple apache plugin show hits hits 2xx label hits 2xx hits 3xx label hits 3xx hits 4xx label hits 4xx hits 5xx label hits 5xx eof hits 2xx grep quot var log apache access log hits 3xx grep quot var log apache access log hits 4xx grep quot var log apache access log hits 5xx grep quot var log apache access log now emit the values the value part important and should not omitted echo quot hits 2xx value hits 2xx quot echo quot hits 3xx value hits 3xx quot echo quot hits 4xx value hits 4xx quot echo quot hits 5xx value hits 5xx quot","a:1:{s:3:\"tag\";a:3:{i:0;s:8:\"sysadmin\";i:1;s:8:\"tutorial\";i:2;s:5:\"munin\";}}","-2","0","1","0","0","0","0"
"381574","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-03-31 18:19:00","Europe/Paris","2009-03-31 11:03:55","2010-04-13 16:28:42","","post","wiki","2009/03/Databases:-Version-History","en","Databases: Version History","","","One concept gaining huge momentum lately is file versionning (mostly [Git|http://en.wikipedia.org/wiki/Git_(software)|en] and [Subversion|http://en.wikipedia.org/wiki/Subversion_(software)|en]). It is quite interesting to track the evolution of the data contained in the files, and not only the last time the file was updated.\r\n\r\nOn the last post [Databases: Meta-Data|/post/2009/03/Databases%3A-Meta-Data-ctime-mtime] I was discussing about the merits of having a modification timestamp among with other various informations and more generally about the parallels between a database and a filesystem.\r\n\r\nWe can also adapt this model to our database by not only storing the last modification date, but also the old data.\r\n\r\nTwo main options are : \r\n* replicate the whole row in a ''history table''\r\n* have a generic ''history table'' that only store the old values of the column that have changed\r\n\r\nThe approach is done in the way [this ddj database article|http://www.ddj.com/database/184406340], but in a much less intrusive manner. We just have our trigger-based system evolve with every modification logging an insertion in the derived history table. \r\n\r\nThe history table and trigger become : \r\n\r\n///\r\nTABLE MD_ORDERS_HISTORY ( \r\n   SERIAL MD_ORDERS_HISTORY_ID PRIMARY KEY,\r\n   TIMESTAMP CTIME,\r\n   CHAR TYPE DEFAULT 'M',\r\n   INT ORDERS_ID FOREIGN_KEY ON ORDERS(ORDERS_ID),\r\n   MONEY PRICE_ADJUSTMENT\r\n)\r\n\r\nCREATE INSERT TRIGGER ON ORDERS o\r\nBEGIN\r\n  INSERT INTO MD_ORDERS (ORDERS_ID) VALUES (o.ORDERS_ID)\r\nEND\r\n\r\nCREATE UPDATE TRIGGER ON ORDERS o\r\nBEGIN\r\n  UPDATE MD_ORDERS SET MTIME=NOW(), MLOGIN=CURRENT_USER() \r\n    WHERE ORDERS_ID = o.ORDERS_ID\r\n  -- Log the change\r\n  INSERT INTO MD_ORDERS_HISTORY (ORDERS_ID, PRICE_ADJUSTMENT) \r\n    VALUES (o.ORDERS_ID, o.PRICE_ADJUSTMENT)\r\nEND\r\n\r\nCREATE DELETE TRIGGER ON ORDERS o\r\nBEGIN\r\n  -- Log the delete\r\n  INSERT INTO MD_ORDERS_HISTORY (ORDERS_ID, PRICE_ADJUSTMENT, TYPE) \r\n    VALUES (o.ORDERS_ID, o.PRICE_ADJUSTMENT, 'D')\r\nEND\r\n///\r\n\r\nThe interesting point is that you can now travel back in time in order to see what happened to a row in case of debugging the application or its (mis)usage.\r\n\r\nThe main issue of this system is that your database usage will be larger by several orders of magnitude, depending of how often your application update its data. \r\n\r\nSeveral possibilities to limit the size exists : \r\n\r\n* Partitioning the historical data, and storing it on a slower (cheaper) array.\r\n* Pruning the old data since depending on the application, traveling back 1 whole year may be overkill\r\n* Live aggregating changes. If the data was modified less than 1 hour ago, just update the last history line. We can even have a @@history_start@@ timestamp and @@history_stop@@ timestamp to show that a aggregation has taken place.\r\n* Deferred aggregating changes. Like the live one, but on a scheduled basis. It can even have a dynamic granularity (a granularity of 1 day if the data is 1 year old, 1 hour if it is 1 month old).","<p>One concept gaining huge momentum lately is file versionning (mostly\n<a href=\"http://en.wikipedia.org/wiki/Git_(software)\" hreflang=\"en\">Git</a> and\n<a href=\"http://en.wikipedia.org/wiki/Subversion_(software)\" hreflang=\"en\">Subversion</a>). It is quite interesting to track the evolution of the\ndata contained in the files, and not only the last time the file was\nupdated.</p>\n<p>On the last post <a href=\"/post/2009/03/Databases%3A-Meta-Data-ctime-mtime\">Databases: Meta-Data</a> I\nwas discussing about the merits of having a modification timestamp among with\nother various informations and more generally about the parallels between a\ndatabase and a filesystem.</p>\n<p>We can also adapt this model to our database by not only storing the last\nmodification date, but also the old data.</p>\n<p>Two main options are :</p>\n<ul>\n<li>replicate the whole row in a <em>history table</em></li>\n<li>have a generic <em>history table</em> that only store the old values of the\ncolumn that have changed</li>\n</ul>\n<p>The approach is done in the way <a href=\"http://www.ddj.com/database/184406340\">this ddj database article</a>, but in a\nmuch less intrusive manner. We just have our trigger-based system evolve with\nevery modification logging an insertion in the derived history table.</p>\n<p>The history table and trigger become :</p>\n<pre>\nTABLE MD_ORDERS_HISTORY ( \n   SERIAL MD_ORDERS_HISTORY_ID PRIMARY KEY,\n   TIMESTAMP CTIME,\n   CHAR TYPE DEFAULT 'M',\n   INT ORDERS_ID FOREIGN_KEY ON ORDERS(ORDERS_ID),\n   MONEY PRICE_ADJUSTMENT\n)\n\nCREATE INSERT TRIGGER ON ORDERS o\nBEGIN\n  INSERT INTO MD_ORDERS (ORDERS_ID) VALUES (o.ORDERS_ID)\nEND\n\nCREATE UPDATE TRIGGER ON ORDERS o\nBEGIN\n  UPDATE MD_ORDERS SET MTIME=NOW(), MLOGIN=CURRENT_USER() \n    WHERE ORDERS_ID = o.ORDERS_ID\n  -- Log the change\n  INSERT INTO MD_ORDERS_HISTORY (ORDERS_ID, PRICE_ADJUSTMENT) \n    VALUES (o.ORDERS_ID, o.PRICE_ADJUSTMENT)\nEND\n\nCREATE DELETE TRIGGER ON ORDERS o\nBEGIN\n  -- Log the delete\n  INSERT INTO MD_ORDERS_HISTORY (ORDERS_ID, PRICE_ADJUSTMENT, TYPE) \n    VALUES (o.ORDERS_ID, o.PRICE_ADJUSTMENT, 'D')\nEND\n</pre>\n<p>The interesting point is that you can now travel back in time in order to\nsee what happened to a row in case of debugging the application or its\n(mis)usage.</p>\n<p>The main issue of this system is that your database usage will be larger by\nseveral orders of magnitude, depending of how often your application update its\ndata.</p>\n<p>Several possibilities to limit the size exists :</p>\n<ul>\n<li>Partitioning the historical data, and storing it on a slower (cheaper)\narray.</li>\n<li>Pruning the old data since depending on the application, traveling back 1\nwhole year may be overkill</li>\n<li>Live aggregating changes. If the data was modified less than 1 hour ago,\njust update the last history line. We can even have a\n<code>history_start</code> timestamp and <code>history_stop</code> timestamp to\nshow that a aggregation has taken place.</li>\n<li>Deferred aggregating changes. Like the live one, but on a scheduled basis.\nIt can even have a dynamic granularity (a granularity of 1 day if the data is 1\nyear old, 1 hour if it is 1 month old).</li>\n</ul>","","databases version history one concept gaining huge momentum lately file versionning mostly git and subversion quite interesting track the evolution the data contained the files and not only the last time the file was updated the last post databases meta data was discussing about the merits having modification timestamp among with other various informations and more generally about the parallels between database and filesystem can also adapt this model our database not only storing the last modification date but also the old data two main options are replicate the whole row history table have generic history table that only store the old values the column that have changed the approach done the way this ddj database article but much less intrusive manner just have our trigger based system evolve with every modification logging insertion the derived history table the history table and trigger become table orders history serial orders history primary key timestamp ctime char type default int orders foreign key orders orders money price adjustment create insert trigger orders begin insert into orders orders values orders end create update trigger orders begin update orders set mtime now mlogin current user where orders orders log the change insert into orders history orders price adjustment values orders price adjustment end create delete trigger orders begin log the delete insert into orders history orders price adjustment type values orders price adjustment end the interesting point that you can now travel back time order see what happened row case debugging the application its mis usage the main issue this system that your database usage will larger several orders magnitude depending how often your application update its data several possibilities limit the size exists partitioning the historical data and storing slower cheaper array pruning the old data since depending the application traveling back whole year may overkill live aggregating changes the data was modified less than hour ago just update the last history line can even have history start timestamp and history stop timestamp show that aggregation has taken place deferred aggregating changes like the live one but scheduled basis can even have dynamic granularity granularity day the data year old hour month old","a:0:{}","1","0","1","0","1","0","0"
"426279","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-09-08 22:00:00","Europe/Paris","2009-08-12 15:28:49","2009-09-09 11:07:30","","post","wiki","2009/09/Databases:-Efficient-Case-insensitive-searches-with-Function-based-Indexing","en","Databases: Efficient Case-insensitive searches with Function-based Indexing","Doing a __case insensitive search__ is a very __common task__, but is quite __hard to optimize correctly__. But since it's done via a @@UPPER(MY_COLUMN) = UPPER('MY_DATA')@@, it doesn't use the index that could be on @@MY_COLUMN@@.\r\n\r\nDifferent RDMS means different approaches.","<p>Doing a <strong>case insensitive search</strong> is a very <strong>common\ntask</strong>, but is quite <strong>hard to optimize correctly</strong>. But\nsince it's done via a <code>UPPER(MY_COLUMN) = UPPER('MY_DATA')</code>, it\ndoesn't use the index that could be on <code>MY_COLUMN</code>.</p>\n<p>Different RDMS means different approaches.</p>","!!!! Special case of case-insensitive search\r\n\r\nIn Oracle10g, you might use the new case-insensitive search with a @@[NLS_SORT=BINARY_CI|http://www.dba-oracle.com/t_oracle10g_release_2_case_insensitive_searches.htm|en]@@ command. \r\n\r\n!!! Pro\r\n\r\n* Designed for this purpose, so it's very straightforward to use \r\n\r\n!!! Con\r\n\r\n* Limited to case-insensitive searching by design\r\n\r\n!!!! Native functional indexes\r\n\r\nSome databases provides native functional indexes. \r\n\r\nOn these databases optimization is done simply by creating an index on @@UPPER(MY_COLUMN)@@ and letting the query optimizer transparently$$You might have to update some kind of statistics for the new index$$ use the newly created index.\r\n\r\nIt usually work by applies a function to the data just before handing it to the index, so the function output doesn't exist in the database.\r\n\r\n!!! Pro\r\n\r\n* __Very__ easy to use : it just feels right (you can naïvely create an index on the WHERE clause)\r\n* Doesn't take any extra space in the database (only the index).\r\n* Generic, can be used for something else than just case-insensitive searches.\r\n\r\n!!! Con\r\n\r\n* Since the data isn't stored in the database, a call to the function has to be made when \r\n* Functions have to be from the immutable category in the [function volatility categories|http://www.postgresql.org/docs/8.3/static/xfunc-volatility.html|en]\r\n\r\n!!!! Generated columns\r\n\r\nDB2 provides something called [generated columns|http://publib.boulder.ibm.com/infocenter/db2luw/v8/topic/com.ibm.db2.udb.doc/ad/c0007004.htm|en].  It's almost the same than the native indexes, except that the functional column is explicit.\r\n\r\n!!! Pro\r\n\r\n* Quite easy to use, since the column is updated and used transparently.\r\n* Generic, can be used for something else than just case-insensitive searches. Just make sure the optimizer uses the extra column. You might have to rewrite the request a little.\r\n\r\n!!! Con\r\n\r\n* Requires extra space in the table.\r\n* Removing a column can be cumbersome (in DB2 you have to recreate the whole table for example), whereas removing a simple index is much easier.\r\n* The extra column is returned when doing a @@SELECT * FROM ...@@$$But you don't do that anyway, do you ?$$.\r\n\r\n!!!! Trigger-based ''Generated columns'' Emulation\r\n\r\nIf nothing else is provided, you always have the option to emulate. The solution will be trigger-based since it's one of the few perfect match for them. \r\n\r\nSo, the base idea is derived from the ''Generated columns'' : have a special extra column that represents the output of the function. An index will be created on this column and used via a ''manual'' update of the involved requests (Adding an extra @@WHERE@@ clause should be more than enough, this way you might even benefit from a partial match).\r\n\r\n!!! Pro\r\n\r\n* Universal. Useful if portability is paramount.\r\n* Very simple : there is no need to understand advanced database features.\r\n\r\n!!! Con\r\n\r\n* Only a poor man's solution : everything is manual\r\n* The same than ''Generated columns'' since it's the same idea, just manually implemented.","<h2>Special case of case-insensitive search</h2>\n<p>In Oracle10g, you might use the new case-insensitive search with a\n<code><a href=\"http://www.dba-oracle.com/t_oracle10g_release_2_case_insensitive_searches.htm\" hreflang=\"en\">NLS_SORT=BINARY_CI</a></code> command.</p>\n<h3>Pro</h3>\n<ul>\n<li>Designed for this purpose, so it's very straightforward to use</li>\n</ul>\n<h3>Con</h3>\n<ul>\n<li>Limited to case-insensitive searching by design</li>\n</ul>\n<h2>Native functional indexes</h2>\n<p>Some databases provides native functional indexes.</p>\n<p>On these databases optimization is done simply by creating an index on\n<code>UPPER(MY_COLUMN)</code> and letting the query optimizer\ntransparently<sup>[<a href=\"#pnote-426279-1\" id=\"rev-pnote-426279-1\" name=\"rev-pnote-426279-1\">1</a>]</sup> use the newly created index.</p>\n<p>It usually work by applies a function to the data just before handing it to\nthe index, so the function output doesn't exist in the database.</p>\n<h3>Pro</h3>\n<ul>\n<li><strong>Very</strong> easy to use : it just feels right (you can naïvely\ncreate an index on the WHERE clause)</li>\n<li>Doesn't take any extra space in the database (only the index).</li>\n<li>Generic, can be used for something else than just case-insensitive\nsearches.</li>\n</ul>\n<h3>Con</h3>\n<ul>\n<li>Since the data isn't stored in the database, a call to the function has to\nbe made when</li>\n<li>Functions have to be from the immutable category in the <a href=\"http://www.postgresql.org/docs/8.3/static/xfunc-volatility.html\" hreflang=\"en\">function volatility categories</a></li>\n</ul>\n<h2>Generated columns</h2>\n<p>DB2 provides something called <a href=\"http://publib.boulder.ibm.com/infocenter/db2luw/v8/topic/com.ibm.db2.udb.doc/ad/c0007004.htm\" hreflang=\"en\">generated columns</a>. It's almost the same than the native\nindexes, except that the functional column is explicit.</p>\n<h3>Pro</h3>\n<ul>\n<li>Quite easy to use, since the column is updated and used transparently.</li>\n<li>Generic, can be used for something else than just case-insensitive\nsearches. Just make sure the optimizer uses the extra column. You might have to\nrewrite the request a little.</li>\n</ul>\n<h3>Con</h3>\n<ul>\n<li>Requires extra space in the table.</li>\n<li>Removing a column can be cumbersome (in DB2 you have to recreate the whole\ntable for example), whereas removing a simple index is much easier.</li>\n<li>The extra column is returned when doing a <code>SELECT * FROM\n...</code><sup>[<a href=\"#pnote-426279-2\" id=\"rev-pnote-426279-2\" name=\"rev-pnote-426279-2\">2</a>]</sup>.</li>\n</ul>\n<h2>Trigger-based <em>Generated columns</em> Emulation</h2>\n<p>If nothing else is provided, you always have the option to emulate. The\nsolution will be trigger-based since it's one of the few perfect match for\nthem.</p>\n<p>So, the base idea is derived from the <em>Generated columns</em> : have a\nspecial extra column that represents the output of the function. An index will\nbe created on this column and used via a <em>manual</em> update of the involved\nrequests (Adding an extra <code>WHERE</code> clause should be more than enough,\nthis way you might even benefit from a partial match).</p>\n<h3>Pro</h3>\n<ul>\n<li>Universal. Useful if portability is paramount.</li>\n<li>Very simple : there is no need to understand advanced database\nfeatures.</li>\n</ul>\n<h3>Con</h3>\n<ul>\n<li>Only a poor man's solution : everything is manual</li>\n<li>The same than <em>Generated columns</em> since it's the same idea, just\nmanually implemented.</li>\n</ul>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-426279-1\" id=\"pnote-426279-1\" name=\"pnote-426279-1\">1</a>] You might have to update some kind of statistics for\nthe new index</p>\n<p>[<a href=\"#rev-pnote-426279-2\" id=\"pnote-426279-2\" name=\"pnote-426279-2\">2</a>] But you don't do that anyway, do you ?</p>\n</div>","","databases efficient case insensitive searches with function based indexing doing case insensitive search very common task but quite hard optimize correctly but since done via upper column upper data doesn use the index that could column different rdms means different approaches special case case insensitive search oracle10g you might use the new case insensitive search with nls sort binary command pro designed for this purpose very straightforward use con limited case insensitive searching design native functional indexes some databases provides native functional indexes these databases optimization done simply creating index upper column and letting the query optimizer transparently use the newly created index usually work applies function the data just before handing the index the function output doesn exist the database pro very easy use just feels right you can naïvely create index the where clause doesn take any extra space the database only the index generic can used for something else than just case insensitive searches con since the data isn stored the database call the function has made when functions have from the immutable category the function volatility categories generated columns db2 provides something called generated columns almost the same than the native indexes except that the functional column explicit pro quite easy use since the column updated and used transparently generic can used for something else than just case insensitive searches just make sure the optimizer uses the extra column you might have rewrite the request little con requires extra space the table removing column can cumbersome db2 you have recreate the whole table for example whereas removing simple index much easier the extra column returned when doing select from trigger based generated columns emulation nothing else provided you always have the option emulate the solution will trigger based since one the few perfect match for them the base idea derived from the generated columns have special extra column that represents the output the function index will created this column and used via manual update the involved requests adding extra where clause should more than enough this way you might even benefit from partial match pro universal useful portability paramount very simple there need understand advanced database features con only poor man solution everything manual the same than generated columns since the same idea just manually implemented notes you might have update some kind statistics for the new index but you don that anyway you","a:0:{}","1","0","1","0","0","0","0"
"439765","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6847","2009-09-09 14:31:00","Europe/Paris","2009-09-09 12:31:08","2009-09-09 12:31:08","","post","wiki","2009/09/Using-a-GRAFCET-for-workflow-modelling","en","Using a GRAFCET for workflow modelling","","","A nice parallel between GRAFCET & BPM\nhttp://azrulhasni.blogspot.com/2007/12/grafcet-and-resource-state-management.html","<p>A nice parallel between GRAFCET &amp; BPM\nhttp://azrulhasni.blogspot.com/2007/12/grafcet-and-resource-state-management.html</p>","","using grafcet for workflow modelling nice parallel between grafcet amp bpm http azrulhasni blogspot com 2007 grafcet and resource state management html","","-2","0","1","0","0","0","0"
"474846","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2010-01-12 14:58:00","Europe/Paris","2010-01-12 13:58:55","2010-01-12 14:15:56","","post","wiki","2010/01/Easy-Distributed-SVN-on-a-USB-Stick-With-SVK","en","Easy Distributed SVN on a USB Stick With SVK","","","# setup SVK as usual with FSFS (BDB might work, but why bother ?)\r\n# move $HOME/.svk/local (the svn repo) to your USB stick \r\n# use of depotmap, describes by the author and by a tutorial\r\n# edit $HOME/.svk/config to point \"depotmap\" to the moved local svn repos\r\n# Repeat the same process (but not copying the repo to the stick) on each of you computer.\r\n# Enjoy distributed work.\r\n\r\nDistributed Version Control with svk\r\nBy Chia-liang Kao\r\nhttp://www.perl.com/lpt/a/822\r\n\r\nSVKQuickStart \r\nhttp://code.google.com/p/google-gadgets-for-linux/wiki/SVKQuickStart\r\n\r\nSVK – Distributed Version Control – Part II\r\nhttp://www.bieberlabs.com/archives/2004/12/31/svk-distributed-version-control-part-ii/","<ol>\n<li>setup SVK as usual with FSFS (BDB might work, but why bother ?)</li>\n<li>move $HOME/.svk/local (the svn repo) to your USB stick</li>\n<li>use of depotmap, describes by the author and by a tutorial</li>\n<li>edit $HOME/.svk/config to point &quot;depotmap&quot; to the moved local svn\nrepos</li>\n<li>Repeat the same process (but not copying the repo to the stick) on each of\nyou computer.</li>\n<li>Enjoy distributed work.</li>\n</ol>\n<p>Distributed Version Control with svk By Chia-liang Kao\nhttp://www.perl.com/lpt/a/822</p>\n<p>SVKQuickStart\nhttp://code.google.com/p/google-gadgets-for-linux/wiki/SVKQuickStart</p>\n<p>SVK – Distributed Version Control – Part II\nhttp://www.bieberlabs.com/archives/2004/12/31/svk-distributed-version-control-part-ii/</p>","","easy distributed svn usb stick with svk setup svk usual with fsfs bdb might work but why bother move home svk local the svn repo your usb stick use depotmap describes the author and tutorial edit home svk config point quot depotmap quot the moved local svn repos repeat the same process but not copying the repo the stick each you computer enjoy distributed work distributed version control with svk chia liang kao http www perl com lpt 822 svkquickstart http code google com google gadgets for linux wiki svkquickstart svk distributed version control part http www bieberlabs com archives 2004 svk distributed version control part","","-2","0","1","0","0","0","0"
"526512","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2010-06-26 16:07:00","Europe/Paris","2010-06-13 07:07:55","2010-06-26 15:27:27","","post","wiki","2010/06/Waiting-for-Munin-2.0-Performance-Asynchronous-updates","en","Waiting for Munin 2.0 - Performance - Asynchronous updates","","","@@munin-update@@ is __the__ fragile link in the munin architecture. A __missed execution__ means that some __data is lost__. \r\n\r\n!!!! The problem : updates are synchronous\r\n\r\nIn Munin 1.x, updates are synchronous : the value of each service$$in 1.2 it's the same as plugin, but since 1.4 and the introduction of ++multigraph++, one plugin can provide multiple ++services++.$$ is the one that @@munin-update@@ retrieves each scheduled run. \r\n\r\nThe issue is that @@munin-update@@ has to ask every service on every node for their values. Since the values are only computed when asked, @@munin-update@@ has to wait quite some time for every value. \r\n\r\nThis __very simple design__ enables munin to have the __simplest plugins__ : they are completely __stateless__. While being one great strength of munin, it puts a severe blow on scalability : more plugins/node means obviously a slower retrieval. \r\n\r\n!!!! Evolving Solutions\r\n\r\n!!! 1.4 : Parallel Fetching\r\n\r\n1.4 addresses some of these scalability issues by implementing parallel fetching. It takes into account that the __most of the execution time__ of @@munin-update@@ is spent __waiting for replies__. In 1.4 @@munin-update@@ can ask @@max_processes@@ nodes in parallel.\r\n\r\nNow, the I/O part is becoming the next limiting factor, since updating many RRDs in __parallel__ is the same as __random I/O__ access for the underlying munin-master OS. Serializing & grouping the updates will be possible with the new RRDp interface from rrdtool version 1.4 and on-demand graphing. Tomas Zvala  even offered [a patch for 1.4 RRDp|http://sourceforge.net/mailarchive/message.php?msg_name=4BEDB877.9050004%40zvala.cz|en] on the ML. It is very promising, but doesn't address the root defect in this design : a hard dependence of regular @@munin-update@@ runs.\r\n\r\n!!! 2.0 : Stateful plugins\r\n\r\n2.0 provides a way for plugins to be stateful. They might schedule their polling themselves, and then when @@munin-update@@ runs, only emit collect already computed values. This way, a __missed run__ isn't as dramatic as it is in the 1.x series, since __data isn't lost__. The data collection is also __much faster__ because the real computing is done __ahead of time__.\r\n\r\n!!! 2.0 : Asynchronous proxy node\r\n\r\nBut __changing plugins__ to be __stateful__ and self-polled is __difficult__ and tedious. __It even works against of one of the real strength of munin__ : having simple & stateless plugins. \r\n\r\nTo address this concern,  an experimental proxy node is created. For 2.0 it takes the form of a couple of processes : @@munin-async-server@@ and @@munin-sync-client@@. \r\n\r\n!!!! The proxy node in detail (@@munin-async@@)\r\n\r\n!!! Overview\r\n\r\nThese 2 processes form an asynchronous proxy between @@munin-update@@ and @@munin-node@@. This avoids the need to change the plugins or upgrade @@munin-node@@ on all nodes. \r\n\r\n@@munin-async-server@@ should be installed on the same host than the proxied @@munin-node@@ in order to avoid any network issue. It is the process that will poll regularly @@munin-node@@. The I/O issue of @@munin-update@@ is here non-existent, since @@munin-async@@ stores all the values by simply appending them in a text file without any further processing. This file is later read by the client's @@munin-update@@, and it will be processed there.\r\n\r\n!!! Specific update rates\r\n\r\nHaving one proxy per node enables a polling of all the services there with a specific update rate. \r\n\r\nTo achieve this, @@munin-async-server@@ forks into multiple processes, one for each proxied service. This way each service is completely isolated from the other, and therefore is able to have its own update rate, is safe from other plugins slowdowns, and it does even completely parallelize the information gathering.\r\n\r\n!!! SSH transport\r\n\r\n@@munin-async-client@@ uses the new SSH native transport of 2.0. It permits a very simple install of the async proxy.","<p><code>munin-update</code> is <strong>the</strong> fragile link in the munin\narchitecture. A <strong>missed execution</strong> means that some <strong>data\nis lost</strong>.</p>\n<h2>The problem : updates are synchronous</h2>\n<p>In Munin 1.x, updates are synchronous : the value of each\nservice<sup>[<a href=\"#pnote-526512-1\" id=\"rev-pnote-526512-1\" name=\"rev-pnote-526512-1\">1</a>]</sup> is the one that <code>munin-update</code>\nretrieves each scheduled run.</p>\n<p>The issue is that <code>munin-update</code> has to ask every service on\nevery node for their values. Since the values are only computed when asked,\n<code>munin-update</code> has to wait quite some time for every value.</p>\n<p>This <strong>very simple design</strong> enables munin to have the\n<strong>simplest plugins</strong> : they are completely\n<strong>stateless</strong>. While being one great strength of munin, it puts a\nsevere blow on scalability : more plugins/node means obviously a slower\nretrieval.</p>\n<h2>Evolving Solutions</h2>\n<h3>1.4 : Parallel Fetching</h3>\n<p>1.4 addresses some of these scalability issues by implementing parallel\nfetching. It takes into account that the <strong>most of the execution\ntime</strong> of <code>munin-update</code> is spent <strong>waiting for\nreplies</strong>. In 1.4 <code>munin-update</code> can ask\n<code>max_processes</code> nodes in parallel.</p>\n<p>Now, the I/O part is becoming the next limiting factor, since updating many\nRRDs in <strong>parallel</strong> is the same as <strong>random I/O</strong>\naccess for the underlying munin-master OS. Serializing &amp; grouping the\nupdates will be possible with the new RRDp interface from rrdtool version 1.4\nand on-demand graphing. Tomas Zvala even offered <a href=\"http://sourceforge.net/mailarchive/message.php?msg_name=4BEDB877.9050004%40zvala.cz\" hreflang=\"en\">a patch for 1.4 RRDp</a> on the ML. It is very promising, but\ndoesn't address the root defect in this design : a hard dependence of regular\n<code>munin-update</code> runs.</p>\n<h3>2.0 : Stateful plugins</h3>\n<p>2.0 provides a way for plugins to be stateful. They might schedule their\npolling themselves, and then when <code>munin-update</code> runs, only emit\ncollect already computed values. This way, a <strong>missed run</strong> isn't\nas dramatic as it is in the 1.x series, since <strong>data isn't lost</strong>.\nThe data collection is also <strong>much faster</strong> because the real\ncomputing is done <strong>ahead of time</strong>.</p>\n<h3>2.0 : Asynchronous proxy node</h3>\n<p>But <strong>changing plugins</strong> to be <strong>stateful</strong> and\nself-polled is <strong>difficult</strong> and tedious. <strong>It even works\nagainst of one of the real strength of munin</strong> : having simple &amp;\nstateless plugins.</p>\n<p>To address this concern, an experimental proxy node is created. For 2.0 it\ntakes the form of a couple of processes : <code>munin-async-server</code> and\n<code>munin-sync-client</code>.</p>\n<h2>The proxy node in detail (<code>munin-async</code>)</h2>\n<h3>Overview</h3>\n<p>These 2 processes form an asynchronous proxy between\n<code>munin-update</code> and <code>munin-node</code>. This avoids the need to\nchange the plugins or upgrade <code>munin-node</code> on all nodes.</p>\n<p><code>munin-async-server</code> should be installed on the same host than\nthe proxied <code>munin-node</code> in order to avoid any network issue. It is\nthe process that will poll regularly <code>munin-node</code>. The I/O issue of\n<code>munin-update</code> is here non-existent, since <code>munin-async</code>\nstores all the values by simply appending them in a text file without any\nfurther processing. This file is later read by the client's\n<code>munin-update</code>, and it will be processed there.</p>\n<h3>Specific update rates</h3>\n<p>Having one proxy per node enables a polling of all the services there with a\nspecific update rate.</p>\n<p>To achieve this, <code>munin-async-server</code> forks into multiple\nprocesses, one for each proxied service. This way each service is completely\nisolated from the other, and therefore is able to have its own update rate, is\nsafe from other plugins slowdowns, and it does even completely parallelize the\ninformation gathering.</p>\n<h3>SSH transport</h3>\n<p><code>munin-async-client</code> uses the new SSH native transport of 2.0. It\npermits a very simple install of the async proxy.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-526512-1\" id=\"pnote-526512-1\" name=\"pnote-526512-1\">1</a>] in 1.2 it's the same as plugin, but since 1.4 and the\nintroduction of <ins>multigraph</ins>, one plugin can provide multiple\n<ins>services</ins>.</p>\n</div>","","waiting for munin performance asynchronous updates munin update the fragile link the munin architecture missed execution means that some data lost the problem updates are synchronous munin updates are synchronous the value each service the one that munin update retrieves each scheduled run the issue that munin update has ask every service every node for their values since the values are only computed when asked munin update has wait quite some time for every value this very simple design enables munin have the simplest plugins they are completely stateless while being one great strength munin puts severe blow scalability more plugins node means obviously slower retrieval evolving solutions parallel fetching addresses some these scalability issues implementing parallel fetching takes into account that the most the execution time munin update spent waiting for replies munin update can ask max processes nodes parallel now the part becoming the next limiting factor since updating many rrds parallel the same random access for the underlying munin master serializing amp grouping the updates will possible with the new rrdp interface from rrdtool version and demand graphing tomas zvala even offered patch for rrdp the very promising but doesn address the root defect this design hard dependence regular munin update runs stateful plugins provides way for plugins stateful they might schedule their polling themselves and then when munin update runs only emit collect already computed values this way missed run isn dramatic the series since data isn lost the data collection also much faster because the real computing done ahead time asynchronous proxy node but changing plugins stateful and self polled difficult and tedious even works against one the real strength munin having simple amp stateless plugins address this concern experimental proxy node created for takes the form couple processes munin async server and munin sync client the proxy node detail munin async overview these processes form asynchronous proxy between munin update and munin node this avoids the need change the plugins upgrade munin node all nodes munin async server should installed the same host than the proxied munin node order avoid any network issue the process that will poll regularly munin node the issue munin update here non existent since munin async stores all the values simply appending them text file without any further processing this file later read the client munin update and will processed there specific update rates having one proxy per node enables polling all the services there with specific update rate achieve this munin async server forks into multiple processes one for each proxied service this way each service completely isolated from the other and therefore able have its own update rate safe from other plugins slowdowns and does even completely parallelize the information gathering ssh transport munin async client uses the new ssh native transport permits very simple install the async proxy notes the same plugin but since and the introduction multigraph one plugin can provide multiple services","a:1:{s:3:\"tag\";a:3:{i:0;s:11:\"performance\";i:1;s:7:\"munin20\";i:2;s:5:\"munin\";}}","1","0","1","0","3","0","0"
"459477","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2009-11-14 12:18:00","Europe/Paris","2009-11-12 10:11:18","2009-11-16 07:26:10","","post","wiki","2009/11/Sed-is-much-slower-than-Perl-or-not...","en","Sed is much slower than Perl, or not...","","","I wanted to do some text replacement with a __huge__ file (think \~18GiB), filled with __huge__ lines (think \~2MiB per ligne)$$For the record, it was a MySQL dump$$.\r\n\r\nI naïvely piped it through @@sed@@ and I was quite shocked that it was CPU bound, and not I/O bound. The average rate was about 5 MiB/s (measured with [pv|http://www.ivarch.com/programs/pv.shtml|en], and the CPU was at almost 100%.The text file was gzipped on the filesystem, but with a 1/100 ratio, so the gzip process just took less than 2% CPU. I replaced then the @@sed -e@@ with the Perl one-liner @@perl -lnpe@@, and .... ''tadaa'', it was flying at a rate of  50MiB/s !\r\n\r\nWhile I'm a big fan of Perl, and know its effectiveness to handle text streams, I'm was still astonished : being 10x faster than sed was something. \r\n\r\nBut in the good old saying {{Too good to be true means suspect}}, I remembered something about the character encoding of the regular expression. Since the system is entirely configured in UTF8, I suspected the ''infamous'' UTF8 overhead over plain ASCII. \r\n\r\nI was right : a little @@LANG=C@@ in front of the sed command line restored the rate to 50MiB/s.\r\n\r\nSo, __beware__ of the __performance impact__ of __UTF8__ strings, and try to avoid it if you can.","<p>I wanted to do some text replacement with a <strong>huge</strong> file\n(think ~18GiB), filled with <strong>huge</strong> lines (think ~2MiB per\nligne)<sup>[<a href=\"#pnote-459477-1\" id=\"rev-pnote-459477-1\" name=\"rev-pnote-459477-1\">1</a>]</sup>.</p>\n<p>I naïvely piped it through <code>sed</code> and I was quite shocked that it\nwas CPU bound, and not I/O bound. The average rate was about 5 MiB/s (measured\nwith <a href=\"http://www.ivarch.com/programs/pv.shtml\" hreflang=\"en\">pv</a>,\nand the CPU was at almost 100%.The text file was gzipped on the filesystem, but\nwith a 1/100 ratio, so the gzip process just took less than 2% CPU. I replaced\nthen the <code>sed -e</code> with the Perl one-liner <code>perl -lnpe</code>,\nand .... <em>tadaa</em>, it was flying at a rate of 50MiB/s !</p>\n<p>While I'm a big fan of Perl, and know its effectiveness to handle text\nstreams, I'm was still astonished : being 10x faster than sed was\nsomething.</p>\n<p>But in the good old saying <q>Too good to be true means suspect</q>, I\nremembered something about the character encoding of the regular expression.\nSince the system is entirely configured in UTF8, I suspected the\n<em>infamous</em> UTF8 overhead over plain ASCII.</p>\n<p>I was right : a little <code>LANG=C</code> in front of the sed command line\nrestored the rate to 50MiB/s.</p>\n<p>So, <strong>beware</strong> of the <strong>performance impact</strong> of\n<strong>UTF8</strong> strings, and try to avoid it if you can.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-459477-1\" id=\"pnote-459477-1\" name=\"pnote-459477-1\">1</a>] For the record, it was a MySQL dump</p>\n</div>","","sed much slower than perl not wanted some text replacement with huge file think 18gib filled with huge lines think 2mib per ligne naïvely piped through sed and was quite shocked that was cpu bound and not bound the average rate was about mib measured with and the cpu was almost 100 the text file was gzipped the filesystem but with 100 ratio the gzip process just took less than cpu replaced then the sed with the perl one liner perl lnpe and tadaa was flying rate 50mib while big fan perl and know its effectiveness handle text streams was still astonished being 10x faster than sed was something but the good old saying too good true means suspect remembered something about the character encoding the regular expression since the system entirely configured utf8 suspected the infamous utf8 overhead over plain ascii was right little lang front the sed command line restored the rate 50mib beware the performance impact utf8 strings and try avoid you can notes for the record was mysql dump","a:1:{s:3:\"tag\";a:3:{i:0;s:3:\"sql\";i:1;s:7:\"locales\";i:2;s:4:\"perl\";}}","1","0","1","0","2","0","0"
"421699","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","","2009-07-28 16:29:00","Europe/Paris","2009-07-28 14:29:25","2009-07-28 14:29:25","","page","xhtml","Google-Analytics-Static-page","en","Google Analytics Static page","","","<script type=\"text/javascript\">\r\nvar _gat=new Object({c:\"length\",lb:\"4.3.1\",m:\"cookie\",b:undefined,cb:function(d,a){this.zb=d;this.Nb=a},r:\"__utma=\",W:\"__utmb=\",ma:\"__utmc=\",Ta:\"__utmk=\",na:\"__utmv=\",oa:\"__utmx=\",Sa:\"GASO=\",X:\"__utmz=\",lc:\"http://www.google-analytics.com/__utm.gif\",mc:\"https://ssl.google-analytics.com/__utm.gif\",Wa:\"utmcid=\",Ya:\"utmcsr=\",$a:\"utmgclid=\",Ua:\"utmccn=\",Xa:\"utmcmd=\",Za:\"utmctr=\",Va:\"utmcct=\",Hb:false,_gasoDomain:undefined,_gasoCPath:undefined,e:window,a:document,k:navigator,t:function(d){var a=1,c=0,h,\r\no;if(!_gat.q(d)){a=0;for(h=d[_gat.c]-1;h>=0;h--){o=d.charCodeAt(h);a=(a<<6&268435455)+o+(o<<14);c=a&266338304;a=c!=0?a^c>>21:a}}return a},C:function(d,a,c){var h=_gat,o=\"-\",k,l,s=h.q;if(!s(d)&&!s(a)&&!s(c)){k=h.w(d,a);if(k>-1){l=d.indexOf(c,k);if(l<0)l=d[h.c];o=h.F(d,k+h.w(a,\"=\")+1,l)}}return o},Ea:function(d){var a=false,c=0,h,o;if(!_gat.q(d)){a=true;for(h=0;h<d[_gat.c];h++){o=d.charAt(h);c+=\".\"==o?1:0;a=a&&c<=1&&(0==h&&\"-\"==o||_gat.P(\".0123456789\",o))}}return a},d:function(d,a){var c=encodeURIComponent;\r\nreturn c instanceof Function?(a?encodeURI(d):c(d)):escape(d)},J:function(d,a){var c=decodeURIComponent,h;d=d.split(\"+\").join(\" \");if(c instanceof Function)try{h=a?decodeURI(d):c(d)}catch(o){h=unescape(d)}else h=unescape(d);return h},Db:function(d){return d&&d.hash?_gat.F(d.href,_gat.w(d.href,\"#\")):\"\"},q:function(d){return _gat.b==d||\"-\"==d||\"\"==d},Lb:function(d){return d[_gat.c]>0&&_gat.P(\" \n\r\t\",d)},P:function(d,a){return _gat.w(d,a)>-1},h:function(d,a){d[d[_gat.c]]=a},T:function(d){return d.toLowerCase()},\r\nz:function(d,a){return d.split(a)},w:function(d,a){return d.indexOf(a)},F:function(d,a,c){c=_gat.b==c?d[_gat.c]:c;return d.substring(a,c)},uc:function(){var d=_gat.b,a=window;if(a&&a.gaGlobal&&a.gaGlobal.hid)d=a.gaGlobal.hid;else{d=Math.round(Math.random()*2147483647);a.gaGlobal=a.gaGlobal?a.gaGlobal:{};a.gaGlobal.hid=d}return d},wa:function(){return Math.round(Math.random()*2147483647)},Gc:function(){return(_gat.wa()^_gat.vc())*2147483647},vc:function(){var d=_gat.k,a=_gat.a,c=_gat.e,h=a[_gat.m]?\r\na[_gat.m]:\"\",o=c.history[_gat.c],k,l,s=[d.appName,d.version,d.language?d.language:d.browserLanguage,d.platform,d.userAgent,d.javaEnabled()?1:0].join(\"\");if(c.screen)s+=c.screen.width+\"x\"+c.screen.height+c.screen.colorDepth;else if(c.java){l=java.awt.Toolkit.getDefaultToolkit().getScreenSize();s+=l.screen.width+\"x\"+l.screen.height}s+=h;s+=a.referrer?a.referrer:\"\";k=s[_gat.c];while(o>0)s+=o--^k++;return _gat.t(s)}});_gat.hc=function(){var d=this,a=_gat.cb;function c(h,o){return new a(h,o)}d.db=\"utm_campaign\";d.eb=\"utm_content\";d.fb=\"utm_id\";d.gb=\"utm_medium\";d.hb=\"utm_nooverride\";d.ib=\"utm_source\";d.jb=\"utm_term\";d.kb=\"gclid\";d.pa=0;d.I=0;d.wb=\"15768000\";d.Tb=\"1800\";d.ea=[];d.ga=[];d.Ic=\"cse\";d.Gb=\"q\";d.ab=\"google\";d.fa=[c(d.ab,d.Gb),c(\"yahoo\",\"p\"),c(\"msn\",\"q\"),c(\"bing\",\"q\"),c(\"aol\",\"query\"),c(\"aol\",\"encquery\"),c(\"lycos\",\"query\"),c(\"ask\",\"q\"),c(\"altavista\",\"q\"),c(\"netscape\",\"query\"),c(\"cnn\",\"query\"),c(\"looksmart\",\"qt\"),c(\"about\",\r\n\"terms\"),c(\"mamma\",\"query\"),c(\"alltheweb\",\"q\"),c(\"gigablast\",\"q\"),c(\"voila\",\"rdata\"),c(\"virgilio\",\"qs\"),c(\"live\",\"q\"),c(\"baidu\",\"wd\"),c(\"alice\",\"qs\"),c(\"yandex\",\"text\"),c(\"najdi\",\"q\"),c(\"aol\",\"q\"),c(\"club-internet\",\"query\"),c(\"mama\",\"query\"),c(\"seznam\",\"q\"),c(\"search\",\"q\"),c(\"wp\",\"szukaj\"),c(\"onet\",\"qt\"),c(\"netsprint\",\"q\"),c(\"google.interia\",\"q\"),c(\"szukacz\",\"q\"),c(\"yam\",\"k\"),c(\"pchome\",\"q\"),c(\"kvasir\",\"searchExpr\"),c(\"sesam\",\"q\"),c(\"ozu\",\"q\"),c(\"terra\",\"query\"),c(\"nostrum\",\"query\"),c(\"mynet\",\"q\"),\r\nc(\"ekolay\",\"q\"),c(\"search.ilse\",\"search_for\")];d.B=undefined;d.Kb=false;d.p=\"/\";d.ha=100;d.Da=\"/__utm.gif\";d.ta=1;d.ua=1;d.G=\"|\";d.sa=1;d.qa=1;d.pb=1;d.g=\"auto\";d.D=1;d.Ga=1000;d.Yc=10;d.nc=10;d.Zc=0.2};_gat.Y=function(d,a){var c,h,o,k,l,s,q,f=this,n=_gat,w=n.q,x=n.c,g,z=a;f.a=d;function B(i){var b=i instanceof Array?i.join(\".\"):\"\";return w(b)?\"-\":b}function A(i,b){var e=[],j;if(!w(i)){e=n.z(i,\".\");if(b)for(j=0;j<e[x];j++)if(!n.Ea(e[j]))e[j]=\"-\"}return e}function p(){return u(63072000000)}function u(i){var b=new Date,e=new Date(b.getTime()+i);return\"expires=\"+e.toGMTString()+\"; \"}function m(i,b){f.a[n.m]=i+\"; path=\"+z.p+\"; \"+b+f.Cc()}function r(i,b,e){var j=f.V,t,v;for(t=0;t<j[x];t++){v=j[t][0];\r\nv+=w(b)?b:b+j[t][4];j[t][2](n.C(i,v,e))}}f.Jb=function(){return n.b==g||g==f.t()};f.Ba=function(){return l?l:\"-\"};f.Wb=function(i){l=i};f.Ma=function(i){g=n.Ea(i)?i*1:\"-\"};f.Aa=function(){return B(s)};f.Na=function(i){s=A(i)};f.Hc=function(){return g?g:\"-\"};f.Cc=function(){return w(z.g)?\"\":\"domain=\"+z.g+\";\"};f.ya=function(){return B(c)};f.Ub=function(i){c=A(i,1)};f.K=function(){return B(h)};f.La=function(i){h=A(i,1)};f.za=function(){return B(o)};f.Vb=function(i){o=A(i,1)};f.Ca=function(){return B(k)};\r\nf.Xb=function(i){k=A(i);for(var b=0;b<k[x];b++)if(b<4&&!n.Ea(k[b]))k[b]=\"-\"};f.Dc=function(){return q};f.Uc=function(i){q=i};f.pc=function(){c=[];h=[];o=[];k=[];l=n.b;s=[];g=n.b};f.t=function(){var i=\"\",b;for(b=0;b<f.V[x];b++)i+=f.V[b][1]();return n.t(i)};f.Ha=function(i){var b=f.a[n.m],e=false;if(b){r(b,i,\";\");f.Ma(f.t());e=true}return e};f.Rc=function(i){r(i,\"\",\"&\");f.Ma(n.C(i,n.Ta,\"&\"))};f.Wc=function(){var i=f.V,b=[],e;for(e=0;e<i[x];e++)n.h(b,i[e][0]+i[e][1]());n.h(b,n.Ta+f.t());return b.join(\"&\")};\r\nf.bd=function(i,b){var e=f.V,j=z.p,t;f.Ha(i);z.p=b;for(t=0;t<e[x];t++)if(!w(e[t][1]()))e[t][3]();z.p=j};f.dc=function(){m(n.r+f.ya(),p())};f.Pa=function(){m(n.W+f.K(),u(z.Tb*1000))};f.ec=function(){m(n.ma+f.za(),\"\")};f.Ra=function(){m(n.X+f.Ca(),u(z.wb*1000))};f.fc=function(){m(n.oa+f.Ba(),p())};f.Qa=function(){m(n.na+f.Aa(),p())};f.cd=function(){m(n.Sa+f.Dc(),\"\")};f.V=[[n.r,f.ya,f.Ub,f.dc,\".\"],[n.W,f.K,f.La,f.Pa,\"\"],[n.ma,f.za,f.Vb,f.ec,\"\"],[n.oa,f.Ba,f.Wb,f.fc,\"\"],[n.X,f.Ca,f.Xb,f.Ra,\".\"],[n.na,\r\nf.Aa,f.Na,f.Qa,\".\"]]};_gat.jc=function(d){var a=this,c=_gat,h=d,o,k=function(l){var s=(new Date).getTime(),q;q=(s-l[3])*(h.Zc/1000);if(q>=1){l[2]=Math.min(Math.floor(l[2]*1+q),h.nc);l[3]=s}return l};a.O=function(l,s,q,f,n,w,x){var g,z=h.D,B=q.location;if(!o)o=new c.Y(q,h);o.Ha(f);g=c.z(o.K(),\".\");if(g[1]<500||n){if(w)g=k(g);if(n||!w||g[2]>=1){if(!n&&w)g[2]=g[2]*1-1;g[1]=g[1]*1+1;l=\"?utmwv=\"+_gat.lb+\"&utmn=\"+c.wa()+(c.q(B.hostname)?\"\":\"&utmhn=\"+c.d(B.hostname))+(h.ha==100?\"\":\"&utmsp=\"+c.d(h.ha))+l;if(0==z||2==z){var A=\r\nnew Image(1,1);A.src=h.Da+l;var p=2==z?function(){}:x||function(){};A.onload=p}if(1==z||2==z){var u=new Image(1,1);u.src=(\"https:\"==B.protocol?c.mc:c.lc)+l+\"&utmac=\"+s+\"&utmcc=\"+a.wc(q,f);u.onload=x||function(){}}}}o.La(g.join(\".\"));o.Pa()};a.wc=function(l,s){var q=[],f=[c.r,c.X,c.na,c.oa],n,w=l[c.m],x;for(n=0;n<f[c.c];n++){x=c.C(w,f[n]+s,\";\");if(!c.q(x))c.h(q,f[n]+x+\";\")}return c.d(q.join(\"+\"))}};_gat.i=function(){this.la=[]};_gat.i.bb=function(d,a,c,h,o,k){var l=this;l.cc=d;l.Oa=a;l.L=c;l.sb=h;l.Pb=o;l.Qb=k};_gat.i.bb.prototype.S=function(){var d=this,a=_gat.d;return\"&\"+[\"utmt=item\",\"utmtid=\"+a(d.cc),\"utmipc=\"+a(d.Oa),\"utmipn=\"+a(d.L),\"utmiva=\"+a(d.sb),\"utmipr=\"+a(d.Pb),\"utmiqt=\"+a(d.Qb)].join(\"&\")};_gat.i.$=function(d,a,c,h,o,k,l,s){var q=this;q.v=d;q.ob=a;q.bc=c;q.ac=h;q.Yb=o;q.ub=k;q.$b=l;q.xb=s;q.ca=[]};_gat.i.$.prototype.mb=function(d,a,c,h,o){var k=this,l=k.Eb(d),s=k.v,q=_gat;if(q.b==\r\nl)q.h(k.ca,new q.i.bb(s,d,a,c,h,o));else{l.cc=s;l.Oa=d;l.L=a;l.sb=c;l.Pb=h;l.Qb=o}};_gat.i.$.prototype.Eb=function(d){var a,c=this.ca,h;for(h=0;h<c[_gat.c];h++)a=d==c[h].Oa?c[h]:a;return a};_gat.i.$.prototype.S=function(){var d=this,a=_gat.d;return\"&\"+[\"utmt=tran\",\"utmtid=\"+a(d.v),\"utmtst=\"+a(d.ob),\"utmtto=\"+a(d.bc),\"utmttx=\"+a(d.ac),\"utmtsp=\"+a(d.Yb),\"utmtci=\"+a(d.ub),\"utmtrg=\"+a(d.$b),\"utmtco=\"+a(d.xb)].join(\"&\")};_gat.i.prototype.nb=function(d,a,c,h,o,k,l,s){var q=this,f=_gat,n=q.xa(d);if(f.b==\r\nn){n=new f.i.$(d,a,c,h,o,k,l,s);f.h(q.la,n)}else{n.ob=a;n.bc=c;n.ac=h;n.Yb=o;n.ub=k;n.$b=l;n.xb=s}return n};_gat.i.prototype.xa=function(d){var a,c=this.la,h;for(h=0;h<c[_gat.c];h++)a=d==c[h].v?c[h]:a;return a};_gat.gc=function(d){var a=this,c=\"-\",h=_gat,o=d;a.Ja=screen;a.qb=!self.screen&&self.java?java.awt.Toolkit.getDefaultToolkit():h.b;a.a=document;a.e=window;a.k=navigator;a.Ka=c;a.Sb=c;a.tb=c;a.Ob=c;a.Mb=1;a.Bb=c;function k(){var l,s,q,f,n=\"ShockwaveFlash\",w=\"$version\",x=a.k?a.k.plugins:h.b;if(x&&x[h.c]>0)for(l=0;l<x[h.c]&&!q;l++){s=x[l];if(h.P(s.name,\"Shockwave Flash\"))q=h.z(s.description,\"Shockwave Flash \")[1]}else{n=n+\".\"+n;try{f=new ActiveXObject(n+\".7\");q=f.GetVariable(w)}catch(g){}if(!q)try{f=\r\nnew ActiveXObject(n+\".6\");q=\"WIN 6,0,21,0\";f.AllowScriptAccess=\"always\";q=f.GetVariable(w)}catch(z){}if(!q)try{f=new ActiveXObject(n);q=f.GetVariable(w)}catch(z){}if(q){q=h.z(h.z(q,\" \")[1],\",\");q=q[0]+\".\"+q[1]+\" r\"+q[2]}}return q?q:c}a.xc=function(){var l;if(self.screen){a.Ka=a.Ja.width+\"x\"+a.Ja.height;a.Sb=a.Ja.colorDepth+\"-bit\"}else if(a.qb)try{l=a.qb.getScreenSize();a.Ka=l.width+\"x\"+l.height}catch(s){}a.Ob=h.T(a.k&&a.k.language?a.k.language:(a.k&&a.k.browserLanguage?a.k.browserLanguage:c));a.Mb=\r\na.k&&a.k.javaEnabled()?1:0;a.Bb=o?k():c;a.tb=h.d(a.a.characterSet?a.a.characterSet:(a.a.charset?a.a.charset:c))};a.Xc=function(){return\"&\"+[\"utmcs=\"+h.d(a.tb),\"utmsr=\"+a.Ka,\"utmsc=\"+a.Sb,\"utmul=\"+a.Ob,\"utmje=\"+a.Mb,\"utmfl=\"+h.d(a.Bb)].join(\"&\")}};_gat.n=function(d,a,c,h,o){var k=this,l=_gat,s=l.q,q=l.b,f=l.P,n=l.C,w=l.T,x=l.z,g=l.c;k.a=a;k.f=d;k.Rb=c;k.ja=h;k.o=o;function z(p){return s(p)||\"0\"==p||!f(p,\"://\")}function B(p){var u=\"\";p=w(x(p,\"://\")[1]);if(f(p,\"/\")){p=x(p,\"/\")[1];if(f(p,\"?\"))u=x(p,\"?\")[0]}return u}function A(p){var u=\"\";u=w(x(p,\"://\")[1]);if(f(u,\"/\"))u=x(u,\"/\")[0];return u}k.Fc=function(p){var u=k.Fb(),m=k.o;return new l.n.s(n(p,m.fb+\"=\",\"&\"),n(p,m.ib+\"=\",\"&\"),n(p,m.kb+\"=\",\"&\"),k.ba(p,m.db,\"(not set)\"),k.ba(p,m.gb,\"(not set)\"),\r\nk.ba(p,m.jb,u&&!s(u.R)?l.J(u.R):q),k.ba(p,m.eb,q))};k.Ib=function(p){var u=A(p),m=B(p);if(f(u,k.o.ab)){p=x(p,\"?\").join(\"&\");if(f(p,\"&\"+k.o.Gb+\"=\"))if(m==k.o.Ic)return true}return false};k.Fb=function(){var p,u,m=k.Rb,r,i,b=k.o.fa;if(z(m)||k.Ib(m))return;p=A(m);for(r=0;r<b[g];r++){i=b[r];if(f(p,w(i.zb))){m=x(m,\"?\").join(\"&\");if(f(m,\"&\"+i.Nb+\"=\")){u=x(m,\"&\"+i.Nb+\"=\")[1];if(f(u,\"&\"))u=x(u,\"&\")[0];return new l.n.s(q,i.zb,q,\"(organic)\",\"organic\",u,q)}}}};k.ba=function(p,u,m){var r=n(p,u+\"=\",\"&\"),i=!s(r)?\r\nl.J(r):(!s(m)?m:\"-\");return i};k.Nc=function(p){var u=k.o.ea,m=false,r,i;if(p&&\"organic\"==p.da){r=w(l.J(p.R));for(i=0;i<u[g];i++)m=m||w(u[i])==r}return m};k.Ec=function(){var p=\"\",u=\"\",m=k.Rb;if(z(m)||k.Ib(m))return;p=w(x(m,\"://\")[1]);if(f(p,\"/\")){u=l.F(p,l.w(p,\"/\"));if(f(u,\"?\"))u=x(u,\"?\")[0];p=x(p,\"/\")[0]}if(0==l.w(p,\"www.\"))p=l.F(p,4);return new l.n.s(q,p,q,\"(referral)\",\"referral\",q,u)};k.sc=function(p){var u=\"\";if(k.o.pa){u=l.Db(p);u=\"\"!=u?u+\"&\":u}u+=p.search;return u};k.zc=function(){return new l.n.s(q,\r\n\"(direct)\",q,\"(direct)\",\"(none)\",q,q)};k.Oc=function(p){var u=false,m,r,i=k.o.ga;if(p&&\"referral\"==p.da){m=w(l.d(p.ia));for(r=0;r<i[g];r++)u=u||f(m,w(i[r]))}return u};k.U=function(p){return q!=p&&p.Fa()};k.yc=function(p,u){var m=\"\",r=\"-\",i,b,e=0,j,t,v=k.f;if(!p)return\"\";t=k.a[l.m]?k.a[l.m]:\"\";m=k.sc(k.a.location);if(k.o.I&&p.Jb()){r=p.Ca();if(!s(r)&&!f(r,\";\")){p.Ra();return\"\"}}r=n(t,l.X+v+\".\",\";\");i=k.Fc(m);if(k.U(i)){b=n(m,k.o.hb+\"=\",\"&\");if(\"1\"==b&&!s(r))return\"\"}if(!k.U(i)){i=k.Fb();if(!s(r)&&\r\nk.Nc(i))return\"\"}if(!k.U(i)&&u){i=k.Ec();if(!s(r)&&k.Oc(i))return\"\"}if(!k.U(i))if(s(r)&&u)i=k.zc();if(!k.U(i))return\"\";if(!s(r)){var y=x(r,\".\"),E=new l.n.s;E.Cb(y.slice(4).join(\".\"));j=w(E.ka())==w(i.ka());e=y[3]*1}if(!j||u){var F=n(t,l.r+v+\".\",\";\"),I=F.lastIndexOf(\".\"),G=I>9?l.F(F,I+1)*1:0;e++;G=0==G?1:G;p.Xb([v,k.ja,G,e,i.ka()].join(\".\"));p.Ra();return\"&utmcn=1\"}else return\"&utmcr=1\"}};_gat.n.s=function(d,a,c,h,o,k,l){var s=this;s.v=d;s.ia=a;s.ra=c;s.L=h;s.da=o;s.R=k;s.vb=l};_gat.n.s.prototype.ka=\r\nfunction(){var d=this,a=_gat,c=[],h=[[a.Wa,d.v],[a.Ya,d.ia],[a.$a,d.ra],[a.Ua,d.L],[a.Xa,d.da],[a.Za,d.R],[a.Va,d.vb]],o,k;if(d.Fa())for(o=0;o<h[a.c];o++)if(!a.q(h[o][1])){k=h[o][1].split(\"+\").join(\"%20\");k=k.split(\" \").join(\"%20\");a.h(c,h[o][0]+k)}return c.join(\"|\")};_gat.n.s.prototype.Fa=function(){var d=this,a=_gat.q;return!(a(d.v)&&a(d.ia)&&a(d.ra))};_gat.n.s.prototype.Cb=function(d){var a=this,c=_gat,h=function(o){return c.J(c.C(d,o,\"|\"))};a.v=h(c.Wa);a.ia=h(c.Ya);a.ra=h(c.$a);a.L=h(c.Ua);a.da=\r\nh(c.Xa);a.R=h(c.Za);a.vb=h(c.Va)};_gat.Z=function(){var d=this,a=_gat,c={},h=\"k\",o=\"v\",k=[h,o],l=\"(\",s=\")\",q=\"*\",f=\"!\",n=\"'\",w={};w[n]=\"'0\";w[s]=\"'1\";w[q]=\"'2\";w[f]=\"'3\";var x=1;function g(m,r,i,b){if(a.b==c[m])c[m]={};if(a.b==c[m][r])c[m][r]=[];c[m][r][i]=b}function z(m,r,i){return a.b!=c[m]&&a.b!=c[m][r]?c[m][r][i]:a.b}function B(m,r){if(a.b!=c[m]&&a.b!=c[m][r]){c[m][r]=a.b;var i=true,b;for(b=0;b<k[a.c];b++)if(a.b!=c[m][k[b]]){i=false;break}if(i)c[m]=a.b}}function A(m){var r=\"\",i=false,b,e;for(b=0;b<k[a.c];b++){e=m[k[b]];if(a.b!=\r\ne){if(i)r+=k[b];r+=p(e);i=false}else i=true}return r}function p(m){var r=[],i,b;for(b=0;b<m[a.c];b++)if(a.b!=m[b]){i=\"\";if(b!=x&&a.b==m[b-1]){i+=b.toString();i+=f}i+=u(m[b]);a.h(r,i)}return l+r.join(q)+s}function u(m){var r=\"\",i,b,e;for(i=0;i<m[a.c];i++){b=m.charAt(i);e=w[b];r+=a.b!=e?e:b}return r}d.Kc=function(m){return a.b!=c[m]};d.N=function(){var m=[],r;for(r in c)if(a.b!=c[r])a.h(m,r.toString()+A(c[r]));return m.join(\"\")};d.Sc=function(m){if(m==a.b)return d.N();var r=[m.N()],i;for(i in c)if(a.b!=\r\nc[i]&&!m.Kc(i))a.h(r,i.toString()+A(c[i]));return r.join(\"\")};d._setKey=function(m,r,i){if(typeof i!=\"string\")return false;g(m,h,r,i);return true};d._setValue=function(m,r,i){if(typeof i!=\"number\"&&(a.b==Number||!(i instanceof Number)))return false;if(Math.round(i)!=i||i==NaN||i==Infinity)return false;g(m,o,r,i.toString());return true};d._getKey=function(m,r){return z(m,h,r)};d._getValue=function(m,r){return z(m,o,r)};d._clearKey=function(m){B(m,h)};d._clearValue=function(m){B(m,o)}};_gat.ic=function(d,a){var c=this;c.jd=a;c.Pc=d;c._trackEvent=function(h,o,k){return a._trackEvent(c.Pc,h,o,k)}};_gat.kc=function(d){var a=this,c=_gat,h=c.b,o=c.q,k=c.w,l=c.F,s=c.C,q=c.P,f=c.z,n=\"location\",w=c.c,x=h,g=new c.hc,z=false;a.a=document;a.e=window;a.ja=Math.round((new Date).getTime()/1000);a.H=d;a.yb=a.a.referrer;a.va=h;a.j=h;a.A=h;a.M=false;a.aa=h;a.rb=\"\";a.l=h;a.Ab=h;a.f=h;a.u=h;function B(){if(\"auto\"==g.g){var b=a.a.domain;if(\"www.\"==l(b,0,4))b=l(b,4);g.g=b}g.g=c.T(g.g)}function A(){var b=g.g,e=k(b,\"www.google.\")*k(b,\".google.\")*k(b,\"google.\");return e||\"/\"!=g.p||k(b,\"google.org\")>-1}function p(b,\r\ne,j){if(o(b)||o(e)||o(j))return\"-\";var t=s(b,c.r+a.f+\".\",e),v;if(!o(t)){v=f(t,\".\");v[5]=v[5]?v[5]*1+1:1;v[3]=v[4];v[4]=j;t=v.join(\".\")}return t}function u(){return\"file:\"!=a.a[n].protocol&&A()}function m(b){if(!b||\"\"==b)return\"\";while(c.Lb(b.charAt(0)))b=l(b,1);while(c.Lb(b.charAt(b[w]-1)))b=l(b,0,b[w]-1);return b}function r(b,e,j){if(!o(b())){e(c.J(b()));if(!q(b(),\";\"))j()}}function i(b){var e,j=\"\"!=b&&a.a[n].host!=b;if(j)for(e=0;e<g.B[w];e++)j=j&&k(c.T(b),c.T(g.B[e]))==-1;return j}a.Bc=function(){if(!g.g||\r\n\"\"==g.g||\"none\"==g.g){g.g=\"\";return 1}B();return g.pb?c.t(g.g):1};a.tc=function(b,e){if(o(b))b=\"-\";else{e+=g.p&&\"/\"!=g.p?g.p:\"\";var j=k(b,e);b=j>=0&&j<=8?\"0\":(\"[\"==b.charAt(0)&&\"]\"==b.charAt(b[w]-1)?\"-\":b)}return b};a.Ia=function(b){var e=\"\",j=a.a;e+=a.aa?a.aa.Xc():\"\";e+=g.qa?a.rb:\"\";e+=g.ta&&!o(j.title)?\"&utmdt=\"+c.d(j.title):\"\";e+=\"&utmhid=\"+c.uc()+\"&utmr=\"+a.va+\"&utmp=\"+a.Tc(b);return e};a.Tc=function(b){var e=a.a[n];b=h!=b&&\"\"!=b?c.d(b,true):c.d(e.pathname+unescape(e.search),true);return b};a.$c=\r\nfunction(b){if(a.Q()){var e=\"\";if(a.l!=h&&a.l.N().length>0)e+=\"&utme=\"+c.d(a.l.N());e+=a.Ia(b);x.O(e,a.H,a.a,a.f)}};a.qc=function(){var b=new c.Y(a.a,g);return b.Ha(a.f)?b.Wc():h};a._getLinkerUrl=function(b,e){var j=f(b,\"#\"),t=b,v=a.qc();if(v)if(e&&1>=j[w])t+=\"#\"+v;else if(!e||1>=j[w])if(1>=j[w])t+=(q(b,\"?\")?\"&\":\"?\")+v;else t=j[0]+(q(b,\"?\")?\"&\":\"?\")+v+\"#\"+j[1];return t};a.Zb=function(){var b;if(a.A&&a.A[w]>=10&&!q(a.A,\"=\")){a.u.Uc(a.A);a.u.cd();c._gasoDomain=g.g;c._gasoCPath=g.p;b=a.a.createElement(\"script\");\r\nb.type=\"text/javascript\";b.id=\"_gasojs\";b.src=\"https://www.google.com/analytics/reporting/overlay_js?gaso=\"+a.A+\"&\"+c.wa();a.a.getElementsByTagName(\"head\")[0].appendChild(b)}};a.Jc=function(){var b=a.a[c.m],e=a.ja,j=a.u,t=a.f+\"\",v=a.e,y=v?v.gaGlobal:h,E,F=q(b,c.r+t+\".\"),I=q(b,c.W+t),G=q(b,c.ma+t),C,D=[],H=\"\",K=false,J;b=o(b)?\"\":b;if(g.I){E=c.Db(a.a[n]);if(g.pa&&!o(E))H=E+\"&\";H+=a.a[n].search;if(!o(H)&&q(H,c.r)){j.Rc(H);if(!j.Jb())j.pc();C=j.ya()}r(j.Ba,j.Wb,j.fc);r(j.Aa,j.Na,j.Qa)}if(!o(C))if(o(j.K())||\r\no(j.za())){C=p(H,\"&\",e);a.M=true}else{D=f(j.K(),\".\");t=D[0]}else if(F)if(!I||!G){C=p(b,\";\",e);a.M=true}else{C=s(b,c.r+t+\".\",\";\");D=f(s(b,c.W+t,\";\"),\".\")}else{C=[t,c.Gc(),e,e,e,1].join(\".\");a.M=true;K=true}C=f(C,\".\");if(v&&y&&y.dh==t){C[4]=y.sid?y.sid:C[4];if(K){C[3]=y.sid?y.sid:C[4];if(y.vid){J=f(y.vid,\".\");C[1]=J[0];C[2]=J[1]}}}j.Ub(C.join(\".\"));D[0]=t;D[1]=D[1]?D[1]:0;D[2]=undefined!=D[2]?D[2]:g.Yc;D[3]=D[3]?D[3]:C[4];j.La(D.join(\".\"));j.Vb(t);if(!o(j.Hc()))j.Ma(j.t());j.dc();j.Pa();j.ec()};a.Lc=\r\nfunction(){x=new c.jc(g)};a._initData=function(){var b;if(!z){a.Lc();a.f=a.Bc();a.u=new c.Y(a.a,g)}if(u())a.Jc();if(!z){if(u()){a.va=a.tc(a.Ac(),a.a.domain);if(g.sa){a.aa=new c.gc(g.ua);a.aa.xc()}if(g.qa){b=new c.n(a.f,a.a,a.va,a.ja,g);a.rb=b.yc(a.u,a.M)}}a.l=new c.Z;a.Ab=new c.Z;z=true}if(!c.Hb)a.Mc()};a._visitCode=function(){a._initData();var b=s(a.a[c.m],c.r+a.f+\".\",\";\"),e=f(b,\".\");return e[w]<4?\"\":e[1]};a._cookiePathCopy=function(b){a._initData();if(a.u)a.u.bd(a.f,b)};a.Mc=function(){var b=a.a[n].hash,\r\ne;e=b&&\"\"!=b&&0==k(b,\"#gaso=\")?s(b,\"gaso=\",\"&\"):s(a.a[c.m],c.Sa,\";\");if(e[w]>=10){a.A=e;if(a.e.addEventListener)a.e.addEventListener(\"load\",a.Zb,false);else a.e.attachEvent(\"onload\",a.Zb)}c.Hb=true};a.Q=function(){return a._visitCode()%10000<g.ha*100};a.Vc=function(){var b,e,j=a.a.links;if(!g.Kb){var t=a.a.domain;if(\"www.\"==l(t,0,4))t=l(t,4);g.B.push(\".\"+t)}for(b=0;b<j[w]&&(g.Ga==-1||b<g.Ga);b++){e=j[b];if(i(e.host))if(!e.gatcOnclick){e.gatcOnclick=e.onclick?e.onclick:a.Qc;e.onclick=function(v){var y=\r\n!this.target||this.target==\"_self\"||this.target==\"_top\"||this.target==\"_parent\";y=y&&!a.oc(v);a.ad(v,this,y);return y?false:(this.gatcOnclick?this.gatcOnclick(v):true)}}}};a.Qc=function(){};a._trackPageview=function(b){if(u()){a._initData();if(g.B)a.Vc();a.$c(b);a.M=false}};a._trackTrans=function(){var b=a.f,e=[],j,t,v,y;a._initData();if(a.j&&a.Q()){for(j=0;j<a.j.la[w];j++){t=a.j.la[j];c.h(e,t.S());for(v=0;v<t.ca[w];v++)c.h(e,t.ca[v].S())}for(y=0;y<e[w];y++)x.O(e[y],a.H,a.a,b,true)}};a._setTrans=\r\nfunction(){var b=a.a,e,j,t,v,y=b.getElementById?b.getElementById(\"utmtrans\"):(b.utmform&&b.utmform.utmtrans?b.utmform.utmtrans:h);a._initData();if(y&&y.value){a.j=new c.i;v=f(y.value,\"UTM:\");g.G=!g.G||\"\"==g.G?\"|\":g.G;for(e=0;e<v[w];e++){v[e]=m(v[e]);j=f(v[e],g.G);for(t=0;t<j[w];t++)j[t]=m(j[t]);if(\"T\"==j[0])a._addTrans(j[1],j[2],j[3],j[4],j[5],j[6],j[7],j[8]);else if(\"I\"==j[0])a._addItem(j[1],j[2],j[3],j[4],j[5],j[6])}}};a._addTrans=function(b,e,j,t,v,y,E,F){a.j=a.j?a.j:new c.i;return a.j.nb(b,e,\r\nj,t,v,y,E,F)};a._addItem=function(b,e,j,t,v,y){var E;a.j=a.j?a.j:new c.i;E=a.j.xa(b);if(!E)E=a._addTrans(b,\"\",\"\",\"\",\"\",\"\",\"\",\"\");E.mb(e,j,t,v,y)};a._setVar=function(b){if(b&&\"\"!=b&&A()){a._initData();var e=new c.Y(a.a,g),j=a.f;e.Na(j+\".\"+c.d(b));e.Qa();if(a.Q())x.O(\"&utmt=var\",a.H,a.a,a.f)}};a._link=function(b,e){if(g.I&&b){a._initData();a.a[n].href=a._getLinkerUrl(b,e)}};a._linkByPost=function(b,e){if(g.I&&b&&b.action){a._initData();b.action=a._getLinkerUrl(b.action,e)}};a._setXKey=function(b,e,\r\nj){a.l._setKey(b,e,j)};a._setXValue=function(b,e,j){a.l._setValue(b,e,j)};a._getXKey=function(b,e){return a.l._getKey(b,e)};a._getXValue=function(b,e){return a.l.getValue(b,e)};a._clearXKey=function(b){a.l._clearKey(b)};a._clearXValue=function(b){a.l._clearValue(b)};a._createXObj=function(){a._initData();return new c.Z};a._sendXEvent=function(b){var e=\"\";a._initData();if(a.Q()){e+=\"&utmt=event&utme=\"+c.d(a.l.Sc(b))+a.Ia();x.O(e,a.H,a.a,a.f,false,true)}};a._createEventTracker=function(b){a._initData();\r\nreturn new c.ic(b,a)};a._trackEvent=function(b,e,j,t){var v=true,y=a.Ab;if(h!=b&&h!=e&&\"\"!=b&&\"\"!=e){y._clearKey(5);y._clearValue(5);v=y._setKey(5,1,b)?v:false;v=y._setKey(5,2,e)?v:false;v=h==j||y._setKey(5,3,j)?v:false;v=h==t||y._setValue(5,1,t)?v:false;if(v)a._sendXEvent(y)}else v=false;return v};a.ad=function(b,e,j){a._initData();if(a.Q()){var t=new c.Z;t._setKey(6,1,e.href);var v=j?function(){a.rc(b,e)}:undefined;x.O(\"&utmt=event&utme=\"+c.d(t.N())+a.Ia(),a.H,a.a,a.f,false,true,v)}};a.rc=function(b,\r\ne){if(!b)b=a.e.event;var j=true;if(e.gatcOnclick)j=e.gatcOnclick(b);if(j||typeof j==\"undefined\")if(!e.target||e.target==\"_self\")a.e.location=e.href;else if(e.target==\"_top\")a.e.top.document.location=e.href;else if(e.target==\"_parent\")a.e.parent.document.location=e.href};a.oc=function(b){if(!b)b=a.e.event;var e=b.shiftKey||b.ctrlKey||b.altKey;if(!e)if(b.modifiers&&a.e.Event)e=b.modifiers&a.e.Event.CONTROL_MASK||b.modifiers&a.e.Event.SHIFT_MASK||b.modifiers&a.e.Event.ALT_MASK;return e};a._setDomainName=\r\nfunction(b){g.g=b};a.dd=function(){return g.g};a._addOrganic=function(b,e){c.h(g.fa,new c.cb(b,e))};a._clearOrganic=function(){g.fa=[]};a.hd=function(){return g.fa};a._addIgnoredOrganic=function(b){c.h(g.ea,b)};a._clearIgnoredOrganic=function(){g.ea=[]};a.ed=function(){return g.ea};a._addIgnoredRef=function(b){c.h(g.ga,b)};a._clearIgnoredRef=function(){g.ga=[]};a.fd=function(){return g.ga};a._setAllowHash=function(b){g.pb=b?1:0};a._setCampaignTrack=function(b){g.qa=b?1:0};a._setClientInfo=function(b){g.sa=\r\nb?1:0};a._getClientInfo=function(){return g.sa};a._setCookiePath=function(b){g.p=b};a._setTransactionDelim=function(b){g.G=b};a._setCookieTimeout=function(b){g.wb=b};a._setDetectFlash=function(b){g.ua=b?1:0};a._getDetectFlash=function(){return g.ua};a._setDetectTitle=function(b){g.ta=b?1:0};a._getDetectTitle=function(){return g.ta};a._setLocalGifPath=function(b){g.Da=b};a._getLocalGifPath=function(){return g.Da};a._setLocalServerMode=function(){g.D=0};a._setRemoteServerMode=function(){g.D=1};a._setLocalRemoteServerMode=\r\nfunction(){g.D=2};a.gd=function(){return g.D};a._getServiceMode=function(){return g.D};a._setSampleRate=function(b){g.ha=b};a._setSessionTimeout=function(b){g.Tb=b};a._setAllowLinker=function(b){g.I=b?1:0};a._setAllowAnchor=function(b){g.pa=b?1:0};a._setCampNameKey=function(b){g.db=b};a._setCampContentKey=function(b){g.eb=b};a._setCampIdKey=function(b){g.fb=b};a._setCampMediumKey=function(b){g.gb=b};a._setCampNOKey=function(b){g.hb=b};a._setCampSourceKey=function(b){g.ib=b};a._setCampTermKey=function(b){g.jb=\r\nb};a._setCampCIdKey=function(b){g.kb=b};a._getAccount=function(){return a.H};a._getVersion=function(){return _gat.lb};a.kd=function(b){g.B=[];if(b)g.B=b};a.md=function(b){g.Kb=b};a.ld=function(b){g.Ga=b};a._setReferrerOverride=function(b){a.yb=b};a.Ac=function(){return a.yb}};_gat._getTracker=function(d){var a=new _gat.kc(d);return a};\r\n</script>","//","","google analytics static page","","-2","0","0","0","0","0","0"
"614219","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","49436","2011-06-16 20:20:00","Europe/Paris","2011-06-16 18:20:22","2011-06-17 09:51:33","","post","wiki","2011/06/Autovivification-in-Perl-Great-Idea-Huge-Trap-Leaking-Abstraction","en","Autovivification in Perl : Great Idea but also Huge Trap - Another Leaking Abstraction...","","","[Autovivification|http://en.wikipedia.org/wiki/Autovivification|en] is one of Perl's really great design success. \r\n\r\nIt all comes to ''you don't need to worry about existence before dereferencing something''.\r\n\r\nThat means, for setting a nested hash, you only need to write :\r\n\r\n///\r\n$h->{foo}{bar} = \"value\";\r\n///\r\n\r\nAnd that will work out of the box. Perl will happily create all the data-structure for you.\r\n\r\nSo, now a little coding test, what does the following code output ? \r\n\r\n///\r\nmy $a;\r\n\r\nif ($a->{foo}{bar}) {\r\n   print \"Found foo/bar\n\";\r\n}\r\n\r\nif ($a->{foo}) {\r\n   print \"Found foo\n\";\r\n}\r\n///\r\n\r\nNaively, it shouldn’t output anything, right ?\r\n\r\nNot so fast. Upon a careful read of {{Perl will happily create all the data-structure for you}}, we can put some emphasis on one word : {{Perl will happily __create__ all the data-structure for you}}.\r\n\r\nThat might be just perfect, except that Perl creates it whenever it needs it, even if it is only for __reading__.\r\n\r\nAnd now you understand the catch : a __read__ operation can result in a __write__ one.\r\n\r\nAs Uncle Ben (from SpiderMan) said$$Voltaire, Franklin D. Roosevelt and other said something very similar, but they are not as geeky.$$ : {{With Great Power Comes Great Responsibility}}. \r\n\r\n[Dagfinn Ilmari Mannsåker|http://search.cpan.org/~ilmari/|en] showed me a nice [autovivification|http://search.cpan.org/perldoc?autovivification|en] module on CPAN that fixes this behavior, and enables a fine tuning of this process.\r\n\r\nI really think the fact that creation also happen when querying the value is a real bug in Perl itself, or at least a bug in the design of the feature.","<p><a href=\"http://en.wikipedia.org/wiki/Autovivification\" hreflang=\"en\">Autovivification</a> is one of Perl's really great design success.</p>\n<p>It all comes to <em>you don't need to worry about existence before\ndereferencing something</em>.</p>\n<p>That means, for setting a nested hash, you only need to write :</p>\n<pre>\n$h-&gt;{foo}{bar} = &quot;value&quot;;\n</pre>\n<p>And that will work out of the box. Perl will happily create all the\ndata-structure for you.</p>\n<p>So, now a little coding test, what does the following code output ?</p>\n<pre>\nmy $a;\n\nif ($a-&gt;{foo}{bar}) {\n   print &quot;Found foo/bar\n&quot;;\n}\n\nif ($a-&gt;{foo}) {\n   print &quot;Found foo\n&quot;;\n}\n</pre>\n<p>Naively, it shouldn’t output anything, right ?</p>\n<p>Not so fast. Upon a careful read of <q>Perl will happily create all the\ndata-structure for you</q>, we can put some emphasis on one word : <q>Perl will\nhappily <strong>create</strong> all the data-structure for you</q>.</p>\n<p>That might be just perfect, except that Perl creates it whenever it needs\nit, even if it is only for <strong>reading</strong>.</p>\n<p>And now you understand the catch : a <strong>read</strong> operation can\nresult in a <strong>write</strong> one.</p>\n<p>As Uncle Ben (from SpiderMan) said<sup>[<a href=\"#pnote-614219-1\" id=\"rev-pnote-614219-1\" name=\"rev-pnote-614219-1\">1</a>]</sup> : <q>With Great\nPower Comes Great Responsibility</q>.</p>\n<p><a href=\"http://search.cpan.org/~ilmari/\" hreflang=\"en\">Dagfinn Ilmari\nMannsåker</a> showed me a nice <a href=\"http://search.cpan.org/perldoc?autovivification\" hreflang=\"en\">autovivification</a> module on CPAN that fixes this behavior, and enables\na fine tuning of this process.</p>\n<p>I really think the fact that creation also happen when querying the value is\na real bug in Perl itself, or at least a bug in the design of the feature.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-614219-1\" id=\"pnote-614219-1\" name=\"pnote-614219-1\">1</a>] Voltaire, Franklin D. Roosevelt and other said\nsomething very similar, but they are not as geeky.</p>\n</div>","","autovivification perl great idea but also huge trap another leaking abstraction autovivification one perl really great design success all comes you don need worry about existence before dereferencing something that means for setting nested hash you only need write foo bar quot value quot and that will work out the box perl will happily create all the data structure for you now little coding test what does the following code output foo bar print quot found foo bar quot foo print quot found foo quot naively shouldn’t output anything right not fast upon careful read perl will happily create all the data structure for you can put some emphasis one word perl will happily create all the data structure for you that might just perfect except that perl creates whenever needs even only for reading and now you understand the catch read operation can result write one uncle ben from spiderman said with great power comes great responsibility dagfinn ilmari mannsåker showed nice autovivification module cpan that fixes this behavior and enables fine tuning this process really think the fact that creation also happen when querying the value real bug perl itself least bug the design the feature notes voltaire franklin roosevelt and other said something very similar but they are not geeky","a:1:{s:3:\"tag\";a:2:{i:0;s:4:\"fail\";i:1;s:4:\"perl\";}}","1","0","1","0","0","0","0"
"191687","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2008-08-01 21:17:00","Europe/Paris","2007-12-28 16:20:20","2009-11-27 07:40:56","","post","wiki","2008/08/Dynamic-Typing-in-Java-why-and-when","en","Dynamic Typing in Java : why & when","","","!!!!Basics of dynamic typing\r\n\r\nWhen I began programming in Java, I had deep Perl & C backgrounds. I naturally was quite fond of dynamic typing since C was usually considered as quite painful to write. It feeled like magic : you didn't need to write many different classes to represent different data objects. \r\n\r\nThe idea is quite basic : wrap every property in a @@Map@@ and instead of using a named getter, you use a generic getter that has the property name as an argument. You might even have multiple generic getters to be able to directly return the needed type.\r\n\r\nYou can combine theses objects in Maps of Maps of Lists of Maps to be able to store quite complex structures. When the model becomes quite complicated, you might upgrade your home-grown solution in the much more fancy DOM.\r\n\r\nDOM makes filtering, retrieving and iterating a breeze.\r\n\r\nThis has a very nice property of make code quite easy to adapt. No constraints. \r\n\r\n!!!!The Problem\r\n\r\nThis extreme flexibility comes at an extreme price : __every type checking has to be done at runtime__. Now you can't easily divide user data that inherently dynamic from application meta-data that is constant over a run.\r\n\r\n!!!!A bit of context\r\n\r\nThe main point to go on the road of dynamic typing was the la","<h2>Basics of dynamic typing</h2>\n<p>When I began programming in Java, I had deep Perl &amp; C\nbackgrounds. I naturally was quite fond of dynamic typing since C was\nusually considered as quite painful to write. It feeled like magic : you didn't\nneed to write many different classes to represent different data objects.</p>\n<p>The idea is quite basic : wrap every property in a <code>Map</code> and\ninstead of using a named getter, you use a generic getter that has the property\nname as an argument. You might even have multiple generic getters to be able to\ndirectly return the needed type.</p>\n<p>You can combine theses objects in Maps of Maps of Lists of Maps to be able\nto store quite complex structures. When the model becomes quite complicated,\nyou might upgrade your home-grown solution in the much more fancy DOM.</p>\n<p>DOM makes filtering, retrieving and iterating a breeze.</p>\n<p>This has a very nice property of make code quite easy to adapt. No\nconstraints.</p>\n<h2>The Problem</h2>\n<p>This extreme flexibility comes at an extreme price : <strong>every type\nchecking has to be done at runtime</strong>. Now you can't easily divide user\ndata that inherently dynamic from application meta-data that is constant over a\nrun.</p>\n<h2>A bit of context</h2>\n<p>The main point to go on the road of dynamic typing was the la</p>","","dynamic typing java why when basics dynamic typing when began programming java had deep perl amp backgrounds naturally was quite fond dynamic typing since was usually considered quite painful write feeled like magic you didn need write many different classes represent different data objects the idea quite basic wrap every property map and instead using named getter you use generic getter that has the property name argument you might even have multiple generic getters able directly return the needed type you can combine theses objects maps maps lists maps able store quite complex structures when the model becomes quite complicated you might upgrade your home grown solution the much more fancy dom dom makes filtering retrieving and iterating breeze this has very nice property make code quite easy adapt constraints the problem this extreme flexibility comes extreme price every type checking has done runtime now you can easily divide user data that inherently dynamic from application meta data that constant over run bit context the main point the road dynamic typing was the","a:0:{}","-2","0","1","1","0","0","0"
"415135","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-07-31 08:00:00","Europe/Paris","2009-07-03 09:28:26","2009-07-31 09:24:11","","post","wiki","2009/07/Databases:-Better-Defer-Constraints-than-Avoid-Them","en","Databases: Better Defer Constraints than Avoid Them","Constraints are considered a {{Good Thing}}. They enable to rely more heavily on the validity of the database. It is quite important to note that validity is in terms of __modeling__ and not in terms of __business__ $$I'll write an article about this later$$.\r\n\r\nThe biggest complains we can have about constraints is that it is sometimes quite annoying to do some updates while consistently validating the constraints. You have to care about the order of the operations you do.","<p>Constraints are considered a <q>Good Thing</q>. They enable to rely more\nheavily on the validity of the database. It is quite important to note that\nvalidity is in terms of <strong>modeling</strong> and not in terms of\n<strong>business</strong> <sup>[<a href=\"#pnote-415135-1\" id=\"rev-pnote-415135-1\" name=\"rev-pnote-415135-1\">1</a>]</sup>.</p>\n<p>The biggest complains we can have about constraints is that it is sometimes\nquite annoying to do some updates while consistently validating the\nconstraints. You have to care about the order of the operations you do.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-415135-1\" id=\"pnote-415135-1\" name=\"pnote-415135-1\">1</a>] I'll write an article about this later</p>\n</div>","!!!! ''ACID''ity of transactions as rescue...\r\n\r\nLet's examine what all of the @@A@@, the @@C@@, the @@I@@ and the @@D@@ have for implications on our current subject.\r\n\r\n!!! Transactions are __atomic__ (A)\r\n\r\nThe database should not really care about validating the constraints all the time Only the beginning and end state is really important. __Inside a transaction, the data may be inconsistent__ : {{Dust hasn't yet settled}}.\r\n\r\nLet's study an example. You have an unique index on an ordering. \r\n\r\n///\r\nTABLE ITEMS (\r\n    ITEM_ID SERIAL,\r\n    GROUP_ID INTEGER,\r\n    ORDER INTEGER,\r\n    LABEL VARCHAR\r\n)\r\n\r\nADD UNIQUE INDEX ON ITEMS(GROUP_ID, ORDER)\r\n///\r\n\r\nHere, if you want to swap 2 items, you have to use an unused temporary value, otherwise the check won't be valid __at all times__.\r\n\r\n!!! Transactions are __consistent__ (C)\r\n\r\nThe check __needs__ to be done at least when the transaction ends. No special need here except that you __need__ to re-enable before ending the transaction.\r\n\r\n!!! Transactions are __isolated__ (I)\r\n\r\nWe can also easily imagine that the check only __needs__ to be done when the transaction ends, just before the commit. Normally no one should be able to see the changing data meanwhile it's not completed. \r\n\r\nOne __very important__ thing to know is that if you are using a __READ UNCOMMITTED isolation__ for other transactions __you will see inconsistent data__. \r\n\r\nAnyway, if you are doing that, you know what you are doing and are obviously taking special care about it, don't you ?\r\n\r\n!!! Transactions are __durable__ (D)\r\n\r\nThis final property has nothing to do with our current issue. Good.\r\n\r\n!!!! ... But sometimes early warning is quite nice\r\n\r\nWhen interacting with external systems, that don't participate in our transaction, extra care should be taken in order to cope with the exceptional case of a constraint failure. Distributed transactions is a very complex subject, and usually it's not --supported-- done$$Do you always implement a double-phase commit on every SOAP/REST/XML-HTTP/POST interface that you expose ?$$.\r\n\r\n!!!! Conclusion\r\n\r\nWith the principle of ''least surprise'' in mind, we can easily understand why @@deferred@@ is not the default behavior, but it makes a very nice addition to our toolbox.\r\n\r\nSo now there isn't any good reason anymore not to use (and abuse) constraints in your databases.\r\n\r\nRemember, __your data is you most precious asset__, protect it at all cost from evil misbehaved bug-ridden software$$Yeah, I have chosen my side in the coming war...$$ !\r\n\r\nWow... Just in time before the end of the month... I would have failed [my motto on URLs|/post/2009/04/Should-The-URL-Include-a-Date-or-Not] ;-)","<h2><em>ACID</em>ity of transactions as rescue...</h2>\n<p>Let's examine what all of the <code>A</code>, the <code>C</code>, the\n<code>I</code> and the <code>D</code> have for implications on our current\nsubject.</p>\n<h3>Transactions are <strong>atomic</strong> (A)</h3>\n<p>The database should not really care about validating the constraints all the\ntime Only the beginning and end state is really important. <strong>Inside a\ntransaction, the data may be inconsistent</strong> : <q>Dust hasn't yet\nsettled</q>.</p>\n<p>Let's study an example. You have an unique index on an ordering.</p>\n<pre>\nTABLE ITEMS (\n    ITEM_ID SERIAL,\n    GROUP_ID INTEGER,\n    ORDER INTEGER,\n    LABEL VARCHAR\n)\n\nADD UNIQUE INDEX ON ITEMS(GROUP_ID, ORDER)\n</pre>\n<p>Here, if you want to swap 2 items, you have to use an unused temporary\nvalue, otherwise the check won't be valid <strong>at all times</strong>.</p>\n<h3>Transactions are <strong>consistent</strong> (C)</h3>\n<p>The check <strong>needs</strong> to be done at least when the transaction\nends. No special need here except that you <strong>need</strong> to re-enable\nbefore ending the transaction.</p>\n<h3>Transactions are <strong>isolated</strong> (I)</h3>\n<p>We can also easily imagine that the check only <strong>needs</strong> to be\ndone when the transaction ends, just before the commit. Normally no one should\nbe able to see the changing data meanwhile it's not completed.</p>\n<p>One <strong>very important</strong> thing to know is that if you are using a\n<strong>READ UNCOMMITTED isolation</strong> for other transactions <strong>you\nwill see inconsistent data</strong>.</p>\n<p>Anyway, if you are doing that, you know what you are doing and are obviously\ntaking special care about it, don't you ?</p>\n<h3>Transactions are <strong>durable</strong> (D)</h3>\n<p>This final property has nothing to do with our current issue. Good.</p>\n<h2>... But sometimes early warning is quite nice</h2>\n<p>When interacting with external systems, that don't participate in our\ntransaction, extra care should be taken in order to cope with the exceptional\ncase of a constraint failure. Distributed transactions is a very complex\nsubject, and usually it's not <del>supported</del> done<sup>[<a href=\"#pnote-415135-1\" id=\"rev-pnote-415135-1\" name=\"rev-pnote-415135-1\">1</a>]</sup>.</p>\n<h2>Conclusion</h2>\n<p>With the principle of <em>least surprise</em> in mind, we can easily\nunderstand why <code>deferred</code> is not the default behavior, but it makes\na very nice addition to our toolbox.</p>\n<p>So now there isn't any good reason anymore not to use (and abuse)\nconstraints in your databases.</p>\n<p>Remember, <strong>your data is you most precious asset</strong>, protect it\nat all cost from evil misbehaved bug-ridden software<sup>[<a href=\"#pnote-415135-2\" id=\"rev-pnote-415135-2\" name=\"rev-pnote-415135-2\">2</a>]</sup> !</p>\n<p>Wow... Just in time before the end of the month... I would have failed\n<a href=\"/post/2009/04/Should-The-URL-Include-a-Date-or-Not\">my motto on\nURLs</a> ;-)</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-415135-1\" id=\"pnote-415135-1\" name=\"pnote-415135-1\">1</a>] Do you always implement a double-phase commit on every\nSOAP/REST/XML-HTTP/POST interface that you expose ?</p>\n<p>[<a href=\"#rev-pnote-415135-2\" id=\"pnote-415135-2\" name=\"pnote-415135-2\">2</a>] Yeah, I have chosen my side in the coming war...</p>\n</div>","","databases better defer constraints than avoid them constraints are considered good thing they enable rely more heavily the validity the database quite important note that validity terms modeling and not terms business the biggest complains can have about constraints that sometimes quite annoying some updates while consistently validating the constraints you have care about the order the operations you notes write article about this later acidity transactions rescue let examine what all the the the and the have for implications our current subject transactions are atomic the database should not really care about validating the constraints all the time only the beginning and end state really important inside transaction the data may inconsistent dust hasn yet settled let study example you have unique index ordering table items item serial group integer order integer label varchar add unique index items group order here you want swap items you have use unused temporary value otherwise the check won valid all times transactions are consistent the check needs done least when the transaction ends special need here except that you need enable before ending the transaction transactions are isolated can also easily imagine that the check only needs done when the transaction ends just before the commit normally one should able see the changing data meanwhile not completed one very important thing know that you are using read uncommitted isolation for other transactions you will see inconsistent data anyway you are doing that you know what you are doing and are obviously taking special care about don you transactions are durable this final property has nothing with our current issue good but sometimes early warning quite nice when interacting with external systems that don participate our transaction extra care should taken order cope with the exceptional case constraint failure distributed transactions very complex subject and usually not supported done conclusion with the principle least surprise mind can easily understand why deferred not the default behavior but makes very nice addition our toolbox now there isn any good reason anymore not use and abuse constraints your databases remember your data you most precious asset protect all cost from evil misbehaved bug ridden software wow just time before the end the month would have failed motto urls notes you always implement double phase commit every soap rest xml http post interface that you expose yeah have chosen side the coming war","a:0:{}","1","0","1","0","0","0","0"
"415129","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-07-03 11:07:00","Europe/Paris","2009-07-03 09:07:57","2009-07-03 09:07:57","","post","wiki","2009/07/Efficient-Trees-queries-with-plain-SQL","en","Efficient Trees queries with plain SQL","","","A common technique is to use adjacency list model \n\n* Precedent article for denormalisation with views & triggers.\nThe Use it to maintain a cached extra PATH column on each row to be able to speedup selects","<p>A common technique is to use adjacency list model</p>\n<ul>\n<li>Precedent article for denormalisation with views &amp; triggers.</li>\n</ul>\n<p>The Use it to maintain a cached extra PATH column on each row to be able to\nspeedup selects</p>","","efficient trees queries with plain sql common technique use adjacency list model precedent article for denormalisation with views amp triggers the use maintain cached extra path column each row able speedup selects","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","-2","0","1","0","0","0","0"
"406128","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-05-29 09:16:00","Europe/Paris","2009-05-29 07:16:57","2009-11-23 19:45:19","","post","wiki","2009/05/Sample-Data-Generation-with-SQL","en","Sample Data Generation with SQL and Perl","","","When doing some development you often need dummy, placeholder, data. [Loren ipsum|http://en.wikipedia.org/wiki/Lorem_ipsum|en] is well known in the publishing industry. For day to day \r\n\r\n* For PWKF, data was needed to make a sample\r\n* Sample data insertion can be automatized since the column name are usually a comprehensive name\r\n* Autoextract the schema from the standard INFORMATION_SCHEMA tables","<p>When doing some development you often need dummy, placeholder, data.\n<a href=\"http://en.wikipedia.org/wiki/Lorem_ipsum\" hreflang=\"en\">Loren\nipsum</a> is well known in the publishing industry. For day to day</p>\n<ul>\n<li>For PWKF, data was needed to make a sample</li>\n<li>Sample data insertion can be automatized since the column name are usually\na comprehensive name</li>\n<li>Autoextract the schema from the standard INFORMATION_SCHEMA tables</li>\n</ul>","","sample data generation with sql and perl when doing some development you often need dummy placeholder data loren ipsum well known the publishing industry for day day for pwkf data was needed make sample sample data insertion can automatized since the column name are usually comprehensive name autoextract the schema from the standard information schema tables","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","-2","0","1","0","0","0","0"
"717685","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2013-02-01 01:47:00","Europe/Paris","2013-02-01 09:47:16","2013-02-02 17:24:41","","post","wiki","2013/02/Avoid-those-milli-hits-in-Munin","en","Avoid those milli-hits in Munin","","","A recurring question on IRC is : {{why do I have 500 million hit/s in my graph ?}}.\r\n\r\nTurns out that they are really seeing @@500m hit/s@@, and that lower-case @@m@@ means ''milli'', and not ''Mega'' as specified in the [Metric system|http://en.wikipedia.org/wiki/Metric_prefix|en]. This is automatically done by RRD.\r\n\r\nTo avoid this you should just specify @@graph_scale no@@ as [specified|http://munin-monitoring.org/wiki/graph_scale|en].","<p>A recurring question on IRC is : <q>why do I have 500 million hit/s in my\ngraph ?</q>.</p>\n<p>Turns out that they are really seeing <code>500m hit/s</code>, and that\nlower-case <code>m</code> means <em>milli</em>, and not <em>Mega</em> as\nspecified in the <a href=\"http://en.wikipedia.org/wiki/Metric_prefix\" hreflang=\"en\">Metric system</a>. This is automatically done by RRD.</p>\n<p>To avoid this you should just specify <code>graph_scale no</code> as\n<a href=\"http://munin-monitoring.org/wiki/graph_scale\" hreflang=\"en\">specified</a>.</p>","","avoid those milli hits munin recurring question irc why have 500 million hit graph turns out that they are really seeing 500m hit and that lower case means milli and not mega specified the metric system this automatically done rrd avoid this you should just specify graph scale specified","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"sysadmin\";i:1;s:5:\"munin\";}}","1","0","1","0","0","0","0"
"527747","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2010-06-21 22:22:00","Europe/Paris","2010-06-17 16:23:03","2010-06-21 20:28:22","","post","wiki","2010/06/CGI-on-steroids-with-FastCGI,-but-on-a-CGI-only-server-The-FastCGI-wrapper","en","CGI on steroids with FastCGI, but on a CGI-only server - The FastCGI wrapper","","","!!!! FastCGI is really ''CGI on steroids''\r\n\r\nFastCGI is very common way to increase performance of a CGI installation. It is based on the fact that usually the startup of CGI scripts is slow, whereas the response is quite fast.\r\n\r\nSo if you have a persistent process, you only have to take care of the startup once, and you then experience a real speedup. \r\n\r\n!!! FastCGI vs mod_perl (or mod_python, ...)\r\n\r\nOnce a big fan of @@mod_perl@@, I'm converted to FastCGI since. @@mod_perl@@ was for a long time __the__ answer for speeding up Perl CGI scripts. It has a very good track record of stability and has real hooks deep in the Apache processing requests. \r\n\r\nFastCGI focuses on a different feature set that is more actual than @@mod_perl@@$$who really need deep apache hooks ?$$ : \r\n* It is much simpler to install and configure, especially when having multiple applications. \r\n* Able to connect to a distant server (running as a different UID, chrooted or even on a remote host)\r\n* Able to mix scripting languages without any need to compile some other apache modules.\r\n* Able to be used with several webservers, even closed-source ones : FastCGI is a protocol, not an API.\r\n\r\n!!!! But steroids do have some side effects\r\n\r\n!!! CGI issues \r\n\r\nOne downside is that your CGI script should be adapted to FastCGI and the fact that the script doesn't end with the end of the request.\r\n\r\nIn the real world that's quite easy. Every language that is commonly used for CGI offers [CGI-wrapper libraries|http://www.fastcgi.com/drupal/node/5|en] that works in a FastCGI context as well as a plain CGI one. \r\n\r\n!!! Webserver issues\r\n\r\nAnother issue can also come from the webserver. Since CGI is dead simple to implement even the micro-webserver [thttpd|http://www.acme.com/software/thttpd/|en] implements it.  \r\n\r\nFastCGI on the other hand is a little more difficult to implement, since the webserver needs to create a container that monitors and calls the FastCGI-enabled script.  \r\n\r\n!!! A standalone FastCGI container\r\n\r\nFortunately, the FastCGI team provided us with a ready-to-use container and a very simple client that acts a plain CGI script, but proxies it to a full-blown container.\r\n\r\nSince the plain CGI part is a very small native executable its overhead is negligible compared to the reply time, even without  comparison with the startup time of the whole script.\r\n\r\nIts installation is also quite straightforward. I just installed the @@libfcgi@@ package on Debian : it provides @@/usr/bin/cgi-fcgi@@. \r\n\r\nI created a simple CGI wrapper for my previous [munin benchmarking|/post/2010/06/Waiting-for-Munin-2.0-Performance-FastCGI] needs :\r\n\r\n///\r\n#! /bin/sh\r\n\r\nexec /usr/bin/cgi-fcgi -connect /tmp/munin-cgi.sock \\r\n     /usr/lib/cgi-bin/munin-cgi-graph\r\n///","<h2>FastCGI is really <em>CGI on steroids</em></h2>\n<p>FastCGI is very common way to increase performance of a CGI installation. It\nis based on the fact that usually the startup of CGI scripts is slow, whereas\nthe response is quite fast.</p>\n<p>So if you have a persistent process, you only have to take care of the\nstartup once, and you then experience a real speedup.</p>\n<h3>FastCGI vs mod_perl (or mod_python, ...)</h3>\n<p>Once a big fan of <code>mod_perl</code>, I'm converted to FastCGI since.\n<code>mod_perl</code> was for a long time <strong>the</strong> answer for\nspeeding up Perl CGI scripts. It has a very good track record of stability and\nhas real hooks deep in the Apache processing requests.</p>\n<p>FastCGI focuses on a different feature set that is more actual than\n<code>mod_perl</code><sup>[<a href=\"#pnote-527747-1\" id=\"rev-pnote-527747-1\" name=\"rev-pnote-527747-1\">1</a>]</sup> :</p>\n<ul>\n<li>It is much simpler to install and configure, especially when having\nmultiple applications.</li>\n<li>Able to connect to a distant server (running as a different UID, chrooted\nor even on a remote host)</li>\n<li>Able to mix scripting languages without any need to compile some other\napache modules.</li>\n<li>Able to be used with several webservers, even closed-source ones : FastCGI\nis a protocol, not an API.</li>\n</ul>\n<h2>But steroids do have some side effects</h2>\n<h3>CGI issues</h3>\n<p>One downside is that your CGI script should be adapted to FastCGI and the\nfact that the script doesn't end with the end of the request.</p>\n<p>In the real world that's quite easy. Every language that is commonly used\nfor CGI offers <a href=\"http://www.fastcgi.com/drupal/node/5\" hreflang=\"en\">CGI-wrapper libraries</a> that works in a FastCGI context as well as a\nplain CGI one.</p>\n<h3>Webserver issues</h3>\n<p>Another issue can also come from the webserver. Since CGI is dead simple to\nimplement even the micro-webserver <a href=\"http://www.acme.com/software/thttpd/\" hreflang=\"en\">thttpd</a> implements\nit.</p>\n<p>FastCGI on the other hand is a little more difficult to implement, since the\nwebserver needs to create a container that monitors and calls the\nFastCGI-enabled script.</p>\n<h3>A standalone FastCGI container</h3>\n<p>Fortunately, the FastCGI team provided us with a ready-to-use container and\na very simple client that acts a plain CGI script, but proxies it to a\nfull-blown container.</p>\n<p>Since the plain CGI part is a very small native executable its overhead is\nnegligible compared to the reply time, even without comparison with the startup\ntime of the whole script.</p>\n<p>Its installation is also quite straightforward. I just installed the\n<code>libfcgi</code> package on Debian : it provides\n<code>/usr/bin/cgi-fcgi</code>.</p>\n<p>I created a simple CGI wrapper for my previous <a href=\"/post/2010/06/Waiting-for-Munin-2.0-Performance-FastCGI\">munin\nbenchmarking</a> needs :</p>\n<pre>\n#! /bin/sh\n\nexec /usr/bin/cgi-fcgi -connect /tmp/munin-cgi.sock \\n     /usr/lib/cgi-bin/munin-cgi-graph\n</pre>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-527747-1\" id=\"pnote-527747-1\" name=\"pnote-527747-1\">1</a>] who really need deep apache hooks ?</p>\n</div>","","cgi steroids with fastcgi but cgi only server the fastcgi wrapper fastcgi really cgi steroids fastcgi very common way increase performance cgi installation based the fact that usually the startup cgi scripts slow whereas the response quite fast you have persistent process you only have take care the startup once and you then experience real speedup fastcgi mod perl mod python once big fan mod perl converted fastcgi since mod perl was for long time the answer for speeding perl cgi scripts has very good track record stability and has real hooks deep the apache processing requests fastcgi focuses different feature set that more actual than mod perl much simpler install and configure especially when having multiple applications able connect distant server running different uid chrooted even remote host able mix scripting languages without any need compile some other apache modules able used with several webservers even closed source ones fastcgi protocol not api but steroids have some side effects cgi issues one downside that your cgi script should adapted fastcgi and the fact that the script doesn end with the end the request the real world that quite easy every language that commonly used for cgi offers cgi wrapper libraries that works fastcgi context well plain cgi one webserver issues another issue can also come from the webserver since cgi dead simple implement even the micro webserver thttpd implements fastcgi the other hand little more difficult implement since the webserver needs create container that monitors and calls the fastcgi enabled script standalone fastcgi container fortunately the fastcgi team provided with ready use container and very simple client that acts plain cgi script but proxies full blown container since the plain cgi part very small native executable its overhead negligible compared the reply time even without comparison with the startup time the whole script its installation also quite straightforward just installed the libfcgi package debian provides usr bin cgi fcgi created simple cgi wrapper for previous munin benchmarking needs bin exec usr bin cgi fcgi connect tmp munin cgi sock usr lib cgi bin munin cgi graph notes who really need deep apache hooks","a:1:{s:3:\"tag\";a:3:{i:0;s:4:\"perl\";i:1;s:11:\"performance\";i:2;s:6:\"design\";}}","1","0","1","0","1","0","0"
"614814","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2011-06-20 09:42:00","Europe/Paris","2011-06-20 07:42:41","2011-06-20 08:15:38","","post","wiki","2011/06/Enhance-RRD-I/O-performance-in-Munin-1.4-and-Scale","en","Enhance RRD I/O performance in Munin 1.4 and Scale","","","As with most of the RRD-based monitoring software (Cacti, Ganglia, ...), it is quite difficult to scale. \r\n\r\nThe bad part is that __updating__ __lots__ of small RRD files seems like __pure random I/O to__ the OS as stated in there documentation.\r\n\r\nThe __good part__ is that we are not alone, and therefore the __RRD developers did tackle the issue__ with [rrdcached|http://oss.oetiker.ch/rrdtool/doc/rrdcached.en.html|en]. It spools the updates, and flushs them to disk in a batched manner, or when needed by a rrd read command such as graphing. That's why it is __scales__ well when using __CGI graphing__. Otherwise, munin-graph will read every rrd, and therefore force a flush on all the cache.\r\n\r\nAnd the icing on the cake is that, although it is only fully integrated to munin 2.0, __you can use it right away in the 1.4.x series__. \r\n\r\nYou only need to define the environment variable @@RRDCACHED_ADDRESS@@ while running the scripts accessing the RRDs.\r\n\r\nThen, you have to remove the @@munin-graph@@ part of the @@munin-cron@@ and run it on its own line. Usually only every hour or so, to be able to accumulate data in @@rrdcached@@ before flushing it all to disk when graphing.\r\n\r\nUpdating to 2.0 is also an option to have a real CGI support. (CGI on 1.4 is existing but has nowhere decent performance).","<p>As with most of the RRD-based monitoring software (Cacti, Ganglia, ...), it\nis quite difficult to scale.</p>\n<p>The bad part is that <strong>updating</strong> <strong>lots</strong> of\nsmall RRD files seems like <strong>pure random I/O to</strong> the OS as stated\nin there documentation.</p>\n<p>The <strong>good part</strong> is that we are not alone, and therefore the\n<strong>RRD developers did tackle the issue</strong> with <a href=\"http://oss.oetiker.ch/rrdtool/doc/rrdcached.en.html\" hreflang=\"en\">rrdcached</a>. It spools the updates, and flushs them to disk in a batched\nmanner, or when needed by a rrd read command such as graphing. That's why it is\n<strong>scales</strong> well when using <strong>CGI graphing</strong>.\nOtherwise, munin-graph will read every rrd, and therefore force a flush on all\nthe cache.</p>\n<p>And the icing on the cake is that, although it is only fully integrated to\nmunin 2.0, <strong>you can use it right away in the 1.4.x series</strong>.</p>\n<p>You only need to define the environment variable\n<code>RRDCACHED_ADDRESS</code> while running the scripts accessing the\nRRDs.</p>\n<p>Then, you have to remove the <code>munin-graph</code> part of the\n<code>munin-cron</code> and run it on its own line. Usually only every hour or\nso, to be able to accumulate data in <code>rrdcached</code> before flushing it\nall to disk when graphing.</p>\n<p>Updating to 2.0 is also an option to have a real CGI support. (CGI on 1.4 is\nexisting but has nowhere decent performance).</p>","","enhance rrd performance munin and scale with most the rrd based monitoring software cacti ganglia quite difficult scale the bad part that updating lots small rrd files seems like pure random the stated there documentation the good part that are not alone and therefore the rrd developers did tackle the issue with rrdcached spools the updates and flushs them disk batched manner when needed rrd read command such graphing that why scales well when using cgi graphing otherwise munin graph will read every rrd and therefore force flush all the cache and the icing the cake that although only fully integrated munin you can use right away the series you only need define the environment variable rrdcached address while running the scripts accessing the rrds then you have remove the munin graph part the munin cron and run its own line usually only every hour able accumulate data rrdcached before flushing all disk when graphing updating also option have real cgi support cgi existing but has nowhere decent performance","a:1:{s:3:\"tag\";a:3:{i:0;s:3:\"rrd\";i:1;s:11:\"performance\";i:2;s:5:\"munin\";}}","1","0","1","0","0","0","0"
"466295","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38367","2009-12-14 22:00:00","Europe/Paris","2009-12-09 17:24:34","2009-12-12 11:23:08","","post","wiki","2009/12/Hierarchical-Memory-Allocation-Enables-Easy-C-and-C-Plus-Plus-Garbage-Collection","en","Hierarchical Memory Allocation Enables Easy C/C++ Garbage Collection","Memory allocation, and its deallocation, is a difficult part. It's so difficult than almost every modern language out here tries really hard to offer a kind of garbage collection. Several types exists, but the 2 most used are then mark-and-sweep type, or reference-counting type. \r\n\r\nThere is one that derives from the fact that usually software are organised in ''layers''. So the basic idea is that each allocated memory from each layer can be safely deallocation when the layer returns to the upper one. Only the ''out'' parameters should be preserved.","<p>Memory allocation, and its deallocation, is a difficult part. It's so\ndifficult than almost every modern language out here tries really hard to offer\na kind of garbage collection. Several types exists, but the 2 most used are\nthen mark-and-sweep type, or reference-counting type.</p>\n<p>There is one that derives from the fact that usually software are organised\nin <em>layers</em>. So the basic idea is that each allocated memory from each\nlayer can be safely deallocation when the layer returns to the upper one. Only\nthe <em>out</em> parameters should be preserved.</p>","!!!! A naïve vector-based allocator\r\n\r\nA container class\r\n\r\n\r\n!!!! Existing allocators\r\n\r\n2 mains projects use a hiera\r\n\r\nThink [talloc|http://talloc.samba.org/|en]","<h2>A naïve vector-based allocator</h2>\n<p>A container class</p>\n<h2>Existing allocators</h2>\n<p>2 mains projects use a hiera</p>\n<p>Think <a href=\"http://talloc.samba.org/\" hreflang=\"en\">talloc</a></p>","","hierarchical memory allocation enables easy garbage collection memory allocation and its deallocation difficult part difficult than almost every modern language out here tries really hard offer kind garbage collection several types exists but the most used are then mark and sweep type reference counting type there one that derives from the fact that usually software are organised layers the basic idea that each allocated memory from each layer can safely deallocation when the layer returns the upper one only the out parameters should preserved naïve vector based allocator container class existing allocators mains projects use hiera think talloc","a:1:{s:3:\"tag\";a:2:{i:0;s:18:\"garbage-collection\";i:1;s:11:\"c-plus-plus\";}}","-2","0","1","0","0","0","0"
"729285","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2013-04-13 14:42:00","Europe/Paris","2013-04-13 10:53:37","2013-04-15 08:30:36","","post","wiki","2013/04/Spinoffs-in-the-munin-ecosystem","en","Spinoffs in the munin ecosystem","","","!!!! KISS is the core design of Munin\r\n\r\nMunin's greatest strength is its very KISS architecture. It therefore gets many things right, such as a huge modularity. \r\n\r\nEach component (master/node/plugin) has a simple API to communicate with the others. \r\n\r\n!!!! Spin-offs ...\r\n\r\nI admit that the master, even the node, have convoluted code. In fact some rewrites already do exist. \r\n\r\n!!! ... are welcomed ...\r\n\r\nAnd they are a really good thing, as it enables rapid prototyping on things that the stock munin has (currently) trouble to do.\r\n\r\nThe stock munin is a piece of software that many depend upon, so it has to move at a much slower pace than one does want, even me. As much as I really want to add many many features to it, I still have to take extra care that it doesn't break stuff, even the least known features. \r\n\r\nSo I take munin off-springs very seriously and even offer as much help as I can in order for them to succeed.\r\n\r\n!!! ... because they are very valuable in the long term\r\n\r\nIn my opinion competition is only short bad in the short term, and in the long term they usually add significant value to the whole ecosystem. That said, there's always a risk to become slowly irrelevant, but I think that's the real power of open-source's evolutionary paradigm : embrace them or become obsolete and get replaced.\r\n\r\nSince, if someone takes the time to author a competitor that has a real threat potential, it mostly means that there's a real itch to scratch and that many things are to be learnt.\r\n\r\n!!!! Different layers of spin-offs\r\n\r\nThe munin ecosystem is divided in 3 main categories, obviously related to the 3 main components of munin : master, node & plugin.\r\n\r\n!!! Plugins\r\n\r\nThat's the most obvious part as custom plugins are the real ''bread and butter'' of munin. \r\n\r\nStock plugins are mostly written in Perl or POSIX shell, as Perl is munin's own language and POSIX shell is ubiquitous. That fact is acknowledged by the fact that core munin provides 2 libraries (Perl & Shell) to help plugin authoring.\r\n\r\nSo, it's quite natural that each mainstream language has grown its own plugin library. Some language even have [two|https://github.com/aouyar/PyMunin|en] of [them|https://github.com/samuel/python-munin|en].\r\n\r\n!! C\r\n\r\nSome plugins got even rewritten in plain C, as it was shown that shell plugins do have a significant impact on very under-powered nodes, such as embedded routers.\r\n\r\n!!! Node\r\n\r\nThis component is very simple. Yet, it has to be run on all the nodes that one wants to monitor. It is currently written in Perl, and while that's not an issue on UNIX-like systems, it can be quite problematic on embedded ones\r\n\r\n!! Simple munin\r\n\r\nThe official package comes with a POSIX shell rewrite that has to be run from inetd. It is quite useful for embedded routers like OpenWRT, but still suffers from an hard dep on POSIX shell and inetd.\r\n\r\n!! SNMP\r\n\r\nSNMP is another way to monitor nodes. While it works really well, it mostly suffers the fact that its configuration is quite different of the usual way, so I guess some things will change on that side.\r\n\r\n!! Win32 ports\r\n\r\nWin32 has long been a very difficult OS to monitor, as it doesn't offer much of the UNIX-esque features. Yet the number of win32 nodes that one wants to monitor is quite high, as it makes munin one the few systems that can easily monitor heterogeneous systems.\r\n\r\nTherefore, while you can install the stock munin-node, several projects emerged. We decided to adopt [munin-node-win32|http://munin-monitoring.org/browser/munin-node-win32|en].\r\n\r\n!! Android\r\n\r\nThere's also a dedicated [node for Android|https://github.com/silverchris/Android-Munin-Node|en]. It makes sense, given that the Android is yet Linux-derived, but lacks Perl, and is a Java mostly platform. This node also has some basic capabilities of pushing data to the master instead of the usual polling. \r\n\r\nThis is specially interesting given the fact that Android nodes are usually loosely connected, so the node spools values itself and pushes them when it recovers connectivity. \r\n\r\nNote that this is specifically an aspect that is currently lacking in munin, and I'm planning to address it in the 2.1 series. So thanks to its author for showing a relevant use-case.\r\n \r\n!! C\r\n\r\nThat's my last experiment. It started with a simple question : ''how difficult would it be to code a fairly portable version of the node ?''\r\n\r\nIt turned out that it wasn't that difficult. I'm even asking myself about eventually replacing the win32 specific port with this one, as the code is much simpler. The win32 node has several plugin built-in mostly due to platform specifics. I still have to find a way to work my way around it, but it's in quite good shape.\r\n\r\nThis post was originally done to promote it, but while writing it I noticed that the ecosystem deserved a post on its own. So I'll write another one, specific to the C port of munin-node and plugins.\r\n\r\n!!! Master\r\n\r\nThe master is the most complex component. So rewrites of it won't happen as-is. They usually take the form of a bridge between the munin protocol and another graphing system, such as Graphite. \r\n\r\n!! Clients\r\n\r\nThere are also client libraries that are able to directly query munin nodes, to be able to reuse the vast ecosystem. Languages are various, from the obvious [Python|http://julien.danjou.info/projects/pymunincli/|en] to [Ruby|https://github.com/sosedoff/munin-ruby|en], along with a quite modern [node.js|https://github.com/Redpill-Linpro/node-munin-client] one.","<h2>KISS is the core design of Munin</h2>\n<p>Munin's greatest strength is its very KISS architecture. It therefore gets\nmany things right, such as a huge modularity.</p>\n<p>Each component (master/node/plugin) has a simple API to communicate with the\nothers.</p>\n<h2>Spin-offs ...</h2>\n<p>I admit that the master, even the node, have convoluted code. In fact some\nrewrites already do exist.</p>\n<h3>... are welcomed ...</h3>\n<p>And they are a really good thing, as it enables rapid prototyping on things\nthat the stock munin has (currently) trouble to do.</p>\n<p>The stock munin is a piece of software that many depend upon, so it has to\nmove at a much slower pace than one does want, even me. As much as I really\nwant to add many many features to it, I still have to take extra care that it\ndoesn't break stuff, even the least known features.</p>\n<p>So I take munin off-springs very seriously and even offer as much help as I\ncan in order for them to succeed.</p>\n<h3>... because they are very valuable in the long term</h3>\n<p>In my opinion competition is only short bad in the short term, and in the\nlong term they usually add significant value to the whole ecosystem. That said,\nthere's always a risk to become slowly irrelevant, but I think that's the real\npower of open-source's evolutionary paradigm : embrace them or become obsolete\nand get replaced.</p>\n<p>Since, if someone takes the time to author a competitor that has a real\nthreat potential, it mostly means that there's a real itch to scratch and that\nmany things are to be learnt.</p>\n<h2>Different layers of spin-offs</h2>\n<p>The munin ecosystem is divided in 3 main categories, obviously related to\nthe 3 main components of munin : master, node &amp; plugin.</p>\n<h3>Plugins</h3>\n<p>That's the most obvious part as custom plugins are the real <em>bread and\nbutter</em> of munin.</p>\n<p>Stock plugins are mostly written in Perl or POSIX shell, as Perl is munin's\nown language and POSIX shell is ubiquitous. That fact is acknowledged by the\nfact that core munin provides 2 libraries (Perl &amp; Shell) to help plugin\nauthoring.</p>\n<p>So, it's quite natural that each mainstream language has grown its own\nplugin library. Some language even have <a href=\"https://github.com/aouyar/PyMunin\" hreflang=\"en\">two</a> of <a href=\"https://github.com/samuel/python-munin\" hreflang=\"en\">them</a>.</p>\n<h4>C</h4>\n<p>Some plugins got even rewritten in plain C, as it was shown that shell\nplugins do have a significant impact on very under-powered nodes, such as\nembedded routers.</p>\n<h3>Node</h3>\n<p>This component is very simple. Yet, it has to be run on all the nodes that\none wants to monitor. It is currently written in Perl, and while that's not an\nissue on UNIX-like systems, it can be quite problematic on embedded ones</p>\n<h4>Simple munin</h4>\n<p>The official package comes with a POSIX shell rewrite that has to be run\nfrom inetd. It is quite useful for embedded routers like OpenWRT, but still\nsuffers from an hard dep on POSIX shell and inetd.</p>\n<h4>SNMP</h4>\n<p>SNMP is another way to monitor nodes. While it works really well, it mostly\nsuffers the fact that its configuration is quite different of the usual way, so\nI guess some things will change on that side.</p>\n<h4>Win32 ports</h4>\n<p>Win32 has long been a very difficult OS to monitor, as it doesn't offer much\nof the UNIX-esque features. Yet the number of win32 nodes that one wants to\nmonitor is quite high, as it makes munin one the few systems that can easily\nmonitor heterogeneous systems.</p>\n<p>Therefore, while you can install the stock munin-node, several projects\nemerged. We decided to adopt <a href=\"http://munin-monitoring.org/browser/munin-node-win32\" hreflang=\"en\">munin-node-win32</a>.</p>\n<h4>Android</h4>\n<p>There's also a dedicated <a href=\"https://github.com/silverchris/Android-Munin-Node\" hreflang=\"en\">node for\nAndroid</a>. It makes sense, given that the Android is yet Linux-derived, but\nlacks Perl, and is a Java mostly platform. This node also has some basic\ncapabilities of pushing data to the master instead of the usual polling.</p>\n<p>This is specially interesting given the fact that Android nodes are usually\nloosely connected, so the node spools values itself and pushes them when it\nrecovers connectivity.</p>\n<p>Note that this is specifically an aspect that is currently lacking in munin,\nand I'm planning to address it in the 2.1 series. So thanks to its author for\nshowing a relevant use-case.</p>\n<h4>C</h4>\n<p>That's my last experiment. It started with a simple question : <em>how\ndifficult would it be to code a fairly portable version of the node ?</em></p>\n<p>It turned out that it wasn't that difficult. I'm even asking myself about\neventually replacing the win32 specific port with this one, as the code is much\nsimpler. The win32 node has several plugin built-in mostly due to platform\nspecifics. I still have to find a way to work my way around it, but it's in\nquite good shape.</p>\n<p>This post was originally done to promote it, but while writing it I noticed\nthat the ecosystem deserved a post on its own. So I'll write another one,\nspecific to the C port of munin-node and plugins.</p>\n<h3>Master</h3>\n<p>The master is the most complex component. So rewrites of it won't happen\nas-is. They usually take the form of a bridge between the munin protocol and\nanother graphing system, such as Graphite.</p>\n<h4>Clients</h4>\n<p>There are also client libraries that are able to directly query munin nodes,\nto be able to reuse the vast ecosystem. Languages are various, from the obvious\n<a href=\"http://julien.danjou.info/projects/pymunincli/\" hreflang=\"en\">Python</a> to <a href=\"https://github.com/sosedoff/munin-ruby\" hreflang=\"en\">Ruby</a>, along with a quite modern <a href=\"https://github.com/Redpill-Linpro/node-munin-client\">node.js</a> one.</p>","","spinoffs the munin ecosystem kiss the core design munin munin greatest strength its very kiss architecture therefore gets many things right such huge modularity each component master node plugin has simple api communicate with the others spin offs admit that the master even the node have convoluted code fact some rewrites already exist are welcomed and they are really good thing enables rapid prototyping things that the stock munin has currently trouble the stock munin piece software that many depend upon has move much slower pace than one does want even much really want add many many features still have take extra care that doesn break stuff even the least known features take munin off springs very seriously and even offer much help can order for them succeed because they are very valuable the long term opinion competition only short bad the short term and the long term they usually add significant value the whole ecosystem that said there always risk become slowly irrelevant but think that the real power open source evolutionary paradigm embrace them become obsolete and get replaced since someone takes the time author competitor that has real threat potential mostly means that there real itch scratch and that many things are learnt different layers spin offs the munin ecosystem divided main categories obviously related the main components munin master node amp plugin plugins that the most obvious part custom plugins are the real bread and butter munin stock plugins are mostly written perl posix shell perl munin own language and posix shell ubiquitous that fact acknowledged the fact that core munin provides libraries perl amp shell help plugin authoring quite natural that each mainstream language has grown its own plugin library some language even have two them some plugins got even rewritten plain was shown that shell plugins have significant impact very under powered nodes such embedded routers node this component very simple yet has run all the nodes that one wants monitor currently written perl and while that not issue unix like systems can quite problematic embedded ones simple munin the official package comes with posix shell rewrite that has run from inetd quite useful for embedded routers like openwrt but still suffers from hard dep posix shell and inetd snmp snmp another way monitor nodes while works really well mostly suffers the fact that its configuration quite different the usual way guess some things will change that side win32 ports win32 has long been very difficult monitor doesn offer much the unix esque features yet the number win32 nodes that one wants monitor quite high makes munin one the few systems that can easily monitor heterogeneous systems therefore while you can install the stock munin node several projects emerged decided adopt munin node win32 android there also dedicated node for android makes sense given that the android yet linux derived but lacks perl and java mostly platform this node also has some basic capabilities pushing data the master instead the usual polling this specially interesting given the fact that android nodes are usually loosely connected the node spools values itself and pushes them when recovers connectivity note that this specifically aspect that currently lacking munin and planning address the series thanks its author for showing relevant use case that last experiment started with simple question how difficult would code fairly portable version the node turned out that wasn that difficult even asking myself about eventually replacing the win32 specific port with this one the code much simpler the win32 node has several plugin built mostly due platform specifics still have find way work way around but quite good shape this post was originally done promote but while writing noticed that the ecosystem deserved post its own write another one specific the port munin node and plugins master the master the most complex component rewrites won happen they usually take the form bridge between the munin protocol and another graphing system such graphite clients there are also client libraries that are able directly query munin nodes able reuse the vast ecosystem languages are various from the obvious python ruby along with quite modern node one","a:1:{s:3:\"tag\";a:3:{i:0;s:9:\"ecosystem\";i:1;s:8:\"sysadmin\";i:2;s:5:\"munin\";}}","1","0","1","0","0","0","0"
"468806","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38366","2009-12-15 23:00:00","Europe/Paris","2009-12-15 16:28:59","2009-12-16 13:08:49","","post","wiki","2009/12/Use-Both-Offensive-Defensive-Programming","en","Offensive or Defensive Programming ? Use Both !","","","!!!! The rational behind defensive programming\r\n\r\nDefensive programming is a \r\n\r\n!!!! The problem of defensive programming\r\n\r\n!!!! The best defence is offence\r\n\r\nhttp://c2.com/cgi/wiki?OffensiveProgramming\r\n\r\n!!!!","<h2>The rational behind defensive programming</h2>\n<p>Defensive programming is a</p>\n<h2>The problem of defensive programming</h2>\n<h2>The best defence is offence</h2>\n<p>http://c2.com/cgi/wiki?OffensiveProgramming</p>","","offensive defensive programming use both the rational behind defensive programming defensive programming the problem defensive programming the best defence offence http com cgi wiki offensiveprogramming","a:0:{}","-2","0","1","0","0","0","0"
"391276","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-04-17 17:05:00","Europe/Paris","2009-04-07 07:09:38","2009-05-18 10:09:06","","post","wiki","2009/04/Databases:-Partial-Indexing","en","Databases: Partial Indexing","In huge tables, usually only a fraction of the table is used on a regular basis. The rest is useful either for historical purpose, or is just not being treated right now.","<p>In huge tables, usually only a fraction of the table is used on a regular\nbasis. The rest is useful either for historical purpose, or is just not being\ntreated right now.</p>","Let's take the table that contains orders that we created in [my previous post|/post/2009/03/Databases%3A-Meta-Data-ctime-mtime] to show you some examples.\r\n\r\nOrders have usually a state, Let's create it $$Notice that here I don't create all the meta-data stuff. That is left as an exercise to the reader - Yeah, I __always__ wanted to say that...$$ : \r\n\r\n///\r\nALTER TABLE ORDERS ADD COLUMN STATE\r\n///\r\n\r\nNow usually a application makes use of the state of the order by querying it, even quite often. The main problem is that this query will do a full scan of the table unless the state field is indexed. Here comes the other issue : indexes take very much space since they have to be ordered and cannot be trimmed (except throw an expensive DROP/CREATE or REINDEX). \r\n\r\nThe space used is adequate if the index has many different values, but usually a state has only a finite set of values, and usually some are very transient and other quite long. Compare for example the state  CANCELLED or SHIPPED that are mostly final to the very transient state INVOICE_TO_BE_PRINTED.\r\n\r\nThe number of orders that are in the state INVOICE_TO_BE_PRINTED are quite a few (especially compared to the final states). However these temporary states are the mostly queried (usually by batches, pollers or even web pages that present the workload to the users for ''manual polling'').\r\n\r\nAnother overlooked usage is to have an ''ALMOST''-UNIQUE index. I mean an unique index, but only on selected entries. In our ORDERS table, we can add a field that represent the PRINTING_PID of the process that is responsible for printing the invoice. It is only unique for the orders that are in the INVOICE_TO_BE_PRINTED state. A normal index isn't much of a help here since either you have to clear it afterwards, but you loose information, or you cannot enforce any uniqueness.\r\n\r\nSo there comes the PARTIAL INDEX. It is native in some databases$$MS-SQL has it but calls it Filtered Index. PostgreSQL calls it PARTIAL INDEX. )$$ but in most it isn't. Triggers comes then to the rescue. Some are not really fond of triggers since many things can happen undercover, but my moto is to use trigger in order to, and only to, maintain business-agnostic extra datas (usually to cope automatically with denormalization). Triggers shall not __change__ the data, merely copy and/or move it around.\r\n\r\nThe new ''almost-unique'' column is also created :\r\n\r\n///\r\nALTER TABLE ORDERS ADD COLUMN PRINTING_ID\r\n///\r\n\r\n!!!! Implementations\r\n\r\n!!! Reducing the index size and the overhead\r\n\r\nSo, a new table is created to contain the partial index : \r\n\r\n///\r\nTABLE PI_ORDERS_STATE ( \r\n   INT ORDERS_ID FOREIGN KEY ON ORDERS(ORDERS_ID),\r\n   STATE, \r\n   INDEX ON (STATE)\r\n)\r\n///\r\n\r\nAnd the insertion/deletion for the table is done via triggers. I do it a little rapidly since the real creation is a little more tedious than the pseudo-code here :\r\n\r\n///\r\nCREATE TRIGGER ON ORDERS WHERE INSERT, UPDATE\r\n  WHEN o.STATE not in (PENDING, CANCELLED, SHIPPED) \r\n    INSERT INTO PI_ORDERS_STATE(o.ORDERS_ID, o.STATE) IF ORDERS_ID NOT PRESENT\r\n  ELSE\r\n    DELETE FROM PI_ORDERS_STATE \r\n      WHERE ORDERS_ID = o.ORDERS_ID\r\n///\r\n\r\nOne should rewrite queries to take advantage of this partial index since when it's integrated in the database the optimizer just does it himself. A query like this \r\n\r\nOld :\r\n///\r\nSELECT * FROM ORDERS WHERE ORDERS.STATE = INVOICE_TO_BE_PRINTED\r\n///\r\n\r\nNew : \r\n///\r\nSELECT o.* FROM ORDERS o\r\nINNER JOIN PI_ORDERS_STATE p ON p.ORDERS_ID = o.ORDERS_ID p.STATE = INVOICE_TO_BE_PRINTED\r\n///\r\n\r\nThe query optimizer should now be able to use the small index in PI_ORDERS_STATE, and retrieve only the relevant orders via their primary key in ORDERS.\r\n\r\n!!! Almost-unique colums\r\n\r\nTo handle the almost-unique column, it's basically the same idea : a new table, new triggers : \r\n\r\n///\r\nTABLE PI_ORDERS_PRINTING_PID ( \r\n   INT ORDERS_ID FOREIGN KEY ON ORDERS(ORDERS_ID),\r\n   PRINTING_PID, \r\n   UNIQUE INDEX ON (PRINTING_PID)\r\n)\r\n///\r\n\r\n///\r\nCREATE TRIGGER ON ORDERS WHERE INSERT, UPDATE\r\n  WHEN o.STATE = INVOICE_TO_BE_PRINTED\r\n    INSERT INTO PI_ORDERS_PRINTING_PID(o.ORDERS_ID, o.PRINTING_PID) IF ORDERS_ID NOT PRESENT\r\n  ELSE\r\n    DELETE FROM PI_ORDERS_PRINTING_PID\r\n      WHERE ORDERS_ID = o.ORDERS_ID\r\n///\r\n\r\nNo need to rewrite any query here.","<p>Let's take the table that contains orders that we created in <a href=\"/post/2009/03/Databases%3A-Meta-Data-ctime-mtime\">my previous post</a> to show\nyou some examples.</p>\n<p>Orders have usually a state, Let's create it <sup>[<a href=\"#pnote-391276-1\" id=\"rev-pnote-391276-1\" name=\"rev-pnote-391276-1\">1</a>]</sup> :</p>\n<pre>\nALTER TABLE ORDERS ADD COLUMN STATE\n</pre>\n<p>Now usually a application makes use of the state of the order by querying\nit, even quite often. The main problem is that this query will do a full scan\nof the table unless the state field is indexed. Here comes the other issue :\nindexes take very much space since they have to be ordered and cannot be\ntrimmed (except throw an expensive DROP/CREATE or REINDEX).</p>\n<p>The space used is adequate if the index has many different values, but\nusually a state has only a finite set of values, and usually some are very\ntransient and other quite long. Compare for example the state CANCELLED or\nSHIPPED that are mostly final to the very transient state\nINVOICE_TO_BE_PRINTED.</p>\n<p>The number of orders that are in the state INVOICE_TO_BE_PRINTED are quite a\nfew (especially compared to the final states). However these temporary states\nare the mostly queried (usually by batches, pollers or even web pages that\npresent the workload to the users for <em>manual polling</em>).</p>\n<p>Another overlooked usage is to have an <em>ALMOST</em>-UNIQUE index. I mean\nan unique index, but only on selected entries. In our ORDERS table, we can add\na field that represent the PRINTING_PID of the process that is responsible for\nprinting the invoice. It is only unique for the orders that are in the\nINVOICE_TO_BE_PRINTED state. A normal index isn't much of a help here since\neither you have to clear it afterwards, but you loose information, or you\ncannot enforce any uniqueness.</p>\n<p>So there comes the PARTIAL INDEX. It is native in some\ndatabases<sup>[<a href=\"#pnote-391276-2\" id=\"rev-pnote-391276-2\" name=\"rev-pnote-391276-2\">2</a>]</sup> but in most it isn't. Triggers comes then to\nthe rescue. Some are not really fond of triggers since many things can happen\nundercover, but my moto is to use trigger in order to, and only to, maintain\nbusiness-agnostic extra datas (usually to cope automatically with\ndenormalization). Triggers shall not <strong>change</strong> the data, merely\ncopy and/or move it around.</p>\n<p>The new <em>almost-unique</em> column is also created :</p>\n<pre>\nALTER TABLE ORDERS ADD COLUMN PRINTING_ID\n</pre>\n<h2>Implementations</h2>\n<h3>Reducing the index size and the overhead</h3>\n<p>So, a new table is created to contain the partial index :</p>\n<pre>\nTABLE PI_ORDERS_STATE ( \n   INT ORDERS_ID FOREIGN KEY ON ORDERS(ORDERS_ID),\n   STATE, \n   INDEX ON (STATE)\n)\n</pre>\n<p>And the insertion/deletion for the table is done via triggers. I do it a\nlittle rapidly since the real creation is a little more tedious than the\npseudo-code here :</p>\n<pre>\nCREATE TRIGGER ON ORDERS WHERE INSERT, UPDATE\n  WHEN o.STATE not in (PENDING, CANCELLED, SHIPPED) \n    INSERT INTO PI_ORDERS_STATE(o.ORDERS_ID, o.STATE) IF ORDERS_ID NOT PRESENT\n  ELSE\n    DELETE FROM PI_ORDERS_STATE \n      WHERE ORDERS_ID = o.ORDERS_ID\n</pre>\n<p>One should rewrite queries to take advantage of this partial index since\nwhen it's integrated in the database the optimizer just does it himself. A\nquery like this</p>\n<p>Old :</p>\n<pre>\nSELECT * FROM ORDERS WHERE ORDERS.STATE = INVOICE_TO_BE_PRINTED\n</pre>\n<p>New :</p>\n<pre>\nSELECT o.* FROM ORDERS o\nINNER JOIN PI_ORDERS_STATE p ON p.ORDERS_ID = o.ORDERS_ID p.STATE = INVOICE_TO_BE_PRINTED\n</pre>\n<p>The query optimizer should now be able to use the small index in\nPI_ORDERS_STATE, and retrieve only the relevant orders via their primary key in\nORDERS.</p>\n<h3>Almost-unique colums</h3>\n<p>To handle the almost-unique column, it's basically the same idea : a new\ntable, new triggers :</p>\n<pre>\nTABLE PI_ORDERS_PRINTING_PID ( \n   INT ORDERS_ID FOREIGN KEY ON ORDERS(ORDERS_ID),\n   PRINTING_PID, \n   UNIQUE INDEX ON (PRINTING_PID)\n)\n</pre>\n<pre>\nCREATE TRIGGER ON ORDERS WHERE INSERT, UPDATE\n  WHEN o.STATE = INVOICE_TO_BE_PRINTED\n    INSERT INTO PI_ORDERS_PRINTING_PID(o.ORDERS_ID, o.PRINTING_PID) IF ORDERS_ID NOT PRESENT\n  ELSE\n    DELETE FROM PI_ORDERS_PRINTING_PID\n      WHERE ORDERS_ID = o.ORDERS_ID\n</pre>\n<p>No need to rewrite any query here.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-391276-1\" id=\"pnote-391276-1\" name=\"pnote-391276-1\">1</a>] Notice that here I don't create all the meta-data\nstuff. That is left as an exercise to the reader - Yeah, I\n<strong>always</strong> wanted to say that...</p>\n<p>[<a href=\"#rev-pnote-391276-2\" id=\"pnote-391276-2\" name=\"pnote-391276-2\">2</a>] MS-SQL has it but calls it Filtered Index. PostgreSQL\ncalls it PARTIAL INDEX. )</p>\n</div>","","databases partial indexing huge tables usually only fraction the table used regular basis the rest useful either for historical purpose just not being treated right now let take the table that contains orders that created previous post show you some examples orders have usually state let create alter table orders add column state now usually application makes use the state the order querying even quite often the main problem that this query will full scan the table unless the state field indexed here comes the other issue indexes take very much space since they have ordered and cannot trimmed except throw expensive drop create reindex the space used adequate the index has many different values but usually state has only finite set values and usually some are very transient and other quite long compare for example the state cancelled shipped that are mostly final the very transient state invoice printed the number orders that are the state invoice printed are quite few especially compared the final states however these temporary states are the mostly queried usually batches pollers even web pages that present the workload the users for manual polling another overlooked usage have almost unique index mean unique index but only selected entries our orders table can add field that represent the printing pid the process that responsible for printing the invoice only unique for the orders that are the invoice printed state normal index isn much help here since either you have clear afterwards but you loose information you cannot enforce any uniqueness there comes the partial index native some databases but most isn triggers comes then the rescue some are not really fond triggers since many things can happen undercover but moto use trigger order and only maintain business agnostic extra datas usually cope automatically with denormalization triggers shall not change the data merely copy and move around the new almost unique column also created alter table orders add column printing implementations reducing the index size and the overhead new table created contain the partial index table orders state int orders foreign key orders orders state index state and the insertion deletion for the table done via triggers little rapidly since the real creation little more tedious than the pseudo code here create trigger orders where insert update when state not pending cancelled shipped insert into orders state orders state orders not present else delete from orders state where orders orders one should rewrite queries take advantage this partial index since when integrated the database the optimizer just does himself query like this old select from orders where orders state invoice printed new select from orders inner join orders state orders orders state invoice printed the query optimizer should now able use the small index orders state and retrieve only the relevant orders via their primary key orders almost unique colums handle the almost unique column basically the same idea new table new triggers table orders printing pid int orders foreign key orders orders printing pid unique index printing pid create trigger orders where insert update when state invoice printed insert into orders printing pid orders printing pid orders not present else delete from orders printing pid where orders orders need rewrite any query here notes notice that here don create all the meta data stuff that left exercise the reader yeah always wanted say that sql has but calls filtered index postgresql calls partial index","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","1","0","1","0","0","0","0"
"397657","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2010-02-20 15:18:00","Europe/Paris","2009-04-27 16:13:30","2010-02-20 14:26:25","","post","wiki","2010/02/Immutability-of-an-URL","en","Immutability of an URL","","","In the pure spirit of [Data is King|http://www.javapractices.com/topic/TopicAction.do?Id=211|en] I think that URL should __never__ change. Even the W3C agrees with their [Cool URIs don't change|http://www.w3.org/Provider/Style/URI.html|en] article.\r\n\r\nBut we all know that in IT ''never'' is only ''not in the foreseen future''. So URL __do__ change, at least after a while, and usually for technical reasons$$upgrade to another blog engine...$$. \r\n\r\nSince you can update your website to update the URLs, but the inbound link cannot be easily updated. To handle this need, the HTTP protocol has specified the 301 response code. \r\n\r\n\r\nThe solution is that the site should remember all the urls that it generated and redirects accordingly. This way you'll never loose a potential reader to the infamous 404 (this page does not exist).\r\n\r\nSome sites even try to approximate the page on a custom 404 page. That's another reason to have user-friendly urls : to be able to hint your reader to appropriate pages in case you don't find his initial destination.\r\n\r\nSadly, this redirect behavior isn't supported by my blog engine (dotclear)... That's for the [eat your own dog's food|http://en.wikipedia.org/wiki/Eating_one%27s_own_dog_food|en], but I'm looking forward to do it on my current blogging platform.","<p>In the pure spirit of <a href=\"http://www.javapractices.com/topic/TopicAction.do?Id=211\" hreflang=\"en\">Data\nis King</a> I think that URL should <strong>never</strong> change. Even the W3C\nagrees with their <a href=\"http://www.w3.org/Provider/Style/URI.html\" hreflang=\"en\">Cool URIs don't change</a> article.</p>\n<p>But we all know that in IT <em>never</em> is only <em>not in the foreseen\nfuture</em>. So URL <strong>do</strong> change, at least after a while, and\nusually for technical reasons<sup>[<a href=\"#pnote-397657-1\" id=\"rev-pnote-397657-1\" name=\"rev-pnote-397657-1\">1</a>]</sup>.</p>\n<p>Since you can update your website to update the URLs, but the inbound link\ncannot be easily updated. To handle this need, the HTTP protocol has specified\nthe 301 response code.</p>\n<p>The solution is that the site should remember all the urls that it generated\nand redirects accordingly. This way you'll never loose a potential reader to\nthe infamous 404 (this page does not exist).</p>\n<p>Some sites even try to approximate the page on a custom 404 page. That's\nanother reason to have user-friendly urls : to be able to hint your reader to\nappropriate pages in case you don't find his initial destination.</p>\n<p>Sadly, this redirect behavior isn't supported by my blog engine\n(dotclear)... That's for the <a href=\"http://en.wikipedia.org/wiki/Eating_one%27s_own_dog_food\" hreflang=\"en\">eat\nyour own dog's food</a>, but I'm looking forward to do it on my current\nblogging platform.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-397657-1\" id=\"pnote-397657-1\" name=\"pnote-397657-1\">1</a>] upgrade to another blog engine...</p>\n</div>","","immutability url the pure spirit data king think that url should never change even the w3c agrees with their cool uris don change article but all know that never only not the foreseen future url change least after while and usually for technical reasons since you can update your website update the urls but the inbound link cannot easily updated handle this need the http protocol has specified the 301 response code the solution that the site should remember all the urls that generated and redirects accordingly this way you never loose potential reader the infamous 404 this page does not exist some sites even try approximate the page custom 404 page that another reason have user friendly urls able hint your reader appropriate pages case you don find his initial destination sadly this redirect behavior isn supported blog engine dotclear that for the eat your own dog food but looking forward current blogging platform notes upgrade another blog engine","a:0:{}","1","0","1","0","0","0","0"
"89222","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2007-03-17 16:00:00","Europe/Paris","2007-03-17 15:00:52","2007-12-31 10:00:46","","post","wiki","2007/03/17/First-Real-Post","en","First (Real) Post","","","This is my first attempt at making a live blog. It should have been a way of expressing my progress of one of my pet project : [Personal Workflow|http://sourceforge.net/projects/pwkf/|en]. \r\n\r\nAs with every pet projects, there is such an excitation at the beginning, but after a while it drops rapidely down to zero. \r\n\r\nIt's quite the same effect when your tendency to postpone is too high as Andy Hunt and Dave Thomas said : [Don't Live with Broken Windows|http://www.artima.com/intv/fixit.html|en] !. It will always be {{If I could postpone it at that time, I can still postpone it.}}, and then it'll take an enormous amount of willpower to be even started.","<p>This is my first attempt at making a live blog. It should have been a way of\nexpressing my progress of one of my pet project : <a href=\"http://sourceforge.net/projects/pwkf/\" hreflang=\"en\">Personal\nWorkflow</a>.</p>\n<p>As with every pet projects, there is such an excitation at the beginning,\nbut after a while it drops rapidely down to zero.</p>\n<p>It's quite the same effect when your tendency to postpone is too high as\nAndy Hunt and Dave Thomas said : <a href=\"http://www.artima.com/intv/fixit.html\" hreflang=\"en\">Don't Live with Broken\nWindows</a> !. It will always be <q>If I could postpone it at that time, I can\nstill postpone it.</q>, and then it'll take an enormous amount of willpower to\nbe even started.</p>","","first real post this first attempt making live blog should have been way expressing progress one pet project personal workflow with every pet projects there such excitation the beginning but after while drops rapidely down zero quite the same effect when your tendency postpone too high andy hunt and dave thomas said don live with broken windows will always could postpone that time can still postpone and then take enormous amount willpower even started","a:0:{}","1","0","1","0","0","0","0"
"267373","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2008-11-04 20:05:00","Europe/Paris","2008-08-08 21:26:39","2009-12-15 16:24:29","","post","wiki","2008/08/08/A-Java-Munin-Node-to-Monitor-a-JVM","en","A Java Munin Node to Monitor a JVM","","","Why a whole Node ?\r\n* a JVM is like a host\r\n* can register some custom elements\r\n* has some predef elements\r\n* very easy (munin power is the architecture, and the protocol)\r\n* multiple hosts -- multiple web sites\r\n\r\nMaybe osoblete since there are JMX plugins for Munin now.","<p>Why a whole Node ?</p>\n<ul>\n<li>a JVM is like a host</li>\n<li>can register some custom elements</li>\n<li>has some predef elements</li>\n<li>very easy (munin power is the architecture, and the protocol)</li>\n<li>multiple hosts -- multiple web sites</li>\n</ul>\n<p>Maybe osoblete since there are JMX plugins for Munin now.</p>","","java munin node monitor jvm why whole node jvm like host can register some custom elements has some predef elements very easy munin power the architecture and the protocol multiple hosts multiple web sites maybe osoblete since there are jmx plugins for munin now","a:1:{s:3:\"tag\";a:1:{i:0;s:5:\"munin\";}}","-2","0","1","0","0","0","0"
"395563","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2009-07-15 20:55:00","Europe/Paris","2009-04-19 12:32:30","2009-07-15 18:56:17","","post","wiki","2009/07/Checked-or-Unchecked-Exceptions-for-Legacy-Code","en","Checked or Unchecked Exceptions for Legacy Code ?","This question can almost trigger a __religious war__. On Internet __no fixed consensus__ on the matter seems to __exists__. But most of the articles there have usually __something in common__ : they are always mostly __applicable__ when you start a __new application__.\r\n\r\nIs the situation the same when you have a certain amount of __existing code to maintain__ ?","<p>This question can almost trigger a <strong>religious war</strong>. On\nInternet <strong>no fixed consensus</strong> on the matter seems to\n<strong>exists</strong>. But most of the articles there have usually\n<strong>something in common</strong> : they are always mostly\n<strong>applicable</strong> when you start a <strong>new\napplication</strong>.</p>\n<p>Is the situation the same when you have a certain amount of <strong>existing\ncode to maintain</strong> ?</p>","!!!! Checked Exceptions are breaking encapsulation \r\n\r\nIn theory it's quite nice since the called code can communicate with it's caller when something unexpected happened.\r\n\r\nThe key word here is ''++unexpected++''. If you have to explicitly know the exceptions that could occur, it's not really unexpected. And if it's not unexpected, using exception handling just add an ''out-of-band'' data path. It's on par with transporting data in a private class field when calling a member instead of using its arguments. This leads to breaking encapsulation as Alan Griffiths wrote in [Exceptional Java|http://www.octopull.demon.co.uk/java/ExceptionalJava.html|en]. This vision is also shared by Bruce Eckel in his article entitled [Does Java need Checked Exceptions|http://www.mindview.net/Etc/Discussions/CheckedExceptions|en]?.\r\n\r\n!!!! Checked Exceptions are quite painful to use\r\n\r\n!!! Local Exception Handling is hard to manage \r\n\r\nI personally find checked exceptions quite painful to use. By definition, you have to catch every exception that is thrown by the underlaying code. \r\n\r\nAnd if the underlaying code doesn't know what to do with the exception, chances are, that you don't know either, so you just pass the exception to the caller. And so on...\r\n\r\n!!! Too much code to change \r\n\r\nTherefore exceptions are usually caught at the top level with a generic catch-all structure that logs the error, since no layer could sensibly do something clever with the exception. \r\n\r\nThen you just have to change all the signature of the whole stack, just to be able to catch them at the top. Using unchecked exception lets you have this for free, and conveys the meaning that nothing is caught until the top.\r\n\r\n!!!! Unchecked Exceptions might be dangerous... \r\n\r\nObviously, unchecked means not checked. So you might fail to catch them at the top level and then the whole application crashes. Checked exceptions are a safeguard against that. Just like strong static typing is. \r\n\r\n__You trade compile-time safety__ (checked) __for development-time speed and ease__ (unchecked).\r\n\r\n!!!! ... but are not really.\r\n\r\nOn the other hand, if you have a good design, you don't have much different top levels, and then the risk is somewhat limited.\r\n\r\n__Moreover, since you ''always'' have to take unchecked one into account, why don't use them ?__\r\n\r\nBy the way, in C++, the exceptions are unchecked by default.","<h2>Checked Exceptions are breaking encapsulation</h2>\n<p>In theory it's quite nice since the called code can communicate with it's\ncaller when something unexpected happened.</p>\n<p>The key word here is <em><ins>unexpected</ins></em>. If you have to\nexplicitly know the exceptions that could occur, it's not really unexpected.\nAnd if it's not unexpected, using exception handling just add an\n<em>out-of-band</em> data path. It's on par with transporting data in a private\nclass field when calling a member instead of using its arguments. This leads to\nbreaking encapsulation as Alan Griffiths wrote in <a href=\"http://www.octopull.demon.co.uk/java/ExceptionalJava.html\" hreflang=\"en\">Exceptional Java</a>. This vision is also shared by Bruce Eckel in his\narticle entitled <a href=\"http://www.mindview.net/Etc/Discussions/CheckedExceptions\" hreflang=\"en\">Does\nJava need Checked Exceptions</a>?.</p>\n<h2>Checked Exceptions are quite painful to use</h2>\n<h3>Local Exception Handling is hard to manage</h3>\n<p>I personally find checked exceptions quite painful to use. By definition,\nyou have to catch every exception that is thrown by the underlaying code.</p>\n<p>And if the underlaying code doesn't know what to do with the exception,\nchances are, that you don't know either, so you just pass the exception to the\ncaller. And so on...</p>\n<h3>Too much code to change</h3>\n<p>Therefore exceptions are usually caught at the top level with a generic\ncatch-all structure that logs the error, since no layer could sensibly do\nsomething clever with the exception.</p>\n<p>Then you just have to change all the signature of the whole stack, just to\nbe able to catch them at the top. Using unchecked exception lets you have this\nfor free, and conveys the meaning that nothing is caught until the top.</p>\n<h2>Unchecked Exceptions might be dangerous...</h2>\n<p>Obviously, unchecked means not checked. So you might fail to catch them at\nthe top level and then the whole application crashes. Checked exceptions are a\nsafeguard against that. Just like strong static typing is.</p>\n<p><strong>You trade compile-time safety</strong> (checked) <strong>for\ndevelopment-time speed and ease</strong> (unchecked).</p>\n<h2>... but are not really.</h2>\n<p>On the other hand, if you have a good design, you don't have much different\ntop levels, and then the risk is somewhat limited.</p>\n<p><strong>Moreover, since you <em>always</em> have to take unchecked one into\naccount, why don't use them ?</strong></p>\n<p>By the way, in C++, the exceptions are unchecked by default.</p>","","checked unchecked exceptions for legacy code this question can almost trigger religious war internet fixed consensus the matter seems exists but most the articles there have usually something common they are always mostly applicable when you start new application the situation the same when you have certain amount existing code maintain checked exceptions are breaking encapsulation theory quite nice since the called code can communicate with caller when something unexpected happened the key word here unexpected you have explicitly know the exceptions that could occur not really unexpected and not unexpected using exception handling just add out band data path par with transporting data private class field when calling member instead using its arguments this leads breaking encapsulation alan griffiths wrote exceptional java this vision also shared bruce eckel his article entitled does java need checked exceptions checked exceptions are quite painful use local exception handling hard manage personally find checked exceptions quite painful use definition you have catch every exception that thrown the underlaying code and the underlaying code doesn know what with the exception chances are that you don know either you just pass the exception the caller and too much code change therefore exceptions are usually caught the top level with generic catch all structure that logs the error since layer could sensibly something clever with the exception then you just have change all the signature the whole stack just able catch them the top using unchecked exception lets you have this for free and conveys the meaning that nothing caught until the top unchecked exceptions might dangerous obviously unchecked means not checked you might fail catch them the top level and then the whole application crashes checked exceptions are safeguard against that just like strong static typing you trade compile time safety checked for development time speed and ease unchecked but are not really the other hand you have good design you don have much different top levels and then the risk somewhat limited moreover since you always have take unchecked one into account why don use them the way the exceptions are unchecked default","a:0:{}","1","0","1","0","2","0","0"
"407344","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-06-25 08:00:00","Europe/Paris","2009-06-03 08:30:55","2010-06-11 13:34:41","","post","wiki","2009/06/Data-Is-King-versus-Code-Is-King","en","\"Data Is King\" or \"Code Is King\" ?","","","An often overlooked secret of __computing__ is that it __is__ all and mostly about 2 things : __data__ and __algorithms__. \r\n\r\nThe resulting 2 kinds of programmers differ in their relationship with the code they produce, especially in how it handles data.\r\n\r\n!!!! Code is King\r\n\r\nSeveral key points : \r\n\r\n* The most important asset is the code. Special care is taken to put layer upon layer in order to be able for the code to survive any change outside itself. \r\n* Databases are mostly taken as a glorified data dump.\r\n* Data constraints (unicity & nullness) are enforced at the application level.\r\n* Coding speed is paramount.\r\n\r\nThe PHP/MySQL tandem is a typical example of this philosophy, I think this has precisely led to the common understanding that MySQL is a database that is very fast but not so careful or feature-full as others.\r\n\r\nThe various J2EE stacks is also a good example with the ubiquitous ORM solutions, such a Hibernate that tries very hard to reproduce the joining and data checks in the Java.\r\n\r\nThe current NoSQL meme turns also around this. Data is quite cheap, and can be \r\n\r\n!!!! Data is King\r\n\r\n* Developper-centric / DBA centric\r\n* fast ROI, long investment\r\n* \"Data is King\" makes you vendor independent, since code can always be rewritten. Data on the other hand, isn't that easily converted.\r\n\r\n!!!! AJAX is a modern version of the same divide\r\n\r\nMore and more we put more and more code inside the browser with the help of JavaScript. In the end, the application server tend to be not much more than a way to relay Ajax/JSON requests to the database, the extreme rationalization of this is [CouchDB|http://en.wikipedia.org/wiki/CouchDB|en].","<p>An often overlooked secret of <strong>computing</strong> is that it\n<strong>is</strong> all and mostly about 2 things : <strong>data</strong> and\n<strong>algorithms</strong>.</p>\n<p>The resulting 2 kinds of programmers differ in their relationship with the\ncode they produce, especially in how it handles data.</p>\n<h2>Code is King</h2>\n<p>Several key points :</p>\n<ul>\n<li>The most important asset is the code. Special care is taken to put layer\nupon layer in order to be able for the code to survive any change outside\nitself.</li>\n<li>Databases are mostly taken as a glorified data dump.</li>\n<li>Data constraints (unicity &amp; nullness) are enforced at the application\nlevel.</li>\n<li>Coding speed is paramount.</li>\n</ul>\n<p>The PHP/MySQL tandem is a typical example of this philosophy, I think this\nhas precisely led to the common understanding that MySQL is a database that is\nvery fast but not so careful or feature-full as others.</p>\n<p>The various J2EE stacks is also a good example with the ubiquitous ORM\nsolutions, such a Hibernate that tries very hard to reproduce the joining and\ndata checks in the Java.</p>\n<p>The current NoSQL meme turns also around this. Data is quite cheap, and can\nbe</p>\n<h2>Data is King</h2>\n<ul>\n<li>Developper-centric / DBA centric</li>\n<li>fast ROI, long investment</li>\n<li>&quot;Data is King&quot; makes you vendor independent, since code can always be\nrewritten. Data on the other hand, isn't that easily converted.</li>\n</ul>\n<h2>AJAX is a modern version of the same divide</h2>\n<p>More and more we put more and more code inside the browser with the help of\nJavaScript. In the end, the application server tend to be not much more than a\nway to relay Ajax/JSON requests to the database, the extreme rationalization of\nthis is <a href=\"http://en.wikipedia.org/wiki/CouchDB\" hreflang=\"en\">CouchDB</a>.</p>","DataIsKing : http://www.javapractices.com/topic/TopicAction.do?Id=211","data king code king often overlooked secret computing that all and mostly about things data and algorithms the resulting kinds programmers differ their relationship with the code they produce especially how handles data code king several key points the most important asset the code special care taken put layer upon layer order able for the code survive any change outside itself databases are mostly taken glorified data dump data constraints unicity amp nullness are enforced the application level coding speed paramount the php mysql tandem typical example this philosophy think this has precisely led the common understanding that mysql database that very fast but not careful feature full others the various j2ee stacks also good example with the ubiquitous orm solutions such hibernate that tries very hard reproduce the joining and data checks the java the current nosql meme turns also around this data quite cheap and can data king developper centric dba centric fast roi long investment quot data king quot makes you vendor independent since code can always rewritten data the other hand isn that easily converted ajax modern version the same divide more and more put more and more code inside the browser with the help javascript the end the application server tend not much more than way relay ajax json requests the database the extreme rationalization this couchdb","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","-2","0","1","0","0","0","0"
"631278","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2011-08-19 23:00:00","Europe/Paris","2011-08-19 11:57:04","2011-08-19 11:57:23","","post","wiki","2011/08/Installing-Soldat-Dedicated-Server-on-a-Debian-64-bits","en","Installing Soldat Dedicated Server on a Debian 64 bits","","","///\r\napt-get install upx ia32-libs\r\nln -s libz.so.1 /usr/lib32/libz.so\r\nupx -d soldatserver\r\n\r\nsh serverscript start\r\n///","<pre>\napt-get install upx ia32-libs\nln -s libz.so.1 /usr/lib32/libz.so\nupx -d soldatserver\n\nsh serverscript start\n</pre>","","installing soldat dedicated server debian bits apt get install upx ia32 libs libz usr lib32 libz upx soldatserver serverscript start","a:1:{s:3:\"tag\";a:3:{i:0;s:8:\"sysadmin\";i:1;s:6:\"soldat\";i:2;s:2:\"64\";}}","-2","0","1","0","0","0","0"
"401164","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-05-19 08:00:00","Europe/Paris","2009-05-11 05:47:09","2009-05-18 18:04:13","","post","wiki","2009/05/Surrogate-Keys-:-Globally-Unique-Application-Unique-or-Type-Unique","en","Surrogate Keys : Globally Unique, Application Unique or Type Unique ?","When you just decided to use Surrogate Keys, another problem arises : which value should I use ? It seems to be a very simple problem since the key now is completely in isolation in your application and is not related to any data. The choice is quite bonding, since a change means that every value has to be changed, and you did ++not++ let this key leak outside the database did you ?\r\n\r\nThe keypoint here is __isolation__. Many differents kinds of isolation are available, and soon you will be able to make a informed decision.","<p>When you just decided to use Surrogate Keys, another problem arises : which\nvalue should I use ? It seems to be a very simple problem since the key now is\ncompletely in isolation in your application and is not related to any data. The\nchoice is quite bonding, since a change means that every value has to be\nchanged, and you did <ins>not</ins> let this key leak outside the database did\nyou ?</p>\n<p>The keypoint here is <strong>isolation</strong>. Many differents kinds of\nisolation are available, and soon you will be able to make a informed\ndecision.</p>","!!!! Type unique : One sequence per table\r\n\r\nThe most common idiom out there. Just define a surrogate key per table, usually done via an @@auto_increment@@ field, @@serial@@ field or a @@sequence@@ per table. \r\n\r\nIt is the most natural way of thinking, since the ID is usually seen as a monotically incrementing counter. Therefore many applications use even the evil$$Evil because it's usually not thread safe and quite horridly inefficient$$ @@SELECT MAX(ID)+1@@ construct, because they specifically require that there should be no missed rows and that the ID have to be strictly in order.\r\n\r\nThe usual implementation of this is to have a kind of thread-local function that hands unique ID allocated in group and never trying to reuse an already issued/allocated ID. If you reserve 50 ID at once you directly level the cost of locking the ressource to only 1 in 50 requests, and not trying to reuse them enables you to just have to lock the end of the spectrum and not the whole spectrum. That's what the @@CACHE@@ keyword is about in [Oracle|http://www.techonthenet.com/oracle/sequences.php|en] and [PostgreSQL|http://www.postgresql.org/docs/8.3/static/sql-createsequence.html|en]. Others RDBMS usually also have a similar option\r\n\r\nBut this optimisation leaves us with a big side-effect : the ID that are generated are not strictly in order anymore, and furthermore there can be huge gaps in them. Since one of the main purpose of one ID per table is now gone, why not directly bite the bullet and use the same ID generation on the the whole application ?\r\n\r\n!!!! Application unique : One sequence to rule them all\r\n\r\nThis is something I first experienced with [Subversion|http://subversion.tigris.org/|en], coming from a CVS background : the revision number is repository-wide and not file-wide anymore. This has some mental drawbacks at first, since you still know that a file of version 5324 is more recent than version 3200, but not ''how much'' more since the commits could have been done in another place. The difference is more visible with 1.4 versus 1.5.\r\n\r\nBut as I was more used to it, that thinking was obviously a red herring. It did divert us that a file could have been completely rewritten in just one revision, and that one signle line could have be flipped back and forth in several revisions, \r\n\r\nSo the number, and the gap between them, as the basis of the amount of modifications has its value decreased. What could be quite interesting though is that the revision number is globally incrementing$$Actually in subversion it has a **strict** incrementing policy$$. That enables us to know that a file in revision 1000 was commited before another one with revision 2000.\r\n\r\nBack in our database, if we just define __one__ sequence for all the ID in the database we have 2 advantages : \r\n# One ID represents only one row. Debugging is easier and when you see an ID ''in the wild'', you can think of which row it represents.\r\n# You can establish an almost exact history of sequencing the creations of the rows.\r\n\r\nThe inevitable locking performance issue that arises often is made painless through bigger block allocations. \r\n\r\nOne of the lesser known issue is that, usually when using an ID per table, after a while you begin to ''know'' which ID belongs to wich table since all the tables are filled at different rates : 50083 is an order ID, whereas 13650239 looks more like a message ID.\r\n\r\n!!!! Globally unique : \r\n\r\nIf you push this logic a little bit further, you can even imagine that all the ID produced could be unique. Then you will have the benefits of unicity, but even on the whole IT level. It makes interapplication debugging easier, since the leaked keys can be nicely tracked down to their origin. How to generate such an unique ID is tricky question, since the applications usually are coded in heterogeneous environemments and technologies.\r\n\r\n!!! A central ID service\r\n\r\nThe most obvious way is to have a central ID service, the same as the unique sequence in the application. It has to be accessed in a distributed way, and the implementation can be quite cumbersome thoughout all the different technologies that exists. \r\n\r\nThis service has also to have the highest  [SLA|http://en.wikipedia.org/wiki/Service_level_agreement|en] as the applications that depends on it. It becomes then quickly a [SPOF|http://en.wikipedia.org/wiki/Single_point_of_failure|en] if not designed carefully.\r\n\r\n!!! GUID\r\n\r\nSince ''online'' ID generation has some serious drawbacks, the easiest ''offline'' way is to generate [GUID|http://en.wikipedia.org/wiki/Globally_Unique_Identifier|en]s, there are many hooks in each technology/application to be a quite educated move.\r\n\r\nIt also has drawbacks, but more in the semantic fields. A GUID is quite semantically opaque since looking at it does not tell you anything about where/when the ID was generated. You have to do a full key scan of all the applications to find its origin.\r\n\r\n!!! Almost globally unique : Application ID + Application Unique\r\n\r\nEarlier, we just found out the application-unique ID. If you pair it with an unique application ID, you just managed to have a globally unique ID.\r\nIn the generation of this ID, you just prefix it with the ID of the application, and tada.... you have your globally unique ID. \r\n\r\nThe net effect is that if you see this ID in the wild, you know where it was created. If you have an increasing one, you can even guess almost ''when''.\r\n\r\n!!!! Parallels between applications and networks\r\n\r\nThe more I think about it, the more I can draw parallels between rows in applications and adresses in networks, but I'll write more on that later in another article.","<h2>Type unique : One sequence per table</h2>\n<p>The most common idiom out there. Just define a surrogate key per table,\nusually done via an <code>auto_increment</code> field, <code>serial</code>\nfield or a <code>sequence</code> per table.</p>\n<p>It is the most natural way of thinking, since the ID is usually seen as a\nmonotically incrementing counter. Therefore many applications use even the\nevil<sup>[<a href=\"#pnote-401164-1\" id=\"rev-pnote-401164-1\" name=\"rev-pnote-401164-1\">1</a>]</sup> <code>SELECT MAX(ID)+1</code> construct,\nbecause they specifically require that there should be no missed rows and that\nthe ID have to be strictly in order.</p>\n<p>The usual implementation of this is to have a kind of thread-local function\nthat hands unique ID allocated in group and never trying to reuse an already\nissued/allocated ID. If you reserve 50 ID at once you directly level the cost\nof locking the ressource to only 1 in 50 requests, and not trying to reuse them\nenables you to just have to lock the end of the spectrum and not the whole\nspectrum. That's what the <code>CACHE</code> keyword is about in <a href=\"http://www.techonthenet.com/oracle/sequences.php\" hreflang=\"en\">Oracle</a> and\n<a href=\"http://www.postgresql.org/docs/8.3/static/sql-createsequence.html\" hreflang=\"en\">PostgreSQL</a>. Others RDBMS usually also have a similar\noption</p>\n<p>But this optimisation leaves us with a big side-effect : the ID that are\ngenerated are not strictly in order anymore, and furthermore there can be huge\ngaps in them. Since one of the main purpose of one ID per table is now gone,\nwhy not directly bite the bullet and use the same ID generation on the the\nwhole application ?</p>\n<h2>Application unique : One sequence to rule them all</h2>\n<p>This is something I first experienced with <a href=\"http://subversion.tigris.org/\" hreflang=\"en\">Subversion</a>, coming from a CVS\nbackground : the revision number is repository-wide and not file-wide anymore.\nThis has some mental drawbacks at first, since you still know that a file of\nversion 5324 is more recent than version 3200, but not <em>how much</em> more\nsince the commits could have been done in another place. The difference is more\nvisible with 1.4 versus 1.5.</p>\n<p>But as I was more used to it, that thinking was obviously a red herring. It\ndid divert us that a file could have been completely rewritten in just one\nrevision, and that one signle line could have be flipped back and forth in\nseveral revisions,</p>\n<p>So the number, and the gap between them, as the basis of the amount of\nmodifications has its value decreased. What could be quite interesting though\nis that the revision number is globally incrementing<sup>[<a href=\"#pnote-401164-2\" id=\"rev-pnote-401164-2\" name=\"rev-pnote-401164-2\">2</a>]</sup>. That enables us to know that a file in\nrevision 1000 was commited before another one with revision 2000.</p>\n<p>Back in our database, if we just define <strong>one</strong> sequence for\nall the ID in the database we have 2 advantages :</p>\n<ol>\n<li>One ID represents only one row. Debugging is easier and when you see an ID\n<em>in the wild</em>, you can think of which row it represents.</li>\n<li>You can establish an almost exact history of sequencing the creations of\nthe rows.</li>\n</ol>\n<p>The inevitable locking performance issue that arises often is made painless\nthrough bigger block allocations.</p>\n<p>One of the lesser known issue is that, usually when using an ID per table,\nafter a while you begin to <em>know</em> which ID belongs to wich table since\nall the tables are filled at different rates : 50083 is an order ID, whereas\n13650239 looks more like a message ID.</p>\n<h2>Globally unique :</h2>\n<p>If you push this logic a little bit further, you can even imagine that all\nthe ID produced could be unique. Then you will have the benefits of unicity,\nbut even on the whole IT level. It makes interapplication debugging easier,\nsince the leaked keys can be nicely tracked down to their origin. How to\ngenerate such an unique ID is tricky question, since the applications usually\nare coded in heterogeneous environemments and technologies.</p>\n<h3>A central ID service</h3>\n<p>The most obvious way is to have a central ID service, the same as the unique\nsequence in the application. It has to be accessed in a distributed way, and\nthe implementation can be quite cumbersome thoughout all the different\ntechnologies that exists.</p>\n<p>This service has also to have the highest <a href=\"http://en.wikipedia.org/wiki/Service_level_agreement\" hreflang=\"en\">SLA</a> as\nthe applications that depends on it. It becomes then quickly a <a href=\"http://en.wikipedia.org/wiki/Single_point_of_failure\" hreflang=\"en\">SPOF</a>\nif not designed carefully.</p>\n<h3>GUID</h3>\n<p>Since <em>online</em> ID generation has some serious drawbacks, the easiest\n<em>offline</em> way is to generate <a href=\"http://en.wikipedia.org/wiki/Globally_Unique_Identifier\" hreflang=\"en\">GUID</a>s, there are many hooks in each technology/application to be a\nquite educated move.</p>\n<p>It also has drawbacks, but more in the semantic fields. A GUID is quite\nsemantically opaque since looking at it does not tell you anything about\nwhere/when the ID was generated. You have to do a full key scan of all the\napplications to find its origin.</p>\n<h3>Almost globally unique : Application ID + Application Unique</h3>\n<p>Earlier, we just found out the application-unique ID. If you pair it with an\nunique application ID, you just managed to have a globally unique ID. In the\ngeneration of this ID, you just prefix it with the ID of the application, and\ntada.... you have your globally unique ID.</p>\n<p>The net effect is that if you see this ID in the wild, you know where it was\ncreated. If you have an increasing one, you can even guess almost\n<em>when</em>.</p>\n<h2>Parallels between applications and networks</h2>\n<p>The more I think about it, the more I can draw parallels between rows in\napplications and adresses in networks, but I'll write more on that later in\nanother article.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-401164-1\" id=\"pnote-401164-1\" name=\"pnote-401164-1\">1</a>] Evil because it's usually not thread safe and quite\nhorridly inefficient</p>\n<p>[<a href=\"#rev-pnote-401164-2\" id=\"pnote-401164-2\" name=\"pnote-401164-2\">2</a>] Actually in subversion it has a **strict** incrementing\npolicy</p>\n</div>","* Like RF1918 : unique IPs per company","surrogate keys globally unique application unique type unique when you just decided use surrogate keys another problem arises which value should use seems very simple problem since the key now completely isolation your application and not related any data the choice quite bonding since change means that every value has changed and you did not let this key leak outside the database did you the keypoint here isolation many differents kinds isolation are available and soon you will able make informed decision type unique one sequence per table the most common idiom out there just define surrogate key per table usually done via auto increment field serial field sequence per table the most natural way thinking since the usually seen monotically incrementing counter therefore many applications use even the evil select max construct because they specifically require that there should missed rows and that the have strictly order the usual implementation this have kind thread local function that hands unique allocated group and never trying reuse already issued allocated you reserve once you directly level the cost locking the ressource only requests and not trying reuse them enables you just have lock the end the spectrum and not the whole spectrum that what the cache keyword about oracle and postgresql others rdbms usually also have similar option but this optimisation leaves with big side effect the that are generated are not strictly order anymore and furthermore there can huge gaps them since one the main purpose one per table now gone why not directly bite the bullet and use the same generation the the whole application application unique one sequence rule them all this something first experienced with subversion coming from cvs background the revision number repository wide and not file wide anymore this has some mental drawbacks first since you still know that file version 5324 more recent than version 3200 but not how much more since the commits could have been done another place the difference more visible with versus but was more used that thinking was obviously red herring did divert that file could have been completely rewritten just one revision and that one signle line could have flipped back and forth several revisions the number and the gap between them the basis the amount modifications has its value decreased what could quite interesting though that the revision number globally incrementing that enables know that file revision 1000 was commited before another one with revision 2000 back our database just define one sequence for all the the database have advantages one represents only one row debugging easier and when you see the wild you can think which row represents you can establish almost exact history sequencing the creations the rows the inevitable locking performance issue that arises often made painless through bigger block allocations one the lesser known issue that usually when using per table after while you begin know which belongs wich table since all the tables are filled different rates 50083 order whereas 13650239 looks more like message globally unique you push this logic little bit further you can even imagine that all the produced could unique then you will have the benefits unicity but even the whole level makes interapplication debugging easier since the leaked keys can nicely tracked down their origin how generate such unique tricky question since the applications usually are coded heterogeneous environemments and technologies central service the most obvious way have central service the same the unique sequence the application has accessed distributed way and the implementation can quite cumbersome thoughout all the different technologies that exists this service has also have the highest sla the applications that depends becomes then quickly spof not designed carefully guid since online generation has some serious drawbacks the easiest offline way generate guids there are many hooks each technology application quite educated move also has drawbacks but more the semantic fields guid quite semantically opaque since looking does not tell you anything about where when the was generated you have full key scan all the applications find its origin almost globally unique application application unique earlier just found out the application unique you pair with unique application you just managed have globally unique the generation this you just prefix with the the application and tada you have your globally unique the net effect that you see this the wild you know where was created you have increasing one you can even guess almost when parallels between applications and networks the more think about the more can draw parallels between rows applications and adresses networks but write more that later another article notes evil because usually not thread safe and quite horridly inefficient actually subversion has strict incrementing policy","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","1","0","1","0","0","0","0"
"505602","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","","2010-04-02 17:22:00","Europe/Paris","2010-04-02 15:22:43","2010-04-02 15:44:25","","post","wiki","2010/04/Automatic-backup","en","Automatic backup","","","!!! Based on FUSE\r\n\r\n!! CopyFS\r\n\r\n! Original (1.0)\r\n\r\nhttp://boklm.eu/copyfs/\r\n\r\n! 1.3M (unofficial successor)\r\nhttp://mattwork.potsdam.edu/projects/wiki/index.php/Copyfs\r\n\r\n!! Wayback\r\nhttp://wayback.sourceforge.net/\r\n\r\n!! CowFS\r\nhttp://wiki.strongswan.org/projects/strongswan/repository/revisions/f8afabcac3858f1c43f5fad4ca9fdbc01614180b/entry/src/dumm/cowfs.c\r\n\r\n!!! Misc\r\n\r\n!! Virtual800 (A virtual filesystem with big files for testing)\r\nhttp://virtual800.sourceforge.net/\r\n\r\n!! Scord \r\nhttp://scord.sourceforge.net","<h3>Based on FUSE</h3>\n<h4>CopyFS</h4>\n<h5>Original (1.0)</h5>\n<p>http://boklm.eu/copyfs/</p>\n<h5>1.3M (unofficial successor)</h5>\n<p>http://mattwork.potsdam.edu/projects/wiki/index.php/Copyfs</p>\n<h4>Wayback</h4>\n<p>http://wayback.sourceforge.net/</p>\n<h4>CowFS</h4>\n<p>\nhttp://wiki.strongswan.org/projects/strongswan/repository/revisions/f8afabcac3858f1c43f5fad4ca9fdbc01614180b/entry/src/dumm/cowfs.c</p>\n<h3>Misc</h3>\n<h4>Virtual800 (A virtual filesystem with big files for testing)</h4>\n<p>http://virtual800.sourceforge.net/</p>\n<h4>Scord</h4>\n<p>http://scord.sourceforge.net</p>","","automatic backup based fuse copyfs original http boklm copyfs unofficial successor http mattwork potsdam edu projects wiki index php copyfs wayback http wayback sourceforge net cowfs http wiki strongswan org projects strongswan repository revisions f8afabcac3858f1c43f5fad4ca9fdbc01614180b entry src dumm cowfs misc virtual800 virtual filesystem with big files for testing http virtual800 sourceforge net scord http scord sourceforge net","a:0:{}","-2","0","1","0","0","0","0"
"117264","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2007-05-30 21:47:00","Europe/Paris","2007-05-30 19:47:49","2009-05-18 10:10:51","","post","wiki","2007/05/30/Should-SQL-die-and-can-it","en","Should SQL die, and can it ?","These days, writing SQL code seems like writing some C/C++ code : it's so old fashioned.","<p>These days, writing SQL code seems like writing some C/C++ code : it's so\nold fashioned.</p>","I just draw a comparison of the data language and the general languages :\r\n* Creating our data structures by hand feels like ASM and its multiple wheels reinvented everytime.\r\n* Vanilla SQL feels like C and its bare instruction set.\r\n* Advanced SQL (Stored Proc et al) feels just like C++ and all those ''almost'' compatible implementations.\r\n* A statically compiled ORM (think [Hibernate|http://www.hibernate.org/]) feels like Java and it's strong typing system with its truckload of runtime casts.\r\n* A dynamic ORM (think [ActiveRecord|http://en.wikipedia.org/wiki/Active_record_pattern]) feels like a dynamic scripting language and its \"wow\" factor at the beginning, rapidly followed by the \"is-this-the-fastest-cpu-available\" factor.\r\n\r\nI have some grudges against the ORM i'm using now : \r\n* It's not really easy to reverse-engineer a database. Everyone seems really happy to start with a fresh new one. \r\n* Joins are optimised for only the simplest joins. (If it's not just a plain @@foreach@@-like application loop)\r\n* If you have some complex things to do, the usual \"you-always-can-write-a-custom-mapping\" applies.\r\n\r\nNot writing any SQL can be a nice goal, but i don't think that writing SQL is specially that hard. It's a quite good language for what it's designed for actually.\r\n\r\nI think that today's ORM solutions are a little half-baked. It puts too much hassle on the user for the result it provides. He has to describe the whole database structure in order to manage to generate the differents objects that impersonate the undelaying tables.\r\n\r\nWhy can't this tedious process happen in runtime ? Ok, maybe not really in runtime, but either on compile time or on launch time. Two directions could be explored : \r\n* The ORM could automatically adapt itself on the undelaying database schema by introspecting the schema (many DBA tools did that for years). \r\n* The ORM could also adapt the database schema to runtime changes by doing some runtime @@ALTER TABLE@@.\r\n\r\nIn contrast, writing SQL yourselfs is often the easiest and fastest solution, since you directly and only ask what you want. It also allow you not to think too much about concurrency since that's usually what good RDBMS do the best. With an ORM, either you explain a lot your intentions, and then you often do the SQL optimiser job, or you hope that your ORM is clever enough to guess what you want to do. Usually it's not that good at it, and tries to hide it behind some agressive caching mecanism.\r\n\r\nSo I think SQL is here to stay a little while, just like C/C++ is. The ORM have no really catched up since. (We're like in the old days of non-JIT Java).\r\n\r\nBtw, I even dreamed of an ORM in bijection with XML or [YAML|http://en.wikipedia.org/wiki/YAML|en] with a XPath query language, but I will write later on that.","<p>I just draw a comparison of the data language and the general languages\n:</p>\n<ul>\n<li>Creating our data structures by hand feels like ASM and its multiple wheels\nreinvented everytime.</li>\n<li>Vanilla SQL feels like C and its bare instruction set.</li>\n<li>Advanced SQL (Stored Proc et al) feels just like C++ and all those\n<em>almost</em> compatible implementations.</li>\n<li>A statically compiled ORM (think <a href=\"http://www.hibernate.org/\">Hibernate</a>) feels like Java and it's strong\ntyping system with its truckload of runtime casts.</li>\n<li>A dynamic ORM (think <a href=\"http://en.wikipedia.org/wiki/Active_record_pattern\">ActiveRecord</a>) feels\nlike a dynamic scripting language and its &quot;wow&quot; factor at the beginning,\nrapidly followed by the &quot;is-this-the-fastest-cpu-available&quot; factor.</li>\n</ul>\n<p>I have some grudges against the ORM i'm using now :</p>\n<ul>\n<li>It's not really easy to reverse-engineer a database. Everyone seems really\nhappy to start with a fresh new one.</li>\n<li>Joins are optimised for only the simplest joins. (If it's not just a plain\n<code>foreach</code>-like application loop)</li>\n<li>If you have some complex things to do, the usual\n&quot;you-always-can-write-a-custom-mapping&quot; applies.</li>\n</ul>\n<p>Not writing any SQL can be a nice goal, but i don't think that writing SQL\nis specially that hard. It's a quite good language for what it's designed for\nactually.</p>\n<p>I think that today's ORM solutions are a little half-baked. It puts too much\nhassle on the user for the result it provides. He has to describe the whole\ndatabase structure in order to manage to generate the differents objects that\nimpersonate the undelaying tables.</p>\n<p>Why can't this tedious process happen in runtime ? Ok, maybe not really in\nruntime, but either on compile time or on launch time. Two directions could be\nexplored :</p>\n<ul>\n<li>The ORM could automatically adapt itself on the undelaying database schema\nby introspecting the schema (many DBA tools did that for years).</li>\n<li>The ORM could also adapt the database schema to runtime changes by doing\nsome runtime <code>ALTER TABLE</code>.</li>\n</ul>\n<p>In contrast, writing SQL yourselfs is often the easiest and fastest\nsolution, since you directly and only ask what you want. It also allow you not\nto think too much about concurrency since that's usually what good RDBMS do the\nbest. With an ORM, either you explain a lot your intentions, and then you often\ndo the SQL optimiser job, or you hope that your ORM is clever enough to guess\nwhat you want to do. Usually it's not that good at it, and tries to hide it\nbehind some agressive caching mecanism.</p>\n<p>So I think SQL is here to stay a little while, just like C/C++ is. The ORM\nhave no really catched up since. (We're like in the old days of non-JIT\nJava).</p>\n<p>Btw, I even dreamed of an ORM in bijection with XML or <a href=\"http://en.wikipedia.org/wiki/YAML\" hreflang=\"en\">YAML</a> with a XPath query\nlanguage, but I will write later on that.</p>","","should sql die and can these days writing sql code seems like writing some code old fashioned just draw comparison the data language and the general languages creating our data structures hand feels like asm and its multiple wheels reinvented everytime vanilla sql feels like and its bare instruction set advanced sql stored proc feels just like and all those almost compatible implementations statically compiled orm think hibernate feels like java and strong typing system with its truckload runtime casts dynamic orm think activerecord feels like dynamic scripting language and its quot wow quot factor the beginning rapidly followed the quot this the fastest cpu available quot factor have some grudges against the orm using now not really easy reverse engineer database everyone seems really happy start with fresh new one joins are optimised for only the simplest joins not just plain foreach like application loop you have some complex things the usual quot you always can write custom mapping quot applies not writing any sql can nice goal but don think that writing sql specially that hard quite good language for what designed for actually think that today orm solutions are little half baked puts too much hassle the user for the result provides has describe the whole database structure order manage generate the differents objects that impersonate the undelaying tables why can this tedious process happen runtime maybe not really runtime but either compile time launch time two directions could explored the orm could automatically adapt itself the undelaying database schema introspecting the schema many dba tools did that for years the orm could also adapt the database schema runtime changes doing some runtime alter table contrast writing sql yourselfs often the easiest and fastest solution since you directly and only ask what you want also allow you not think too much about concurrency since that usually what good rdbms the best with orm either you explain lot your intentions and then you often the sql optimiser job you hope that your orm clever enough guess what you want usually not that good and tries hide behind some agressive caching mecanism think sql here stay little while just like the orm have really catched since like the old days non jit java btw even dreamed orm bijection with xml yaml with xpath query language but will write later that","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"database\";i:1;s:3:\"sql\";}}","1","0","1","0","0","0","0"
"89457","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2007-03-19 22:20:00","Europe/Paris","2007-03-18 19:20:45","2009-04-17 15:07:53","","post","wiki","2007/03/19/Inheritance-of-a-public-method-considered-harmful","en","Inheritance of a public method considered harmful","","","I'm always wondering why all textbooks and reference material on the web always describe inheritance with the same kind of example :\r\n\r\n class A {\r\n 	public execute() {\r\n 		// do something...\r\n 	}\r\n }\r\n\r\n class B extends A {\r\n 	public execute() {\r\n 		// initialize for B\r\n 		super.execute();\r\n 	}\r\n }\r\n\r\n class C extends A {\r\n 	public execute() {\r\n 		// initialize for B\r\n 		super.execute();\r\n 	}\r\n }\r\n\r\nOne thing struck me : if I want to have some code executed __before__ @@execute()@@ in every class, such as preparing the graphic context for example, I have to write a protected method @@initializeContext()@@\r\non the base class and call it from every derived @@execute()@@ method. This is a good candidate for errors since it's manual and therefore not automatic.\r\n\r\nI prefer to go for an [IoC|http://en.wikipedia.org/wiki/Inversion_of_control|en]-like inheritance scheme, we could call it \"Inversion of Inheritance\". The code would be something like :  \r\n\r\nclass A {\r\n 	protected executePre() {\r\n 		// Default implementation \r\n 		// does nothing\r\n 	}\r\n\r\n 	protected executePost() {\r\n 		// Default implementation \r\n 		// does nothing\r\n 	}\r\n\r\n 	protected executeInternal()	{\r\n 		// do something\r\n 	}\r\n 	\r\n 	public final execute() {\r\n 		executePre();\r\n 		executeInternal();\r\n 		executePost();\r\n 	}\r\n }\r\n\r\n class B extends A {\r\n 	protected executeInternal() {\r\n 		// initialize for B\r\n 	}\r\n }\r\n\r\n class C extends A {\r\n 	protected executeInternal() {\r\n 		// initialize for C\r\n 	}\r\n }\r\n\r\nThere are 2 problems with this approach : \r\n# There is a little performance penalty since every calls issues 3 virtual calls.\r\n# If you want to add another interception function, you have to edit the base class.\r\n\r\nBut the main purpose is that __you__ are in control of the derived classes of your class since the creativity of the reuser is severly hampered. I think of it as the most easy way to enforce the [Liskov substitution principle|http://en.wikipedia.org/wiki/Liskov_substitution_principle|en].","<p>I'm always wondering why all textbooks and reference material on the web\nalways describe inheritance with the same kind of example :</p>\n<pre>\nclass A {\n        public execute() {\n                // do something...\n        }\n}\n</pre>\n<pre>\nclass B extends A {\n        public execute() {\n                // initialize for B\n                super.execute();\n        }\n}\n</pre>\n<pre>\nclass C extends A {\n        public execute() {\n                // initialize for B\n                super.execute();\n        }\n}\n</pre>\n<p>One thing struck me : if I want to have some code executed\n<strong>before</strong> <code>execute()</code> in every class, such as\npreparing the graphic context for example, I have to write a protected method\n<code>initializeContext()</code> on the base class and call it from every\nderived <code>execute()</code> method. This is a good candidate for errors\nsince it's manual and therefore not automatic.</p>\n<p>I prefer to go for an <a href=\"http://en.wikipedia.org/wiki/Inversion_of_control\" hreflang=\"en\">IoC</a>-like\ninheritance scheme, we could call it &quot;Inversion of Inheritance&quot;. The code would\nbe something like :</p>\n<p>class A {</p>\n<pre>\n   protected executePre() {\n                // Default implementation \n                // does nothing\n        }\n</pre>\n<pre>\n   protected executePost() {\n                // Default implementation \n                // does nothing\n        }\n</pre>\n<pre>\n   protected executeInternal()     {\n                // do something\n        }\n        \n        public final execute() {\n                executePre();\n                executeInternal();\n                executePost();\n        }\n}\n</pre>\n<pre>\nclass B extends A {\n        protected executeInternal() {\n                // initialize for B\n        }\n}\n</pre>\n<pre>\nclass C extends A {\n        protected executeInternal() {\n                // initialize for C\n        }\n}\n</pre>\n<p>There are 2 problems with this approach :</p>\n<ol>\n<li>There is a little performance penalty since every calls issues 3 virtual\ncalls.</li>\n<li>If you want to add another interception function, you have to edit the base\nclass.</li>\n</ol>\n<p>But the main purpose is that <strong>you</strong> are in control of the\nderived classes of your class since the creativity of the reuser is severly\nhampered. I think of it as the most easy way to enforce the <a href=\"http://en.wikipedia.org/wiki/Liskov_substitution_principle\" hreflang=\"en\">Liskov substitution principle</a>.</p>","","inheritance public method considered harmful always wondering why all textbooks and reference material the web always describe inheritance with the same kind example class public execute something class extends public execute initialize for super execute class extends public execute initialize for super execute one thing struck want have some code executed before execute every class such preparing the graphic context for example have write protected method initializecontext the base class and call from every derived execute method this good candidate for errors since manual and therefore not automatic prefer for ioc like inheritance scheme could call quot inversion inheritance quot the code would something like class protected executepre default implementation does nothing protected executepost default implementation does nothing protected executeinternal something public final execute executepre executeinternal executepost class extends protected executeinternal initialize for class extends protected executeinternal initialize for there are problems with this approach there little performance penalty since every calls issues virtual calls you want add another interception function you have edit the base class but the main purpose that you are control the derived classes your class since the creativity the reuser severly hampered think the most easy way enforce the liskov substitution principle","a:0:{}","1","1","1","0","0","0","0"
"391283","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2009-04-07 21:17:00","Europe/Paris","2009-04-07 07:23:01","2010-08-23 16:45:30","","post","wiki","2009/04/Should-The-URL-Include-a-Date-or-Not","en","Should The URL Include a Date, or Not ?","A common URL pattern for a blog entry URL is __Year/Month/DayOfMonth/Post_Name__. This URL is generated at the blog entry creation time. It will never$$Although it can be manually changed, it is usually discouraged$$ change anymore afterwards.","<p>A common URL pattern for a blog entry URL is\n<strong>Year/Month/DayOfMonth/Post_Name</strong>. This URL is generated at the\nblog entry creation time. It will never<sup>[<a href=\"#pnote-391283-1\" id=\"rev-pnote-391283-1\" name=\"rev-pnote-391283-1\">1</a>]</sup> change anymore\nafterwards.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-391283-1\" id=\"pnote-391283-1\" name=\"pnote-391283-1\">1</a>] Although it can be manually changed, it is usually\ndiscouraged</p>\n</div>","A common issue is that a blog entry takes time to be written, refined and then published. Therefore the date contained in the URL isn't the publication time. But since the publication time is the time the entry is created in the point of view of the public the date in the URL conveys only little meaning, just the time is took to create it.\r\n\r\nAnother point is that some blogger, i'm no exception, just create an entry to make a note. Theses entries are composed of bare notes without any relation or phrases. It serves as a unique receptacle of related raw ideas, useful not to loose them or to be obliged to scan through many medium just to make sure we didn't forget something that we thought important earlier. Then the date in the URL conveys very little meaning, mostly when we first had the idea about it.\r\n\r\nOn the opposite side, a date in the URL is quite interesting to know the relative age of a post without having to look in detail inside. This information is especially important in a technical field such as IT where the world is turning around quite fast. \r\n\r\nWe can see now that the __timing__ information is important, just that the standard precision of __one day__ is a bit overkill. All depends on the time taken to write the blog entry, and on the time scope it we be relevant. \r\n\r\nFor a article about a precise event, a precision of one day is interesting. At this end of the scale, blog entries that relates a event throughout a day would even have a hour precision. If most of the articles had a shift of 1 to 2 days in the creation time and the publication time, a precision of a day would be a fair deal. And the other side of the spectrum, an article about a slowing evolving subject such as demographic studies are fairly represented by a yearly url. For timeless subject such as philosophy, I still think that yearly would be adequate since one year represent quite a long time on a human scale.\r\n\r\nSince usually my blog entries are in gestation for about 1 to 3 weeks, and are about IT subjects, a  monthly precision is the most interesting since that conveys an adequate timing information in a changing world without having a false impression of exactitude.\r\n\r\nTherefore I'm changing my URL format to a monthly rule, meanwhile allowing myself a different precision for special entries on specific subjects.\r\n\r\nUnfortunatly it is very discouraged to change the URL once it's published since otherwise you have to redirect all the links$$from your other articles, from other blogs or from search engines$$ that are currently pointing on it.","<p>A common issue is that a blog entry takes time to be written, refined and\nthen published. Therefore the date contained in the URL isn't the publication\ntime. But since the publication time is the time the entry is created in the\npoint of view of the public the date in the URL conveys only little meaning,\njust the time is took to create it.</p>\n<p>Another point is that some blogger, i'm no exception, just create an entry\nto make a note. Theses entries are composed of bare notes without any relation\nor phrases. It serves as a unique receptacle of related raw ideas, useful not\nto loose them or to be obliged to scan through many medium just to make sure we\ndidn't forget something that we thought important earlier. Then the date in the\nURL conveys very little meaning, mostly when we first had the idea about\nit.</p>\n<p>On the opposite side, a date in the URL is quite interesting to know the\nrelative age of a post without having to look in detail inside. This\ninformation is especially important in a technical field such as IT where the\nworld is turning around quite fast.</p>\n<p>We can see now that the <strong>timing</strong> information is important,\njust that the standard precision of <strong>one day</strong> is a bit overkill.\nAll depends on the time taken to write the blog entry, and on the time scope it\nwe be relevant.</p>\n<p>For a article about a precise event, a precision of one day is interesting.\nAt this end of the scale, blog entries that relates a event throughout a day\nwould even have a hour precision. If most of the articles had a shift of 1 to 2\ndays in the creation time and the publication time, a precision of a day would\nbe a fair deal. And the other side of the spectrum, an article about a slowing\nevolving subject such as demographic studies are fairly represented by a yearly\nurl. For timeless subject such as philosophy, I still think that yearly would\nbe adequate since one year represent quite a long time on a human scale.</p>\n<p>Since usually my blog entries are in gestation for about 1 to 3 weeks, and\nare about IT subjects, a monthly precision is the most interesting since that\nconveys an adequate timing information in a changing world without having a\nfalse impression of exactitude.</p>\n<p>Therefore I'm changing my URL format to a monthly rule, meanwhile allowing\nmyself a different precision for special entries on specific subjects.</p>\n<p>Unfortunatly it is very discouraged to change the URL once it's published\nsince otherwise you have to redirect all the links<sup>[<a href=\"#pnote-391283-1\" id=\"rev-pnote-391283-1\" name=\"rev-pnote-391283-1\">1</a>]</sup> that are currently pointing on it.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-391283-1\" id=\"pnote-391283-1\" name=\"pnote-391283-1\">1</a>] from your other articles, from other blogs or from\nsearch engines</p>\n</div>","","should the url include date not common url pattern for blog entry url year month dayofmonth post name this url generated the blog entry creation time will never change anymore afterwards notes although can manually changed usually discouraged common issue that blog entry takes time written refined and then published therefore the date contained the url isn the publication time but since the publication time the time the entry created the point view the public the date the url conveys only little meaning just the time took create another point that some blogger exception just create entry make note theses entries are composed bare notes without any relation phrases serves unique receptacle related raw ideas useful not loose them obliged scan through many medium just make sure didn forget something that thought important earlier then the date the url conveys very little meaning mostly when first had the idea about the opposite side date the url quite interesting know the relative age post without having look detail inside this information especially important technical field such where the world turning around quite fast can see now that the timing information important just that the standard precision one day bit overkill all depends the time taken write the blog entry and the time scope relevant for article about precise event precision one day interesting this end the scale blog entries that relates event throughout day would even have hour precision most the articles had shift days the creation time and the publication time precision day would fair deal and the other side the spectrum article about slowing evolving subject such demographic studies are fairly represented yearly url for timeless subject such philosophy still think that yearly would adequate since one year represent quite long time human scale since usually blog entries are gestation for about weeks and are about subjects monthly precision the most interesting since that conveys adequate timing information changing world without having false impression exactitude therefore changing url format monthly rule meanwhile allowing myself different precision for special entries specific subjects unfortunatly very discouraged change the url once published since otherwise you have redirect all the links that are currently pointing notes from your other articles from other blogs from search engines","a:0:{}","1","0","1","0","2","0","0"
"263848","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2008-07-27 11:26:00","Europe/Paris","2008-07-27 09:26:24","2010-03-31 16:33:20","","post","wiki","2008/07/27/RAII-in-Java-to-clean-your-code","en","RAII in Java to clean your code","","","[RAII|http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization|en] is a very common idiom in C++ and some other languages that don't have an integrated garbage collection management. \r\n\r\nJava has GC, therefore this idiom is not as popular. But the main problem of Java is that although the GC system has become quite efficient, it only handles the memory management. For other resources (database connections, sockets or file descriptors for exemple), this system is not really adequate. The release of these resources has always to be explicit, and handling this via the @@[finalize()|http://java.sun.com/javase/6/docs/api/java/lang/Object.html#finalize()|en]@@ method is [not satisfactory|http://www.javaworld.com/javaworld/jw-06-1998/jw-06-techniques.html|en].\r\n\r\nIn short the finalize execute itself when the object is about to be ''garbaged''. The main problem is that this garbaging does only take into account the memory limits, not the resources limits (max number of open file descriptors for example). So you can run out of open file descriptors way before running out of free memory.\r\n\r\nSo, the usual construction is like this :\r\n///\r\nMyResource res = null;\r\ntry {\r\n  res = new MyResource();\r\n  res.setSomething(someValue);\r\n  /* Use the resource */\r\n  res.close();\r\n} catch (Exception e) {\r\n  // release the resource if needed\r\n  if (res != null) { res.close(); }\r\n}\r\n///\r\n\r\nBut hey, that's many code lines, and in case of a Throwable, you don't release the resource. The concept of releasing the resources with a @@[try { } finally { }|http://java.sun.com/developer/TechTips/2000/tt0124.html#tip1|en]@@ construct is much better (actually, it's one of the most common usage of @@finally@@).\r\n\r\nThe construction becomes :\r\n\r\n///\r\nMyResource res = null;\r\ntry {\r\n  res = new MyResource();\r\n  res.setSomething(someValue);\r\n  /* Use the resource */\r\n} finally {\r\n  if (res != null) { res.close(); }\r\n}\r\n///\r\n\r\nBut here we can see that Java is not quite different from C\++ for that matter, so we can just adapt the C\++-ism that is [RAII|http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization|en], and write a much cleaner version that aquire the ressource in the constructor, so most failure conditions can be checked immediatly. \r\n\r\nThe construction becomes finally :\r\n\r\n///\r\nMyResource res = new MyResource(someValue);\r\ntry {\r\n  /* Use the resource */\r\n} finally {\r\n  res.close();\r\n}\r\n///\r\n\r\nSince a constructor never returns a null value, there is no need to test. And if the constructor throws an exceptions, the general contract is that [the object does not exists|http://herbsutter.wordpress.com/2008/07/25/constructor-exceptions-in-c-c-and-java/|en]. Therefore no resource has been allocated since it would be impossible for the caller to release it (remember, no object was created). So there is no need to release it.\r\n\r\nThe setter is also integrated in the constructor, since the whole RAII concept is that the constructor returns a completely initialized object. It also enables to write cleaner code since when calling the close() there is no need to do some @@if()@@ to know the object initialisation-state.","<p><a href=\"http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization\" hreflang=\"en\">RAII</a> is a very common idiom in C++ and some other languages that don't\nhave an integrated garbage collection management.</p>\n<p>Java has GC, therefore this idiom is not as popular. But the main problem of\nJava is that although the GC system has become quite efficient, it only handles\nthe memory management. For other resources (database connections, sockets or\nfile descriptors for exemple), this system is not really adequate. The release\nof these resources has always to be explicit, and handling this via the\n<code><a href=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html#finalize()\" hreflang=\"en\">finalize()</a></code> method is <a href=\"http://www.javaworld.com/javaworld/jw-06-1998/jw-06-techniques.html\" hreflang=\"en\">not satisfactory</a>.</p>\n<p>In short the finalize execute itself when the object is about to be\n<em>garbaged</em>. The main problem is that this garbaging does only take into\naccount the memory limits, not the resources limits (max number of open file\ndescriptors for example). So you can run out of open file descriptors way\nbefore running out of free memory.</p>\n<p>So, the usual construction is like this :</p>\n<pre>\nMyResource res = null;\ntry {\n  res = new MyResource();\n  res.setSomething(someValue);\n  /* Use the resource */\n  res.close();\n} catch (Exception e) {\n  // release the resource if needed\n  if (res != null) { res.close(); }\n}\n</pre>\n<p>But hey, that's many code lines, and in case of a Throwable, you don't\nrelease the resource. The concept of releasing the resources with a\n<code><a href=\"http://java.sun.com/developer/TechTips/2000/tt0124.html#tip1\" hreflang=\"en\">try { } finally { }</a></code> construct is much better\n(actually, it's one of the most common usage of <code>finally</code>).</p>\n<p>The construction becomes :</p>\n<pre>\nMyResource res = null;\ntry {\n  res = new MyResource();\n  res.setSomething(someValue);\n  /* Use the resource */\n} finally {\n  if (res != null) { res.close(); }\n}\n</pre>\n<p>But here we can see that Java is not quite different from C++ for that\nmatter, so we can just adapt the C++-ism that is <a href=\"http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization\" hreflang=\"en\">RAII</a>, and write a much cleaner version that aquire the ressource in\nthe constructor, so most failure conditions can be checked immediatly.</p>\n<p>The construction becomes finally :</p>\n<pre>\nMyResource res = new MyResource(someValue);\ntry {\n  /* Use the resource */\n} finally {\n  res.close();\n}\n</pre>\n<p>Since a constructor never returns a null value, there is no need to test.\nAnd if the constructor throws an exceptions, the general contract is that\n<a href=\"http://herbsutter.wordpress.com/2008/07/25/constructor-exceptions-in-c-c-and-java/\" hreflang=\"en\">the object does not exists</a>. Therefore no resource has been\nallocated since it would be impossible for the caller to release it (remember,\nno object was created). So there is no need to release it.</p>\n<p>The setter is also integrated in the constructor, since the whole RAII\nconcept is that the constructor returns a completely initialized object. It\nalso enables to write cleaner code since when calling the close() there is no\nneed to do some <code>if()</code> to know the object initialisation-state.</p>","","raii java clean your code raii very common idiom and some other languages that don have integrated garbage collection management java has therefore this idiom not popular but the main problem java that although the system has become quite efficient only handles the memory management for other resources database connections sockets file descriptors for exemple this system not really adequate the release these resources has always explicit and handling this via the finalize method not satisfactory short the finalize execute itself when the object about garbaged the main problem that this garbaging does only take into account the memory limits not the resources limits max number open file descriptors for example you can run out open file descriptors way before running out free memory the usual construction like this myresource res null try res new myresource res setsomething somevalue use the resource res close catch exception release the resource needed res null res close but hey that many code lines and case throwable you don release the resource the concept releasing the resources with try finally construct much better actually one the most common usage finally the construction becomes myresource res null try res new myresource res setsomething somevalue use the resource finally res null res close but here can see that java not quite different from for that matter can just adapt the ism that raii and write much cleaner version that aquire the ressource the constructor most failure conditions can checked immediatly the construction becomes finally myresource res new myresource somevalue try use the resource finally res close since constructor never returns null value there need test and the constructor throws exceptions the general contract that the object does not exists therefore resource has been allocated since would impossible for the caller release remember object was created there need release the setter also integrated the constructor since the whole raii concept that the constructor returns completely initialized object also enables write cleaner code since when calling the close there need some know the object initialisation state","a:0:{}","1","1","1","0","0","0","0"
"159135","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2007-09-28 07:57:00","Europe/Paris","2007-09-28 06:28:25","2007-09-28 06:49:11","","post","wiki","2007/09/28/Clean-Up-Your-Code-and-Boost-Your-Google-Ranking-at-the-Same-Time-Post-process-Your-Links","en","Clean-Up Your Code and Boost Your Google Ranking at the Same Time : Post-process Your Links","","","The search engine ranking of a web site is very important nowadays. As usually every optimisation means an complexification of the code, many developpers just pay that price. It's mostly paid with a ''javascriptification'' of the links that we don't want to be followed, and ''javascriptification'' of functional dialog popups.\r\n\r\nThere's an other approch to this. It still makes heavy use of Javascript, but : \r\n* your site will still work as intended even with javascript disabled\r\n* your site will be much more maintenable\r\n\r\nThe idea is to post-process the links with a Javascript function that is triggered just before the end of the loading of the page. Usually it can be done with a body-onLoad, or a <script/> element at the bottom of the page.\r\nThis script parses all the <a/> elements, and decodes the classes added to it. If you encode the classenames like this code :\r\n\r\n///\r\n<a \r\nhref=\"my_popup.html\" \r\nclass=\"medium_popup pp/popup\">\r\n	explain these words\r\n</a>\r\n///\r\n\r\nWe can see that the classes of the link are @@medium_popup@@, that is handled normally by the browser, and @@pp/popup@@ is used by the post-processing code to add a onClick() handler that could be expressed as @@onClick=\"javascript:open_popup('my_popup.html');return false;\"@@.\r\n\r\nThe big advantage is that you can very simply add popup to your links, web-spiders knows how to go to your popup with all your shiny glossary, and so can users without javascript or just open in a new tab with a middle-click on some browsers.\r\n\r\nIf it's a dialog box to ask some more informations, you can encode the link like that : \r\n\r\n///\r\n<a \r\nhref=\"\" \r\nclass=\"medium_popup pp/link/my_dialog.html\">\r\n	choose the date\r\n</a>\r\n///\r\n\r\nThis way @@pp/link/my_dialog.html@@ will be parsed and the @@href@@ will be dynamically replaced with @@my_dialog.html@@.\r\n\r\nYou can even later add a @@pp/popup@@ after the @@pp/link/my_dialog.html@@, and have the dialob box magically opened in a popup.\r\n\r\nI'll post later some example code to parse it.","<p>The search engine ranking of a web site is very important nowadays. As\nusually every optimisation means an complexification of the code, many\ndeveloppers just pay that price. It's mostly paid with a\n<em>javascriptification</em> of the links that we don't want to be followed,\nand <em>javascriptification</em> of functional dialog popups.</p>\n<p>There's an other approch to this. It still makes heavy use of Javascript,\nbut :</p>\n<ul>\n<li>your site will still work as intended even with javascript disabled</li>\n<li>your site will be much more maintenable</li>\n</ul>\n<p>The idea is to post-process the links with a Javascript function that is\ntriggered just before the end of the loading of the page. Usually it can be\ndone with a body-onLoad, or a &lt;script/&gt; element at the bottom of the\npage. This script parses all the &lt;a/&gt; elements, and decodes the classes\nadded to it. If you encode the classenames like this code :</p>\n<pre>\n&lt;a \nhref=&quot;my_popup.html&quot; \nclass=&quot;medium_popup pp/popup&quot;&gt;\n        explain these words\n&lt;/a&gt;\n</pre>\n<p>We can see that the classes of the link are <code>medium_popup</code>, that\nis handled normally by the browser, and <code>pp/popup</code> is used by the\npost-processing code to add a onClick() handler that could be expressed as\n<code>onClick=&quot;javascript:open_popup('my_popup.html');return\nfalse;&quot;</code>.</p>\n<p>The big advantage is that you can very simply add popup to your links,\nweb-spiders knows how to go to your popup with all your shiny glossary, and so\ncan users without javascript or just open in a new tab with a middle-click on\nsome browsers.</p>\n<p>If it's a dialog box to ask some more informations, you can encode the link\nlike that :</p>\n<pre>\n&lt;a \nhref=&quot;&quot; \nclass=&quot;medium_popup pp/link/my_dialog.html&quot;&gt;\n        choose the date\n&lt;/a&gt;\n</pre>\n<p>This way <code>pp/link/my_dialog.html</code> will be parsed and the\n<code>href</code> will be dynamically replaced with\n<code>my_dialog.html</code>.</p>\n<p>You can even later add a <code>pp/popup</code> after the\n<code>pp/link/my_dialog.html</code>, and have the dialob box magically opened\nin a popup.</p>\n<p>I'll post later some example code to parse it.</p>","","clean your code and boost your google ranking the same time post process your links the search engine ranking web site very important nowadays usually every optimisation means complexification the code many developpers just pay that price mostly paid with javascriptification the links that don want followed and javascriptification functional dialog popups there other approch this still makes heavy use javascript but your site will still work intended even with javascript disabled your site will much more maintenable the idea post process the links with javascript function that triggered just before the end the loading the page usually can done with body onload script element the bottom the page this script parses all the elements and decodes the classes added you encode the classenames like this code href quot popup html quot class quot medium popup popup quot explain these words can see that the classes the link are medium popup that handled normally the browser and popup used the post processing code add onclick handler that could expressed onclick quot javascript open popup popup html return false quot the big advantage that you can very simply add popup your links web spiders knows how your popup with all your shiny glossary and can users without javascript just open new tab with middle click some browsers dialog box ask some more informations you can encode the link like that href quot quot class quot medium popup link dialog html quot choose the date this way link dialog html will parsed and the href will dynamically replaced with dialog html you can even later add popup after the link dialog html and have the dialob box magically opened popup post later some example code parse","a:0:{}","1","0","1","1","0","0","0"
"182417","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2007-12-03 23:42:00","Europe/Paris","2007-12-03 20:46:00","2010-03-31 16:49:37","","post","wiki","2007/12/03/Use-immutable-objects-to-avoid-synchronisation","en","Use Immutable Objects to Avoid Synchronisation","","","With the future and its multiple core environnements as stated in a previous post about [workflows|/post/2007/09/10/Are-Workflows-the-Future-of-IT-Computing|en], efficient locking will be more and more an issue.\r\n\r\nMy previous way to cut this gordian knot was to : \r\n# multiply the objects that could be locked to reduce contention : have many multiple elementary objects . These can be workcases in the workflow theory.\r\n# ''cheat'' to minimize the time spend on locking : use something called [software transactional memory|http://en.wikipedia.org/wiki/Software_transactional_memory|en] that only locks at aquiring the ressource (actually, here it means taking a copy) and only updating it at the end of the processing (remember those infamous access EJB ?). This can be that at be beginning of computing a task, every data from the workcase is copied in a new, non-shared, worktask. All the task work will then be done on the privately copied data. It can surely be optimised in copying only the data that \"might\" be used (read and/or write). And at the end of the task, the workcase is just \"commited\" (updated) in the main data storage. The nice thing is that you only need to synchronise the beginning, the end and to prevent concurrent modifications (usually done with a incrementing version counter).\r\n\r\nNow, if you cross this with another previous post about [caching cleverly and sparingly|/post/2007/10/11/Keep-your-caches-coherent-%3A-Scope-them|en], you can also have another way of having for exemple a configuration that is at the same time : \r\n# fast\r\n# can be updated at runtime \r\n# transactional (once you access it once, you will have all the properties that are coherent together)\r\n\r\nThe idea is to use [immutable objects|http://en.wikipedia.org/wiki/Immutable_object|en] (such as java.lang.String). They are usually despised as memory eaters since you have to create a whole bunch of objects since you cannot modifiy them, only recreate them with the updated values. But they have a very nice property : there are completly thread-safe, since no one can modify them, so they are lock-free.\r\n\r\nSo, just imaging that the first time you ask for a configuration, you just load the whole in an immutable config object into something like a singleton. You just hand a reference to it to the called after you stored the reference in the caller's context (could be a HttpServletRequest). The second time the caller asks for the configuration it's already in its HttpServletRequest, so you take it from there.\r\n\r\nMeanwhile some other thread just asks to refresh the configuration, a new immutable config object is created, it's swapped with the old one (only the reference is updated, not the object). This swap and the handing are to be synchronized together (it's not even always mandatory, since usually if there are several handings that give the old value, it's often not that problematic : the whole ole value is coherent). When all the old contexts will go out of scope, so will the old config object.\r\n\r\nThe use of immutable objects has becomed much easier with GC, since we don't have to track the scoping anymore (usually it was done through a pseudo-immutable object, that was only mutable on the reference counting).\r\n\r\nIt's also one application of COR (Copy On Read) instead of the more usual COW (Copy On Write).","<p>With the future and its multiple core environnements as stated in a previous\npost about <a href=\"/post/2007/09/10/Are-Workflows-the-Future-of-IT-Computing\" hreflang=\"en\">workflows</a>, efficient locking will be more and more an\nissue.</p>\n<p>My previous way to cut this gordian knot was to :</p>\n<ol>\n<li>multiply the objects that could be locked to reduce contention : have many\nmultiple elementary objects . These can be workcases in the workflow\ntheory.</li>\n<li><em>cheat</em> to minimize the time spend on locking : use something called\n<a href=\"http://en.wikipedia.org/wiki/Software_transactional_memory\" hreflang=\"en\">software transactional memory</a> that only locks at aquiring the\nressource (actually, here it means taking a copy) and only updating it at the\nend of the processing (remember those infamous access EJB ?). This can be that\nat be beginning of computing a task, every data from the workcase is copied in\na new, non-shared, worktask. All the task work will then be done on the\nprivately copied data. It can surely be optimised in copying only the data that\n&quot;might&quot; be used (read and/or write). And at the end of the task, the workcase\nis just &quot;commited&quot; (updated) in the main data storage. The nice thing is that\nyou only need to synchronise the beginning, the end and to prevent concurrent\nmodifications (usually done with a incrementing version counter).</li>\n</ol>\n<p>Now, if you cross this with another previous post about <a href=\"/post/2007/10/11/Keep-your-caches-coherent-%3A-Scope-them\" hreflang=\"en\">caching cleverly and sparingly</a>, you can also have another way of\nhaving for exemple a configuration that is at the same time :</p>\n<ol>\n<li>fast</li>\n<li>can be updated at runtime</li>\n<li>transactional (once you access it once, you will have all the properties\nthat are coherent together)</li>\n</ol>\n<p>The idea is to use <a href=\"http://en.wikipedia.org/wiki/Immutable_object\" hreflang=\"en\">immutable objects</a> (such as java.lang.String). They are\nusually despised as memory eaters since you have to create a whole bunch of\nobjects since you cannot modifiy them, only recreate them with the updated\nvalues. But they have a very nice property : there are completly thread-safe,\nsince no one can modify them, so they are lock-free.</p>\n<p>So, just imaging that the first time you ask for a configuration, you just\nload the whole in an immutable config object into something like a singleton.\nYou just hand a reference to it to the called after you stored the reference in\nthe caller's context (could be a HttpServletRequest). The second time the\ncaller asks for the configuration it's already in its HttpServletRequest, so\nyou take it from there.</p>\n<p>Meanwhile some other thread just asks to refresh the configuration, a new\nimmutable config object is created, it's swapped with the old one (only the\nreference is updated, not the object). This swap and the handing are to be\nsynchronized together (it's not even always mandatory, since usually if there\nare several handings that give the old value, it's often not that problematic :\nthe whole ole value is coherent). When all the old contexts will go out of\nscope, so will the old config object.</p>\n<p>The use of immutable objects has becomed much easier with GC, since we don't\nhave to track the scoping anymore (usually it was done through a\npseudo-immutable object, that was only mutable on the reference counting).</p>\n<p>It's also one application of COR (Copy On Read) instead of the more usual\nCOW (Copy On Write).</p>","","use immutable objects avoid synchronisation with the future and its multiple core environnements stated previous post about workflows efficient locking will more and more issue previous way cut this gordian knot was multiply the objects that could locked reduce contention have many multiple elementary objects these can workcases the workflow theory cheat minimize the time spend locking use something called software transactional memory that only locks aquiring the ressource actually here means taking copy and only updating the end the processing remember those infamous access ejb this can that beginning computing task every data from the workcase copied new non shared worktask all the task work will then done the privately copied data can surely optimised copying only the data that quot might quot used read and write and the end the task the workcase just quot commited quot updated the main data storage the nice thing that you only need synchronise the beginning the end and prevent concurrent modifications usually done with incrementing version counter now you cross this with another previous post about caching cleverly and sparingly you can also have another way having for exemple configuration that the same time fast can updated runtime transactional once you access once you will have all the properties that are coherent together the idea use immutable objects such java lang string they are usually despised memory eaters since you have create whole bunch objects since you cannot modifiy them only recreate them with the updated values but they have very nice property there are completly thread safe since one can modify them they are lock free just imaging that the first time you ask for configuration you just load the whole immutable config object into something like singleton you just hand reference the called after you stored the reference the caller context could httpservletrequest the second time the caller asks for the configuration already its httpservletrequest you take from there meanwhile some other thread just asks refresh the configuration new immutable config object created swapped with the old one only the reference updated not the object this swap and the handing are synchronized together not even always mandatory since usually there are several handings that give the old value often not that problematic the whole ole value coherent when all the old contexts will out scope will the old config object the use immutable objects has becomed much easier with since don have track the scoping anymore usually was done through pseudo immutable object that was only mutable the reference counting also one application cor copy read instead the more usual cow copy write","a:0:{}","1","0","1","1","0","0","0"
"403789","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2009-05-26 08:00:00","Europe/Paris","2009-05-20 05:40:58","2009-06-01 16:13:15","","post","wiki","2009/05/Compare-Efficiently-in-Java-:-Embrace-Smart-Comparison","en","Compare Efficiently in Java : Embrace Smart Comparison","In Java, comparing things can be quite tricky. Comparing them with a simple @@==@@ only compares the references, and for example 2 strings could be the same without being the same __object__. You have to compare them with the @@equals()@@ method. Now the infamous NullPointerException (NPE) shows its ugly head when the first object to compare is @@null@@.","<p>In Java, comparing things can be quite tricky. Comparing them with a simple\n<code>==</code> only compares the references, and for example 2 strings could\nbe the same without being the same <strong>object</strong>. You have to compare\nthem with the <code>equals()</code> method. Now the infamous\nNullPointerException (NPE) shows its ugly head when the first object to compare\nis <code>null</code>.</p>","Bill the Lizard shows us [a nice trick to avoid this NPE|http://www.billthelizard.com/2008/08/avoid-npe-when-comparing-strings.html|en] when used with string literals : call the method on the string literal. It's possible because it's also an object, and cannot be @@null@@. \r\n\r\nBut, as much as this trick code seems sweet, but it just feels like a red herring on the long run, First it only works for comparing strings and even only when comparing a string with a string literal. \r\n\r\n!!!! To Equality ...\r\n\r\nIt is what he said at the end that is much more interesting :\r\n\r\n>  The lesson to be learned here is that if you have to resort to \"clever\" tricks to get something done, with just a little bit of lateral thinking you can probably find a cleaner, simpler way. \r\n\r\nLet's take this lesson even further.\r\n\r\nI had a Perl & C++ background before coming to the Java world, and I am sometimes puzzled that Java makes things quite unnecessary complicated for the programmer from time to time.\r\n\r\nMy little suggestion : create a quite simple helper static function : \r\n\r\n///\r\nstatic boolean isEqual(String a, String b) {\r\n   if (a == b) return true;\r\n\r\n   // a & b cannot be null at the same time\r\n   if (a == null || b == null) return false;\r\n\r\n   // Now none can be null\r\n   return a.equals(b);\r\n}\r\n///\r\n\r\nNow, the NPE is avoided for any kind of string, with any order. We could also have a ''smarter'' equality comparison function by first converting to a string, and comparing the resulting string instead of comparing the original numbers.\r\n\r\n///\r\nstatic boolean isEqualSmart(Object a, Object b) {\r\n   if (a == b) return true;\r\n\r\n   // a & b cannot be null at the same time\r\n   if (a == null || b == null) return false;\r\n\r\n   // Now none can be null\r\n   return a.toString().equals(b.toString());\r\n}\r\n///\r\n\r\nWith the wonders of Java's function overloading, you can even write specialized functions that convert even more ''smartly'' their arguments. And then be able to compare a string with a number (even a primitive type).\r\n\r\n!!!! ... and Beyond\r\n\r\nWe began about testing equality, but for comparison such as @@<=@@ or @@>@@, you have to decode the output of the quite ugly @@compareTo()@@ method. The helper static function trick comes also handy here.\r\n\r\nFirst, we design a complete API that enable alphanumeric, and numeric comparison : the so-called __smart comparison__. The API should take any Object, or even primitive types, and convert it at will to enable hybrid comparison. This will greatly enhance the comparisons abilities of Java.\r\n\r\n///\r\nclass OpAlpha {\r\n\r\n   // Alphanumeric comparison\r\n   public static boolean lt(Object a, Object b) { \r\n      /* convert to string and compare */ \r\n   }\r\n   public static boolean gt(Object a, Object b) { \r\n      /* convert to string and compare */ \r\n   }\r\n\r\n   // ... rest of the implementation ...\r\n}\r\n\r\n\r\nclass OpNum {\r\n\r\n   // numeric comparison\r\n   public static boolean lt(Object a, Object b) { \r\n      /* convert to numeric and compare */ \r\n   }\r\n\r\n   // ... rest of the implementation ...\r\n}\r\n\r\nclass Op {\r\n   // smart comparison, depends on the 2nd argument type\r\n   public static boolean lt(Object a, int b) {\r\n      /* convert to int and compare */\r\n   }\r\n   public static boolean lt(Object a, String b) {\r\n      /* convert to string and compare */\r\n   }\r\n   public static boolean lt(Object a, Number b) {\r\n      /* convert to Number and compare */\r\n   }\r\n\r\n   // ... rest of the implementation ...\r\n}\r\n///\r\n\r\nThe usage is like this : \r\n\r\n///\r\nString a = \"124\";\r\nassertTrue( Op.eq(a, 124) );\r\nassertFalse( Op.lt(a, 1) );\r\nassertTrue( OpAlpha.lt(a, 1) );\r\n///\r\n\r\nWe can see that all the tedious part of handling the conversions is done under the hood. Choosing the comparison to is just a matter of choosing the right class. A little more work could even be done to take the @@Op@@ classes to implement @@Comparator@@ in a complete OO way in addition of the static helpers.\r\n\r\n!!!! Update (01/06/2009)\r\n\r\nIt seems that [equality in Java is quite hazardous|/post/2009/06/Equality-in-Java-is-a-Hot-Topic-but-a-Hazardous-one], and that you have to be extra careful not to set a comparison-land-mine off by accident.","<p>Bill the Lizard shows us <a href=\"http://www.billthelizard.com/2008/08/avoid-npe-when-comparing-strings.html\" hreflang=\"en\">a nice trick to avoid this NPE</a> when used with string literals\n: call the method on the string literal. It's possible because it's also an\nobject, and cannot be <code>null</code>.</p>\n<p>But, as much as this trick code seems sweet, but it just feels like a red\nherring on the long run, First it only works for comparing strings and even\nonly when comparing a string with a string literal.</p>\n<h2>To Equality ...</h2>\n<p>It is what he said at the end that is much more interesting :</p>\n<blockquote>\n<p>The lesson to be learned here is that if you have to resort to &quot;clever&quot;\ntricks to get something done, with just a little bit of lateral thinking you\ncan probably find a cleaner, simpler way.</p>\n</blockquote>\n<p>Let's take this lesson even further.</p>\n<p>I had a Perl &amp; C++ background before coming to the Java world, and I am\nsometimes puzzled that Java makes things quite unnecessary complicated for the\nprogrammer from time to time.</p>\n<p>My little suggestion : create a quite simple helper static function :</p>\n<pre>\nstatic boolean isEqual(String a, String b) {\n   if (a == b) return true;\n\n   // a &amp; b cannot be null at the same time\n   if (a == null || b == null) return false;\n\n   // Now none can be null\n   return a.equals(b);\n}\n</pre>\n<p>Now, the NPE is avoided for any kind of string, with any order. We could\nalso have a <em>smarter</em> equality comparison function by first converting\nto a string, and comparing the resulting string instead of comparing the\noriginal numbers.</p>\n<pre>\nstatic boolean isEqualSmart(Object a, Object b) {\n   if (a == b) return true;\n\n   // a &amp; b cannot be null at the same time\n   if (a == null || b == null) return false;\n\n   // Now none can be null\n   return a.toString().equals(b.toString());\n}\n</pre>\n<p>With the wonders of Java's function overloading, you can even write\nspecialized functions that convert even more <em>smartly</em> their arguments.\nAnd then be able to compare a string with a number (even a primitive type).</p>\n<h2>... and Beyond</h2>\n<p>We began about testing equality, but for comparison such as\n<code>&lt;=</code> or <code>&gt;</code>, you have to decode the output of the\nquite ugly <code>compareTo()</code> method. The helper static function trick\ncomes also handy here.</p>\n<p>First, we design a complete API that enable alphanumeric, and numeric\ncomparison : the so-called <strong>smart comparison</strong>. The API should\ntake any Object, or even primitive types, and convert it at will to enable\nhybrid comparison. This will greatly enhance the comparisons abilities of\nJava.</p>\n<pre>\nclass OpAlpha {\n\n   // Alphanumeric comparison\n   public static boolean lt(Object a, Object b) { \n      /* convert to string and compare */ \n   }\n   public static boolean gt(Object a, Object b) { \n      /* convert to string and compare */ \n   }\n\n   // ... rest of the implementation ...\n}\n\n\nclass OpNum {\n\n   // numeric comparison\n   public static boolean lt(Object a, Object b) { \n      /* convert to numeric and compare */ \n   }\n\n   // ... rest of the implementation ...\n}\n\nclass Op {\n   // smart comparison, depends on the 2nd argument type\n   public static boolean lt(Object a, int b) {\n      /* convert to int and compare */\n   }\n   public static boolean lt(Object a, String b) {\n      /* convert to string and compare */\n   }\n   public static boolean lt(Object a, Number b) {\n      /* convert to Number and compare */\n   }\n\n   // ... rest of the implementation ...\n}\n</pre>\n<p>The usage is like this :</p>\n<pre>\nString a = &quot;124&quot;;\nassertTrue( Op.eq(a, 124) );\nassertFalse( Op.lt(a, 1) );\nassertTrue( OpAlpha.lt(a, 1) );\n</pre>\n<p>We can see that all the tedious part of handling the conversions is done\nunder the hood. Choosing the comparison to is just a matter of choosing the\nright class. A little more work could even be done to take the <code>Op</code>\nclasses to implement <code>Comparator</code> in a complete OO way in addition\nof the static helpers.</p>\n<h2>Update (01/06/2009)</h2>\n<p>It seems that <a href=\"/post/2009/06/Equality-in-Java-is-a-Hot-Topic-but-a-Hazardous-one\">equality in\nJava is quite hazardous</a>, and that you have to be extra careful not to set a\ncomparison-land-mine off by accident.</p>","","compare efficiently java embrace smart comparison java comparing things can quite tricky comparing them with simple only compares the references and for example strings could the same without being the same object you have compare them with the equals method now the infamous nullpointerexception npe shows its ugly head when the first object compare null bill the lizard shows nice trick avoid this npe when used with string literals call the method the string literal possible because also object and cannot null but much this trick code seems sweet but just feels like red herring the long run first only works for comparing strings and even only when comparing string with string literal equality what said the end that much more interesting the lesson learned here that you have resort quot clever quot tricks get something done with just little bit lateral thinking you can probably find cleaner simpler way let take this lesson even further had perl amp background before coming the java world and sometimes puzzled that java makes things quite unnecessary complicated for the programmer from time time little suggestion create quite simple helper static function static boolean isequal string string return true amp cannot null the same time null null return false now none can null return equals now the npe avoided for any kind string with any order could also have smarter equality comparison function first converting string and comparing the resulting string instead comparing the original numbers static boolean isequalsmart object object return true amp cannot null the same time null null return false now none can null return tostring equals tostring with the wonders java function overloading you can even write specialized functions that convert even more smartly their arguments and then able compare string with number even primitive type and beyond began about testing equality but for comparison such you have decode the output the quite ugly compareto method the helper static function trick comes also handy here first design complete api that enable alphanumeric and numeric comparison the called smart comparison the api should take any object even primitive types and convert will enable hybrid comparison this will greatly enhance the comparisons abilities java class opalpha alphanumeric comparison public static boolean object object convert string and compare public static boolean object object convert string and compare rest the implementation class opnum numeric comparison public static boolean object object convert numeric and compare rest the implementation class smart comparison depends the 2nd argument type public static boolean object int convert int and compare public static boolean object string convert string and compare public static boolean object number convert number and compare rest the implementation the usage like this string quot 124 quot asserttrue 124 assertfalse asserttrue opalpha can see that all the tedious part handling the conversions done under the hood choosing the comparison just matter choosing the right class little more work could even done take the classes implement comparator complete way addition the static helpers update 2009 seems that equality java quite hazardous and that you have extra careful not set comparison land mine off accident","","1","0","1","0","0","0","0"
"766348","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2014-01-19 14:01:00","Europe/Paris","2014-01-19 13:01:47","2014-01-19 13:30:11","","post","wiki","2014/01/How-to-try-Munin-2.1.x-easily-in-Debian-derivatives","en","How to try Munin 2.1.x easily in Debian-derivatives","","","In my previous post I explained how to [setup a simple development environment|/post/2014/01/Tutorial-Setup-a-dev-environment-for-Munin], but I feel that's a little too much if you only want to try that new, shiny, munin version.\r\n\r\nSo, let's assume that you're on a debian-derivative distribution (Ubuntu, Raspbian, ... , and of course Debian).\r\n\r\nThe nicest news is that 2.1.x already hit the [experimental part of Debian|https://wiki.debian.org/DebianExperimental|en]. That makes it __very__ easy to use as everything is described there. \r\n\r\nI'll just copy/paste some instructions tailored for munin, please refer to the original page if you'd like to learn more.\r\n\r\n!!! Add experimental to your sources.list\r\n\r\n///\r\necho \"deb http://ftp.debian.org/debian experimental main\" >> /etc/apt/sources.list\r\n///\r\n\r\n!!! Update the apt database\r\n\r\n///\r\napt-update\r\n///\r\n\r\n!!! Install the experimental version of munin\r\n\r\n///\r\napt-get -t experimental install munin\r\n///\r\n\r\n!!! Pin the munin as the experimental version\r\n\r\nThat works quite well, but the munin package won't be updated, as the automatic upgrades only comes from the non-experimental part. And obviously the version there is lower than the experimental one.\r\n\r\nSo, that is called package __pinning__, that is to __pin__ a certain package in a certain repository. It is still very simple to do : \r\n\r\n///\r\ncat >> /etc/apt/preferences <<EOF\r\nPackage: munin\r\nPin: release a=experimental\r\nPin-Priority: 800\r\nEOF\r\n///\r\n\r\n!!! Conclusion\r\n\r\nSo, that's it. You have successfully upgraded to the unstable side of munin...","<p>In my previous post I explained how to <a href=\"/post/2014/01/Tutorial-Setup-a-dev-environment-for-Munin\">setup a simple\ndevelopment environment</a>, but I feel that's a little too much if you only\nwant to try that new, shiny, munin version.</p>\n<p>So, let's assume that you're on a debian-derivative distribution (Ubuntu,\nRaspbian, ... , and of course Debian).</p>\n<p>The nicest news is that 2.1.x already hit the <a href=\"https://wiki.debian.org/DebianExperimental\" hreflang=\"en\">experimental part of\nDebian</a>. That makes it <strong>very</strong> easy to use as everything is\ndescribed there.</p>\n<p>I'll just copy/paste some instructions tailored for munin, please refer to\nthe original page if you'd like to learn more.</p>\n<h3>Add experimental to your sources.list</h3>\n<pre>\necho &quot;deb http://ftp.debian.org/debian experimental main&quot; &gt;&gt; /etc/apt/sources.list\n</pre>\n<h3>Update the apt database</h3>\n<pre>\napt-update\n</pre>\n<h3>Install the experimental version of munin</h3>\n<pre>\napt-get -t experimental install munin\n</pre>\n<h3>Pin the munin as the experimental version</h3>\n<p>That works quite well, but the munin package won't be updated, as the\nautomatic upgrades only comes from the non-experimental part. And obviously the\nversion there is lower than the experimental one.</p>\n<p>So, that is called package <strong>pinning</strong>, that is to\n<strong>pin</strong> a certain package in a certain repository. It is still\nvery simple to do :</p>\n<pre>\ncat &gt;&gt; /etc/apt/preferences &lt;&lt;EOF\nPackage: munin\nPin: release a=experimental\nPin-Priority: 800\nEOF\n</pre>\n<h3>Conclusion</h3>\n<p>So, that's it. You have successfully upgraded to the unstable side of\nmunin...</p>","","how try munin easily debian derivatives previous post explained how setup simple development environment but feel that little too much you only want try that new shiny munin version let assume that you debian derivative distribution ubuntu raspbian and course debian the nicest news that already hit the experimental part debian that makes very easy use everything described there just copy paste some instructions tailored for munin please refer the original page you like learn more add experimental your sources list echo quot deb http ftp debian org debian experimental main quot etc apt sources list update the apt database apt update install the experimental version munin apt get experimental install munin pin the munin the experimental version that works quite well but the munin package won updated the automatic upgrades only comes from the non experimental part and obviously the version there lower than the experimental one that called package pinning that pin certain package certain repository still very simple cat etc apt preferences eof package munin pin release experimental pin priority 800 eof conclusion that you have successfully upgraded the unstable side munin","a:0:{}","1","0","0","0","0","0","0"
"729310","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2013-12-02 21:06:00","Europe/Paris","2013-04-13 16:59:16","2013-12-03 10:06:59","","post","wiki","2013/12/Experimenting-with-a-C-munin-node","en","Experimenting with a C munin node","","","!!!! Core plugins are designed for simplicity...\r\n\r\nAs I wrote about it earlier, Helmut rewrote some core plugins in C. It was maintly done with efficiency in mind. \r\n\r\nAs those plugins are only parsing one @@/proc@@ file, there seemed no need to endure the many forks inherent with even trivial shell programming. It also acknowledges the fact that the measuring system shall be as light as possible \r\n\r\nMunin plugin are highly driven towards simplicity. Therefore having shell plugins is quite logical. It conveys the educational sample purpose for users to write their own, while being quite easy to code/debug for the developpers. Since their impact on current systems is very small, there are not much incentive to change.\r\n\r\n!!!!  ... but efficiency is coming !\r\n\r\nNonetheless, now monitored systems are becoming quite small. \r\n\r\nIt is mostly thanks to embedded systems like the [RaspberryPi|http://www.raspberrypi.org/|en]. This means that processing power available is much lower than on ''normal'' nodes$$Usually datacenter nodes are more in the high end of the spectrum than the low-end.$$.\r\n\r\n__Now__ the embedded C approach for plugins has a new rationale.","<h2>Core plugins are designed for simplicity...</h2>\n<p>As I wrote about it earlier, Helmut rewrote some core plugins in C. It was\nmaintly done with efficiency in mind.</p>\n<p>As those plugins are only parsing one <code>/proc</code> file, there seemed\nno need to endure the many forks inherent with even trivial shell programming.\nIt also acknowledges the fact that the measuring system shall be as light as\npossible</p>\n<p>Munin plugin are highly driven towards simplicity. Therefore having shell\nplugins is quite logical. It conveys the educational sample purpose for users\nto write their own, while being quite easy to code/debug for the developpers.\nSince their impact on current systems is very small, there are not much\nincentive to change.</p>\n<h2>... but efficiency is coming !</h2>\n<p>Nonetheless, now monitored systems are becoming quite small.</p>\n<p>It is mostly thanks to embedded systems like the <a href=\"http://www.raspberrypi.org/\" hreflang=\"en\">RaspberryPi</a>. This means that\nprocessing power available is much lower than on <em>normal</em>\nnodes<sup>[<a href=\"#pnote-729310-1\" id=\"rev-pnote-729310-1\" name=\"rev-pnote-729310-1\">1</a>]</sup>.</p>\n<p><strong>Now</strong> the embedded C approach for plugins has a new\nrationale.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-729310-1\" id=\"pnote-729310-1\" name=\"pnote-729310-1\">1</a>] Usually datacenter nodes are more in the high end of\nthe spectrum than the low-end.</p>\n</div>","","experimenting with munin node core plugins are designed for simplicity wrote about earlier helmut rewrote some core plugins was maintly done with efficiency mind those plugins are only parsing one proc file there seemed need endure the many forks inherent with even trivial shell programming also acknowledges the fact that the measuring system shall light possible munin plugin are highly driven towards simplicity therefore having shell plugins quite logical conveys the educational sample purpose for users write their own while being quite easy code debug for the developpers since their impact current systems very small there are not much incentive change but efficiency coming nonetheless now monitored systems are becoming quite small mostly thanks embedded systems like the raspberrypi this means that processing power available much lower than normal nodes now the embedded approach for plugins has new rationale notes usually datacenter nodes are more the high end the spectrum than the low end","a:1:{s:3:\"tag\";a:5:{i:0;s:8:\"sysadmin\";i:1;s:11:\"performance\";i:2;s:6:\"design\";i:3;s:5:\"munin\";i:4;s:1:\"c\";}}","1","0","1","0","0","0","0"
"525488","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2010-06-08 22:30:00","Europe/Paris","2010-06-08 20:22:12","2010-08-23 16:44:37","","post","wiki","2010/06/Waiting-for-Munin-2.0-Introduction","en","Waiting for Munin 2.0 - Introduction","","","This is the first article of a series about the coming version 2.0 of [Munin|http://munin-monitoring.org/|en]. \r\n\r\nThe idea came from the series [Waiting from 8.5|http://www.depesz.com/index.php/2009/07/03/waiting-for-8-5-lets-start/|en] about PostgreSQL. \r\n\r\nThe ironic part is that their 8.5 release has become a 9.0, just like our 1.5 will be a 2.0.\r\n\r\nI'll post several small articles about new or enhanced-enough features. They will all be tagged [munin20|tag:munin20].\r\n\r\nPlanned summary : \r\n# [Performance - Architecture context|/post/2010/06/Waiting-for-Munin-2.0-Performance-Introduction-Architecture]\r\n# [Performance - FastCGI|/post/2010/06/Using-FastCGI-with-a-CGI-only-server-The-FastCGI-wrapper]\r\n# [Performance - Asynchronous updates|/post/2010/06/Waiting-for-Munin-2.0-Performance-Asynchronous-updates]\r\n# Performance - Misc\r\n# [Native SSH transport|/post/2010/07/Waiting-for-Munin-2.0-Native-SSH-transport] \r\n# [Custom data retention plans (keep more data)|/post/2010/08/Waiting-for-Munin-2.0-Keep-more-data-with-custom-data-retention-plans]\r\n# Dynamic zooming","<p>This is the first article of a series about the coming version 2.0 of\n<a href=\"http://munin-monitoring.org/\" hreflang=\"en\">Munin</a>.</p>\n<p>The idea came from the series <a href=\"http://www.depesz.com/index.php/2009/07/03/waiting-for-8-5-lets-start/\" hreflang=\"en\">Waiting from 8.5</a> about PostgreSQL.</p>\n<p>The ironic part is that their 8.5 release has become a 9.0, just like our\n1.5 will be a 2.0.</p>\n<p>I'll post several small articles about new or enhanced-enough features. They\nwill all be tagged <a href=\"/tag/munin20\">munin20</a>.</p>\n<p>Planned summary :</p>\n<ol>\n<li><a href=\"/post/2010/06/Waiting-for-Munin-2.0-Performance-Introduction-Architecture\">Performance\n- Architecture context</a></li>\n<li><a href=\"/post/2010/06/Using-FastCGI-with-a-CGI-only-server-The-FastCGI-wrapper\">Performance\n- FastCGI</a></li>\n<li><a href=\"/post/2010/06/Waiting-for-Munin-2.0-Performance-Asynchronous-updates\">Performance\n- Asynchronous updates</a></li>\n<li>Performance - Misc</li>\n<li><a href=\"/post/2010/07/Waiting-for-Munin-2.0-Native-SSH-transport\">Native\nSSH transport</a></li>\n<li><a href=\"/post/2010/08/Waiting-for-Munin-2.0-Keep-more-data-with-custom-data-retention-plans\">\nCustom data retention plans (keep more data)</a></li>\n<li>Dynamic zooming</li>\n</ol>","","waiting for munin introduction this the first article series about the coming version munin the idea came from the series waiting from about postgresql the ironic part that their release has become just like our will post several small articles about new enhanced enough features they will all tagged munin20 planned summary performance architecture context performance fastcgi performance asynchronous updates performance misc native ssh transport custom data retention plans keep more data dynamic zooming","a:1:{s:3:\"tag\";a:2:{i:0;s:7:\"munin20\";i:1;s:5:\"munin\";}}","1","1","1","0","0","0","0"
"525782","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2010-06-10 03:48:00","Europe/Paris","2010-06-09 20:26:24","2010-06-17 15:50:53","","post","wiki","2010/06/Waiting-for-Munin-2.0-Performance-Introduction-Architecture","en","Waiting for Munin 2.0 - Performance - Architecture","","","!!!! A little intro/refresh on munin's architecture on the master\r\n\r\nMunin has a very simple architecture on the master : @@munin-cron@@ is launched via cron every 5 minutes. Its only job is to launch in order @@munin-update@@, @@munin-graph@@, @@munin-html@@ & @@munin-limits@@.\r\n\r\n!!!! The various processes\r\n\r\n!!! munin-update \r\n\r\nThis process retrieves the values from the various nodes and to update the rrd files. This one should never take more than 5 minutes to run, otherwise there will be gaps since the next update will not be launched (lockfile-protected runs). \r\n\r\nThis process stresses the I/O on the master, and depends on the plugins execution time on the various nodes. On 1.4 the retrieval is multi-threaded$$more multi-process actually$$, so an slow node doesn't impact too much the whole process.\r\n\r\n__2.0 proposes asynchronous updates and vectorized updates.__\r\n\r\n!!! munin-graph\r\n\r\nThis process generates all the image files from the rrd files. \r\n\r\nIt is usually a process that is quite CPU-bound, it generates also a fair load of I/O. Since 1.4 there might also be a parallel graphing generation in order to take advantage of multiple CPU / multiple I/O paths.\r\n\r\nA simple optimization is to generate only needed graphs instead of all of them each time. This leads to CGI-generation of graphs. 1.2 & 1.4 took a first step in this direction, but it's quite a hack since it's only a very basic script that calls @@munin-update@@ with the correct parameters. \r\n\r\nA FastCGI port of the wrapper (@@munin-cgi-graph@@) removes the overhead of starting the wrapper for each call, but in 1.4 the code is quite experimental and has some serious bugs that would need extensive patching to be fixed.\r\n\r\n__2.0 completes the integration of CGI graphing with removing the overhead of calling @@munin-graph@@ and does this extensive patching for bugs fixing__\r\n\r\n!!! munin-html\r\n\r\nThis process generates all the html files from the rrd files. \r\nThis one is quite fast for now.  \r\n\r\n!!! munin-limits\r\n\r\nThis process checks the limits to see if there is a warning/alert to send via mail or nagios.\r\nThis one is also quite fast for now.","<h2>A little intro/refresh on munin's architecture on the master</h2>\n<p>Munin has a very simple architecture on the master : <code>munin-cron</code>\nis launched via cron every 5 minutes. Its only job is to launch in order\n<code>munin-update</code>, <code>munin-graph</code>, <code>munin-html</code>\n&amp; <code>munin-limits</code>.</p>\n<h2>The various processes</h2>\n<h3>munin-update</h3>\n<p>This process retrieves the values from the various nodes and to update the\nrrd files. This one should never take more than 5 minutes to run, otherwise\nthere will be gaps since the next update will not be launched\n(lockfile-protected runs).</p>\n<p>This process stresses the I/O on the master, and depends on the plugins\nexecution time on the various nodes. On 1.4 the retrieval is\nmulti-threaded<sup>[<a href=\"#pnote-525782-1\" id=\"rev-pnote-525782-1\" name=\"rev-pnote-525782-1\">1</a>]</sup>, so an slow node doesn't impact too much the\nwhole process.</p>\n<p><strong>2.0 proposes asynchronous updates and vectorized\nupdates.</strong></p>\n<h3>munin-graph</h3>\n<p>This process generates all the image files from the rrd files.</p>\n<p>It is usually a process that is quite CPU-bound, it generates also a fair\nload of I/O. Since 1.4 there might also be a parallel graphing generation in\norder to take advantage of multiple CPU / multiple I/O paths.</p>\n<p>A simple optimization is to generate only needed graphs instead of all of\nthem each time. This leads to CGI-generation of graphs. 1.2 &amp; 1.4 took a\nfirst step in this direction, but it's quite a hack since it's only a very\nbasic script that calls <code>munin-update</code> with the correct\nparameters.</p>\n<p>A FastCGI port of the wrapper (<code>munin-cgi-graph</code>) removes the\noverhead of starting the wrapper for each call, but in 1.4 the code is quite\nexperimental and has some serious bugs that would need extensive patching to be\nfixed.</p>\n<p><strong>2.0 completes the integration of CGI graphing with removing the\noverhead of calling <code>munin-graph</code> and does this extensive patching\nfor bugs fixing</strong></p>\n<h3>munin-html</h3>\n<p>This process generates all the html files from the rrd files. This one is\nquite fast for now.</p>\n<h3>munin-limits</h3>\n<p>This process checks the limits to see if there is a warning/alert to send\nvia mail or nagios. This one is also quite fast for now.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-525782-1\" id=\"pnote-525782-1\" name=\"pnote-525782-1\">1</a>] more multi-process actually</p>\n</div>","","waiting for munin performance architecture little intro refresh munin architecture the master munin has very simple architecture the master munin cron launched via cron every minutes its only job launch order munin update munin graph munin html amp munin limits the various processes munin update this process retrieves the values from the various nodes and update the rrd files this one should never take more than minutes run otherwise there will gaps since the next update will not launched lockfile protected runs this process stresses the the master and depends the plugins execution time the various nodes the retrieval multi threaded slow node doesn impact too much the whole process proposes asynchronous updates and vectorized updates munin graph this process generates all the image files from the rrd files usually process that quite cpu bound generates also fair load since there might also parallel graphing generation order take advantage multiple cpu multiple paths simple optimization generate only needed graphs instead all them each time this leads cgi generation graphs amp took first step this direction but quite hack since only very basic script that calls munin update with the correct parameters fastcgi port the wrapper munin cgi graph removes the overhead starting the wrapper for each call but the code quite experimental and has some serious bugs that would need extensive patching fixed completes the integration cgi graphing with removing the overhead calling munin graph and does this extensive patching for bugs fixing munin html this process generates all the html files from the rrd files this one quite fast for now munin limits this process checks the limits see there warning alert send via mail nagios this one also quite fast for now notes more multi process actually","a:1:{s:3:\"tag\";a:2:{i:0;s:7:\"munin20\";i:1;s:5:\"munin\";}}","1","0","1","0","0","0","0"
"765452","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2014-01-15 22:18:00","Europe/Paris","2014-01-14 21:54:50","2014-01-18 22:16:07","","post","wiki","2014/01/Tutorial-Setup-a-dev-environment-for-Munin","en","Tutorial - Setup a dev environment for Munin","","","I discovered some time ago the marvelous @@dev_scripts/@@ directory in the munin source code. So, as its usage is ''very'' easy, I'll just write a tutorial about how to use it\r\n\r\n!!!! Prerequisites\r\n\r\nTo use it, one has to install all the packages needed for munin, and to grab a copy of the source code. Easiest is to use either a tarball, or to clone the git repository.\r\n\r\nNote that the [guidelines on contributing|http://munin-monitoring.org/browser/munin/README?rev=master|en] back are specified directly in the git repo.\r\n\r\nNow, I just assume you want to contribute back, otherwise you would not care much about the said dev environment. That means using the git way of doing it.\r\n\r\n!!!! Download the source code\r\n\r\nFirst step is to clone the git repository. We will use @@$HOME/src/munin@@ as the development directory. \r\n\r\n///\r\nmkdir -p $HOME/src\r\ncd $HOME/src\r\ngit clone https://github.com/munin-monitoring/munin munin\r\ncd munin\r\n///\r\n\r\n!!!! Compile munin\r\n\r\nNow, we have to compile the source code. I know that it sounds strange as the code is mostly Perl, but there are some templates that need to be filled with the environment specifics, such as the Perl interpreter path, a POSIX compatible shell, ...\r\n\r\n///\r\ndev_scripts/install 1\r\n///\r\n\r\nNow all munin (and munin-node) should be compiled and installed in @@$HOME/src/munin/sandbox@@. \r\n\r\nNote that the @@1@@ at the end is explained below.\r\n\r\n\r\n!!!! Using the dev tools\r\n\r\nThere are some different tools in @@dev_scripts/@@ : \r\n\r\n!!! install\r\n\r\nThis is the one you used already. You have to use it every time you want to recompile & deploy the package. \r\n\r\nThe @@1@@ argument, does a full re-install (wipe & install), so you don't usually want to do that.\r\n\r\n!!! restart_munin-node\r\n\r\nThis is a tool to start the development node. Note that it listens on the port 4948, so you can use it alongside a normal munin-node.\r\n\r\n!!! run\r\n\r\nThe @@run@@ command inside is used to launch all the executable parts of munin, such as @@munin-update@@, @@munin-html@@ or @@munin-limits@@. It can also be used to launch @@munin-run@@ and @@munin-node-configure@@.\r\n\r\nThe usage is very simple, just prefix the command to launch with @@dev_scripts/run@@, every environment variable and command line argument will be forwarded to the said command.\r\n\r\n///\r\n# launch munin-cron\r\ndev_scripts/munin-cron\r\n\r\n# launch manually some cron parts\r\ndev_scripts/munin-update\r\ndev_scripts/munin-limits\r\ndev_scripts/munin-html\r\ndev_scripts/munin-graph\r\n\r\n# debug a plugin\r\ndev_scripts/munin-run --debug cpu config\r\n///\r\n\r\n!!! cgi\r\n\r\nThis is the same as @@run@@, only for CGI. It sets up the whole environment vars that emulates a CGI call. Usage is very easy : \r\n\r\n///\r\ndev_scripts/cgi munin-cgi-graph /localnet/localhost/cpu-day.png > out.dat\r\n///\r\n\r\nThe @@out.dat@@ will contain the whole HTTP output, with the HTTP headers and the PNG content. Everything that is sent to STDERR won't be catched, so you can liberally use it while debugging.\r\n\r\n!!! query_munin_node\r\n\r\nThe @@query_munin_node@@ is used to send commands to the node in a very simple way. Node commands are just args of the tool.\r\n\r\n///\r\ndev_scripts/query_munin_node list\r\ndev_scripts/query_munin_node config cpu\r\ndev_scripts/query_munin_node fetch cpu\r\n///\r\n\r\n!!!! Real CGI usage with your web browser\r\n\r\nThat's the holy grail. You will have a development version that behaves the same as a real munin install. \r\n\r\nFirst, let's assume you have a working user cgi configuration (ie @@~user/cgi/whatever@@ is working). If not you should refer yourself to the local documentation of your preferred webserver. Note that nginx will _not_ work, as [it does not support CGI|http://wiki.nginx.org/ThttpdCGI|en]. \r\n\r\nI wrote a very simple cgi wrapper script. The home dir is hard coded in the script.\r\n\r\n///\r\n#! /bin/sh\r\n\r\nROOT=/home/me/src/munin\r\neval \"$(perl -V:version)\"\r\n\r\nPERL5LIB=$ROOT/sandbox/usr/local/share/perl/$version\r\n#export DBI_TRACE=2=/tmp/dbitrace.log\r\n\r\nexec perl -T -I $PERL5LIB $ROOT/sandbox/opt/munin/www/cgi/$CGI_NAME\r\n///","<p>I discovered some time ago the marvelous <code>dev_scripts/</code> directory\nin the munin source code. So, as its usage is <em>very</em> easy, I'll just\nwrite a tutorial about how to use it</p>\n<h2>Prerequisites</h2>\n<p>To use it, one has to install all the packages needed for munin, and to grab\na copy of the source code. Easiest is to use either a tarball, or to clone the\ngit repository.</p>\n<p>Note that the <a href=\"http://munin-monitoring.org/browser/munin/README?rev=master\" hreflang=\"en\">guidelines on contributing</a> back are specified directly in the git\nrepo.</p>\n<p>Now, I just assume you want to contribute back, otherwise you would not care\nmuch about the said dev environment. That means using the git way of doing\nit.</p>\n<h2>Download the source code</h2>\n<p>First step is to clone the git repository. We will use\n<code>$HOME/src/munin</code> as the development directory.</p>\n<pre>\nmkdir -p $HOME/src\ncd $HOME/src\ngit clone https://github.com/munin-monitoring/munin munin\ncd munin\n</pre>\n<h2>Compile munin</h2>\n<p>Now, we have to compile the source code. I know that it sounds strange as\nthe code is mostly Perl, but there are some templates that need to be filled\nwith the environment specifics, such as the Perl interpreter path, a POSIX\ncompatible shell, ...</p>\n<pre>\ndev_scripts/install 1\n</pre>\n<p>Now all munin (and munin-node) should be compiled and installed in\n<code>$HOME/src/munin/sandbox</code>.</p>\n<p>Note that the <code>1</code> at the end is explained below.</p>\n<h2>Using the dev tools</h2>\n<p>There are some different tools in <code>dev_scripts/</code> :</p>\n<h3>install</h3>\n<p>This is the one you used already. You have to use it every time you want to\nrecompile &amp; deploy the package.</p>\n<p>The <code>1</code> argument, does a full re-install (wipe &amp; install), so\nyou don't usually want to do that.</p>\n<h3>restart_munin-node</h3>\n<p>This is a tool to start the development node. Note that it listens on the\nport 4948, so you can use it alongside a normal munin-node.</p>\n<h3>run</h3>\n<p>The <code>run</code> command inside is used to launch all the executable\nparts of munin, such as <code>munin-update</code>, <code>munin-html</code> or\n<code>munin-limits</code>. It can also be used to launch <code>munin-run</code>\nand <code>munin-node-configure</code>.</p>\n<p>The usage is very simple, just prefix the command to launch with\n<code>dev_scripts/run</code>, every environment variable and command line\nargument will be forwarded to the said command.</p>\n<pre>\n# launch munin-cron\ndev_scripts/munin-cron\n\n# launch manually some cron parts\ndev_scripts/munin-update\ndev_scripts/munin-limits\ndev_scripts/munin-html\ndev_scripts/munin-graph\n\n# debug a plugin\ndev_scripts/munin-run --debug cpu config\n</pre>\n<h3>cgi</h3>\n<p>This is the same as <code>run</code>, only for CGI. It sets up the whole\nenvironment vars that emulates a CGI call. Usage is very easy :</p>\n<pre>\ndev_scripts/cgi munin-cgi-graph /localnet/localhost/cpu-day.png &gt; out.dat\n</pre>\n<p>The <code>out.dat</code> will contain the whole HTTP output, with the HTTP\nheaders and the PNG content. Everything that is sent to STDERR won't be\ncatched, so you can liberally use it while debugging.</p>\n<h3>query_munin_node</h3>\n<p>The <code>query_munin_node</code> is used to send commands to the node in a\nvery simple way. Node commands are just args of the tool.</p>\n<pre>\ndev_scripts/query_munin_node list\ndev_scripts/query_munin_node config cpu\ndev_scripts/query_munin_node fetch cpu\n</pre>\n<h2>Real CGI usage with your web browser</h2>\n<p>That's the holy grail. You will have a development version that behaves the\nsame as a real munin install.</p>\n<p>First, let's assume you have a working user cgi configuration (ie\n<code>~user/cgi/whatever</code> is working). If not you should refer yourself\nto the local documentation of your preferred webserver. Note that nginx will\n_not_ work, as <a href=\"http://wiki.nginx.org/ThttpdCGI\" hreflang=\"en\">it does\nnot support CGI</a>.</p>\n<p>I wrote a very simple cgi wrapper script. The home dir is hard coded in the\nscript.</p>\n<pre>\n#! /bin/sh\n\nROOT=/home/me/src/munin\neval &quot;$(perl -V:version)&quot;\n\nPERL5LIB=$ROOT/sandbox/usr/local/share/perl/$version\n#export DBI_TRACE=2=/tmp/dbitrace.log\n\nexec perl -T -I $PERL5LIB $ROOT/sandbox/opt/munin/www/cgi/$CGI_NAME\n</pre>","","tutorial setup dev environment for munin discovered some time ago the marvelous dev scripts directory the munin source code its usage very easy just write tutorial about how use prerequisites use one has install all the packages needed for munin and grab copy the source code easiest use either tarball clone the git repository note that the guidelines contributing back are specified directly the git repo now just assume you want contribute back otherwise you would not care much about the said dev environment that means using the git way doing download the source code first step clone the git repository will use home src munin the development directory mkdir home src home src git clone https github com munin monitoring munin munin munin compile munin now have compile the source code know that sounds strange the code mostly perl but there are some templates that need filled with the environment specifics such the perl interpreter path posix compatible shell dev scripts install now all munin and munin node should compiled and installed home src munin sandbox note that the the end explained below using the dev tools there are some different tools dev scripts install this the one you used already you have use every time you want recompile amp deploy the package the argument does full install wipe amp install you don usually want that restart munin node this tool start the development node note that listens the port 4948 you can use alongside normal munin node run the run command inside used launch all the executable parts munin such munin update munin html munin limits can also used launch munin run and munin node configure the usage very simple just prefix the command launch with dev scripts run every environment variable and command line argument will forwarded the said command launch munin cron dev scripts munin cron launch manually some cron parts dev scripts munin update dev scripts munin limits dev scripts munin html dev scripts munin graph debug plugin dev scripts munin run debug cpu config cgi this the same run only for cgi sets the whole environment vars that emulates cgi call usage very easy dev scripts cgi munin cgi graph localnet localhost cpu day png out dat the out dat will contain the whole http output with the http headers and the png content everything that sent stderr won catched you can liberally use while debugging query munin node the query munin node used send commands the node very simple way node commands are just args the tool dev scripts query munin node list dev scripts query munin node config cpu dev scripts query munin node fetch cpu real cgi usage with your web browser that the holy grail you will have development version that behaves the same real munin install first let assume you have working user cgi configuration user cgi whatever working not you should refer yourself the local documentation your preferred webserver note that nginx will not work does not support cgi wrote very simple cgi wrapper script the home dir hard coded the script bin root home src munin eval quot perl version quot perl5lib root sandbox usr local share perl version export dbi trace tmp dbitrace log exec perl perl5lib root sandbox opt munin www cgi cgi name","a:1:{s:3:\"tag\";a:2:{i:0;s:8:\"tutorial\";i:1;s:5:\"munin\";}}","1","0","0","0","0","0","0"
"613559","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2012-07-12 21:58:00","Europe/Paris","2011-06-13 13:43:23","2012-07-12 20:11:25","","post","wiki","2012/07/Waiting-for-Munin-2.0-Break-the-5-minutes-barrier","en","Waiting for Munin 2.0 - Break the 5 minutes barrier !","","","Every monitoring software has a polling rate. It is usually 5 min, because it's the sweet spot that enables frequent updates yet still having a low overhead.\r\n\r\nMunin is not different in that respect : it's data fetching routines __have__ to be launched every 5 min, otherwise you'll face data loss. And this 5 min period is deeply grained in the code. So changing it is possible, but very tedious and error prone.\r\n\r\nBut sometimes we need a very fine sampling rate. Every 10 seconds enables us to track fast changing metrics that would be averaged out otherwise. Changing the whole polling process to cope with a 10s period is very hard on hardware, since now __every__ update has to finish in these 10 seconds. \r\n\r\nThis triggered an extension in the plugin protocol, commonly known as ''supersampling''.\r\n\r\n!!!! Supersampling\r\n\r\n!!! Overview\r\n\r\nThe basic idea is that fine precision should only be for selected plugins only. It also cannot be triggered from the master, since the overhead would be way too big. \r\n\r\nSo, we just let the plugin sample itself the values at a rate it feels adequate. Then each polling round, the master fetches all the samples since last poll. \r\n\r\nThis enables various constructions, mostly around ''streaming'' plugins to achieve highly detailed sampling with a very small overhead.\r\n\r\n! Notes\r\n\r\nThis protocol is currently completely transparent to @@munin-node@@, and therefore it means that it can be used even on older (1.x) nodes. Only a 2.0 master is required.\r\n\r\n!!! Protocol details\r\n\r\nThe protocol itself is derived from the spoolfetch extension.\r\n\r\n!! Config\r\n\r\nA new directive is used, @@update_rate@@. It enables the master to create the rrd with an adequate step. \r\n\r\nOmitting it would lead to rrd averaging the supersampled values onto the default 5 min rate. This means __data loss__.\r\n\r\n! Notes\r\n\r\nThe heartbeat has always a 2 step size, so failure to send all the samples will result with unknown values, as expected. \r\n\r\nThe RRD file size is always the same in the default config, as all the RRA are configured proportionally to the @@update_rate@@. This means that, since you'll keep as much data as with the default, you keep it for a shorter time.\r\n\r\n!! Fetch\r\n\r\nWhen spoolfetching, the epoch is also sent in front of the value. Supersampling is then just a matter of sending multiple epoch/value lines, with monotonically increasing epoch. Note that since the epoch is an integer value for rrdtool, the smallest granularity is 1 second. For the time being, the protocol itself does also mandates integers. We can easily imagine that with another database as backend, an extension could be hacked together. \r\n\r\n!!! Compatibility with 1.4\r\n\r\nOn older 1.4 masters, only the last sampled value gets into the rrd.\r\n\r\n!!! Sample implementation\r\n\r\nThe canonical sample implementation is multicpu1sec, a contrib plugin on github. It is also a so-called streaming plugin. \r\n\r\n!!! Streaming plugins\r\n\r\nThese plugins fork a background process when called that streams a system tool into a spool file. In multipcu1sec, it is the @@mpstat@@ tool with a period of 1 second.\r\n\r\n!!!! Undersampling\r\n\r\nSome plugins are on the opposite side of the spectrum, as they only need a lower precision. \r\n\r\nIt makes sense when : \r\n* data should be kept for a ''very'' long time\r\n* data is __very__ expensive to generate and it doesn't vary fast.","<p>Every monitoring software has a polling rate. It is usually 5 min, because\nit's the sweet spot that enables frequent updates yet still having a low\noverhead.</p>\n<p>Munin is not different in that respect : it's data fetching routines\n<strong>have</strong> to be launched every 5 min, otherwise you'll face data\nloss. And this 5 min period is deeply grained in the code. So changing it is\npossible, but very tedious and error prone.</p>\n<p>But sometimes we need a very fine sampling rate. Every 10 seconds enables us\nto track fast changing metrics that would be averaged out otherwise. Changing\nthe whole polling process to cope with a 10s period is very hard on hardware,\nsince now <strong>every</strong> update has to finish in these 10 seconds.</p>\n<p>This triggered an extension in the plugin protocol, commonly known as\n<em>supersampling</em>.</p>\n<h2>Supersampling</h2>\n<h3>Overview</h3>\n<p>The basic idea is that fine precision should only be for selected plugins\nonly. It also cannot be triggered from the master, since the overhead would be\nway too big.</p>\n<p>So, we just let the plugin sample itself the values at a rate it feels\nadequate. Then each polling round, the master fetches all the samples since\nlast poll.</p>\n<p>This enables various constructions, mostly around <em>streaming</em> plugins\nto achieve highly detailed sampling with a very small overhead.</p>\n<h5>Notes</h5>\n<p>This protocol is currently completely transparent to\n<code>munin-node</code>, and therefore it means that it can be used even on\nolder (1.x) nodes. Only a 2.0 master is required.</p>\n<h3>Protocol details</h3>\n<p>The protocol itself is derived from the spoolfetch extension.</p>\n<h4>Config</h4>\n<p>A new directive is used, <code>update_rate</code>. It enables the master to\ncreate the rrd with an adequate step.</p>\n<p>Omitting it would lead to rrd averaging the supersampled values onto the\ndefault 5 min rate. This means <strong>data loss</strong>.</p>\n<h5>Notes</h5>\n<p>The heartbeat has always a 2 step size, so failure to send all the samples\nwill result with unknown values, as expected.</p>\n<p>The RRD file size is always the same in the default config, as all the RRA\nare configured proportionally to the <code>update_rate</code>. This means that,\nsince you'll keep as much data as with the default, you keep it for a shorter\ntime.</p>\n<h4>Fetch</h4>\n<p>When spoolfetching, the epoch is also sent in front of the value.\nSupersampling is then just a matter of sending multiple epoch/value lines, with\nmonotonically increasing epoch. Note that since the epoch is an integer value\nfor rrdtool, the smallest granularity is 1 second. For the time being, the\nprotocol itself does also mandates integers. We can easily imagine that with\nanother database as backend, an extension could be hacked together.</p>\n<h3>Compatibility with 1.4</h3>\n<p>On older 1.4 masters, only the last sampled value gets into the rrd.</p>\n<h3>Sample implementation</h3>\n<p>The canonical sample implementation is multicpu1sec, a contrib plugin on\ngithub. It is also a so-called streaming plugin.</p>\n<h3>Streaming plugins</h3>\n<p>These plugins fork a background process when called that streams a system\ntool into a spool file. In multipcu1sec, it is the <code>mpstat</code> tool\nwith a period of 1 second.</p>\n<h2>Undersampling</h2>\n<p>Some plugins are on the opposite side of the spectrum, as they only need a\nlower precision.</p>\n<p>It makes sense when :</p>\n<ul>\n<li>data should be kept for a <em>very</em> long time</li>\n<li>data is <strong>very</strong> expensive to generate and it doesn't vary\nfast.</li>\n</ul>","","waiting for munin break the minutes barrier every monitoring software has polling rate usually min because the sweet spot that enables frequent updates yet still having low overhead munin not different that respect data fetching routines have launched every min otherwise you face data loss and this min period deeply grained the code changing possible but very tedious and error prone but sometimes need very fine sampling rate every seconds enables track fast changing metrics that would averaged out otherwise changing the whole polling process cope with 10s period very hard hardware since now every update has finish these seconds this triggered extension the plugin protocol commonly known supersampling supersampling overview the basic idea that fine precision should only for selected plugins only also cannot triggered from the master since the overhead would way too big just let the plugin sample itself the values rate feels adequate then each polling round the master fetches all the samples since last poll this enables various constructions mostly around streaming plugins achieve highly detailed sampling with very small overhead notes this protocol currently completely transparent munin node and therefore means that can used even older nodes only master required protocol details the protocol itself derived from the spoolfetch extension config new directive used update rate enables the master create the rrd with adequate step omitting would lead rrd averaging the supersampled values onto the default min rate this means data loss notes the heartbeat has always step size failure send all the samples will result with unknown values expected the rrd file size always the same the default config all the rra are configured proportionally the update rate this means that since you keep much data with the default you keep for shorter time fetch when spoolfetching the epoch also sent front the value supersampling then just matter sending multiple epoch value lines with monotonically increasing epoch note that since the epoch integer value for rrdtool the smallest granularity second for the time being the protocol itself does also mandates integers can easily imagine that with another database backend extension could hacked together compatibility with older masters only the last sampled value gets into the rrd sample implementation the canonical sample implementation multicpu1sec contrib plugin github also called streaming plugin streaming plugins these plugins fork background process when called that streams system tool into spool file multipcu1sec the mpstat tool with period second undersampling some plugins are the opposite side the spectrum they only need lower precision makes sense when data should kept for very long time data very expensive generate and doesn vary fast","a:0:{}","1","0","1","0","0","0","0"
"475523","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38366","2010-03-31 21:00:00","Europe/Paris","2010-01-14 13:52:23","2010-03-31 17:17:40","","post","wiki","2010/03/API-Design:-Hidden-costs-of-simple-features","en","API Design: Avoid hidden costs of simple features","","","__Programmers__ are usually like water : they __always use__ the __path of least resistance__. \r\n\r\nLet's see how to use this fact to predict the usage of an API when you design it.\r\n\r\n!!!! Initial API\r\n\r\nConsider the very simple DB API that consumes a connected ResultSet and presents a disconnected version of it.\r\n\r\n///\r\nclass DisconnectedResultSet{\r\n	public DisconnectedResultSet (ResultSet rs);\r\n	public boolean next();\r\n	public Object getObject(int col_idx);\r\n}\r\n///\r\n\r\nIt's usage is quite easy : \r\n\r\n///\r\nwhile (drs.next()) {\r\n	int col_idx = 1;\r\n	drs.getObject(col_idx++); // Do something w/ 1st col\r\n	drs.getObject(col_idx++); // Do something w/ 2st col\r\n	//...\r\n}\r\n///\r\n\r\n!!!! Just a ''little'' evolution...\r\n\r\nSince the @@DisconnectedResultSet@@ is ''disconnected'', we can imagine that it should implement a @@rewind()@@ method in order to use it several times without running the initial query again.\r\nWe now have an updated class : \r\n\r\n///\r\nclass DisconnectedResultSet{\r\n	public DisconnectedResultSet (ResultSet rs);\r\n	public boolean next();\r\n	public Object getObject(int col_idx);	\r\n	public void rewind(); // Be able to rewind it\r\n}\r\n///\r\n\r\nAnd its classical usage :\r\n\r\n///\r\nwhile (drs.next()) {\r\n	// do stuff...\r\n}\r\n// ...\r\ndrs.rewind();\r\nwhile (drs.next()) {\r\n	// do something else with the same data...\r\n}\r\n// ...\r\ndrs.rewind();\r\nwhile (drs.next()) {\r\n	// do something else with the same data...\r\n}\r\n// ...\r\n///\r\n\r\n!!!! A new need comes\r\n\r\nA new need comes : see if the @@DisconnectedResultSet@@ is empty or not in order to avoid sending header. \r\n\r\nThe usual way is to send them once when iterating like : \r\n\r\n///\r\nboolean is_headers_sent = false;\r\nwhile (drs.next()) {\r\n	if (! is_headers_sent) { \r\n		send_headers(); \r\n		is_headers_sent = true;\r\n	}\r\n	// do something else with the same data...\r\n}\r\n///\r\n\r\nBut since there is a nice @@rewind()@@method, just waiting to be used, the code might become : \r\n\r\n///\r\nif (drs.next()) {\r\n	send_headers(); \r\n}\r\ndrs.rewind();\r\nwhile (drs.next()) {\r\n	// do something else with the same data...\r\n}\r\n///\r\n\r\nNow, this code isn't generic anymore to accommodate a connected ResultSet.\r\n\r\nSo, as John Carmack said : \r\n> The cost of adding a feature isn't just the time it takes to code it. \r\n> The cost also includes the addition of an obstacle to future expansion.\r\n\r\nThat's really true when you design __APIs__ since their purpose is to __last long__ and to __be extended__. \r\n\r\nSo, __think twice when you propose an extension \"just in case\"__.\r\n\r\n!!!! The ''little'' evolution, revisited...\r\n\r\nTo solve this case, don't propose a @@rewind()@@ method, but offer a @@duplicate()@@ one.  It offers the same functionality, just in a new object. \r\n\r\nThe usage will be almost the same as shown below, but since it feels more performance-sensitive, it won't be used as lightly : the @@boolean is_headers_sent@@ pattern has now more chances to be used.\r\n\r\n///\r\nwhile (drs.next()) {\r\n	// do stuff...\r\n}\r\n// ...\r\ndrs = drs.duplicate();\r\nwhile (drs.next()) {\r\n	// do something else with the same data...\r\n}\r\n// ...\r\ndrs = drs.duplicate();\r\nwhile (drs.next()) {\r\n	// do something else with the same data...\r\n}\r\n// ...\r\n///\r\n\r\nIt's an other example that [immutable objects are the way to go|/post/2007/12/03/Use-immutable-objects-to-avoid-synchronisation], but for a different reason this time.\r\n\r\nNote: Just finished my March 2010 article, even __on time__... I'm still trying to keep at least a one article per month blogging rate. So far so good for 2010, still 9 months to go !","<p><strong>Programmers</strong> are usually like water : they <strong>always\nuse</strong> the <strong>path of least resistance</strong>.</p>\n<p>Let's see how to use this fact to predict the usage of an API when you\ndesign it.</p>\n<h2>Initial API</h2>\n<p>Consider the very simple DB API that consumes a connected ResultSet and\npresents a disconnected version of it.</p>\n<pre>\nclass DisconnectedResultSet{\n        public DisconnectedResultSet (ResultSet rs);\n        public boolean next();\n        public Object getObject(int col_idx);\n}\n</pre>\n<p>It's usage is quite easy :</p>\n<pre>\nwhile (drs.next()) {\n        int col_idx = 1;\n        drs.getObject(col_idx++); // Do something w/ 1st col\n        drs.getObject(col_idx++); // Do something w/ 2st col\n        //...\n}\n</pre>\n<h2>Just a <em>little</em> evolution...</h2>\n<p>Since the <code>DisconnectedResultSet</code> is <em>disconnected</em>, we\ncan imagine that it should implement a <code>rewind()</code> method in order to\nuse it several times without running the initial query again. We now have an\nupdated class :</p>\n<pre>\nclass DisconnectedResultSet{\n        public DisconnectedResultSet (ResultSet rs);\n        public boolean next();\n        public Object getObject(int col_idx);   \n        public void rewind(); // Be able to rewind it\n}\n</pre>\n<p>And its classical usage :</p>\n<pre>\nwhile (drs.next()) {\n        // do stuff...\n}\n// ...\ndrs.rewind();\nwhile (drs.next()) {\n        // do something else with the same data...\n}\n// ...\ndrs.rewind();\nwhile (drs.next()) {\n        // do something else with the same data...\n}\n// ...\n</pre>\n<h2>A new need comes</h2>\n<p>A new need comes : see if the <code>DisconnectedResultSet</code> is empty or\nnot in order to avoid sending header.</p>\n<p>The usual way is to send them once when iterating like :</p>\n<pre>\nboolean is_headers_sent = false;\nwhile (drs.next()) {\n        if (! is_headers_sent) { \n                send_headers(); \n                is_headers_sent = true;\n        }\n        // do something else with the same data...\n}\n</pre>\n<p>But since there is a nice <code>rewind()</code>method, just waiting to be\nused, the code might become :</p>\n<pre>\nif (drs.next()) {\n        send_headers(); \n}\ndrs.rewind();\nwhile (drs.next()) {\n        // do something else with the same data...\n}\n</pre>\n<p>Now, this code isn't generic anymore to accommodate a connected\nResultSet.</p>\n<p>So, as John Carmack said :</p>\n<blockquote>\n<p>The cost of adding a feature isn't just the time it takes to code it. The\ncost also includes the addition of an obstacle to future expansion.</p>\n</blockquote>\n<p>That's really true when you design <strong>APIs</strong> since their purpose\nis to <strong>last long</strong> and to <strong>be extended</strong>.</p>\n<p>So, <strong>think twice when you propose an extension &quot;just in\ncase&quot;</strong>.</p>\n<h2>The <em>little</em> evolution, revisited...</h2>\n<p>To solve this case, don't propose a <code>rewind()</code> method, but offer\na <code>duplicate()</code> one. It offers the same functionality, just in a new\nobject.</p>\n<p>The usage will be almost the same as shown below, but since it feels more\nperformance-sensitive, it won't be used as lightly : the <code>boolean\nis_headers_sent</code> pattern has now more chances to be used.</p>\n<pre>\nwhile (drs.next()) {\n        // do stuff...\n}\n// ...\ndrs = drs.duplicate();\nwhile (drs.next()) {\n        // do something else with the same data...\n}\n// ...\ndrs = drs.duplicate();\nwhile (drs.next()) {\n        // do something else with the same data...\n}\n// ...\n</pre>\n<p>It's an other example that <a href=\"/post/2007/12/03/Use-immutable-objects-to-avoid-synchronisation\">immutable\nobjects are the way to go</a>, but for a different reason this time.</p>\n<p>Note: Just finished my March 2010 article, even <strong>on time</strong>...\nI'm still trying to keep at least a one article per month blogging rate. So far\nso good for 2010, still 9 months to go !</p>","You can use this fact to predict the usage of an API when you design it : you can be sure that the simpler version of the API will be the most used. Simple can also be replaced with seemingly \r\n\r\nSo while premature optimization is the root of evil, having insightful hints don't usually do much harm. \r\nAnd therefore you should keep the simpler version optimal, performance-wise.","api design avoid hidden costs simple features programmers are usually like water they always use the path least resistance let see how use this fact predict the usage api when you design initial api consider the very simple api that consumes connected resultset and presents disconnected version class disconnectedresultset public disconnectedresultset resultset public boolean next public object getobject int col idx usage quite easy while drs next int col idx drs getobject col idx something 1st col drs getobject col idx something 2st col just little evolution since the disconnectedresultset disconnected can imagine that should implement rewind method order use several times without running the initial query again now have updated class class disconnectedresultset public disconnectedresultset resultset public boolean next public object getobject int col idx public void rewind able rewind and its classical usage while drs next stuff drs rewind while drs next something else with the same data drs rewind while drs next something else with the same data new need comes new need comes see the disconnectedresultset empty not order avoid sending header the usual way send them once when iterating like boolean headers sent false while drs next headers sent send headers headers sent true something else with the same data but since there nice rewind method just waiting used the code might become drs next send headers drs rewind while drs next something else with the same data now this code isn generic anymore accommodate connected resultset john carmack said the cost adding feature isn just the time takes code the cost also includes the addition obstacle future expansion that really true when you design apis since their purpose last long and extended think twice when you propose extension quot just case quot the little evolution revisited solve this case don propose rewind method but offer duplicate one offers the same functionality just new object the usage will almost the same shown below but since feels more performance sensitive won used lightly the boolean headers sent pattern has now more chances used while drs next stuff drs drs duplicate while drs next something else with the same data drs drs duplicate while drs next something else with the same data other example that immutable objects are the way but for different reason this time note just finished march 2010 article even time still trying keep least one article per month blogging rate far good for 2010 still months","a:1:{s:3:\"tag\";a:3:{i:0;s:11:\"performance\";i:1;s:6:\"design\";i:2;s:3:\"api\";}}","1","0","1","0","0","0","0"
"403185","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2009-05-18 12:04:00","Europe/Paris","2009-05-18 10:04:04","2009-05-18 10:05:57","","post","wiki","2009/05/Are-Excerpts-a-Good-Thing","en","Are Excerpts a Good Thing ?","","","One thing I'm really wondering is : {{Should I use excerpts}} ?\r\n\r\nOn the plus side : \r\n* The list is easier to read & shorter to parse. You can then just click on the link that are interesting to you\r\n* I can manage to see which posts are mostly read (useful ?) since the comments aren't a good indicator$$It is __very__ far from being a very famous blog with hordes of readers :-p$$\r\n* Mostly all my readers comes from Google. Having a little list and only then content on a dedicated page seems to help being nicely referenced on more specifics, hence ususally more relevant topics.\r\n\r\nOn the down side : \r\n* The RSS flow and the front page list are truncated. You are one click further to the whole story.\r\n* It seems quite arrogant to force the reader to come to your site.\r\n\r\nI just edited my articles to have an excerpt when it is quite long.\r\n\r\nWhat do you think ?","<p>One thing I'm really wondering is : <q>Should I use excerpts</q> ?</p>\n<p>On the plus side :</p>\n<ul>\n<li>The list is easier to read &amp; shorter to parse. You can then just click\non the link that are interesting to you</li>\n<li>I can manage to see which posts are mostly read (useful ?) since the\ncomments aren't a good indicator<sup>[<a href=\"#pnote-403185-1\" id=\"rev-pnote-403185-1\" name=\"rev-pnote-403185-1\">1</a>]</sup></li>\n<li>Mostly all my readers comes from Google. Having a little list and only then\ncontent on a dedicated page seems to help being nicely referenced on more\nspecifics, hence ususally more relevant topics.</li>\n</ul>\n<p>On the down side :</p>\n<ul>\n<li>The RSS flow and the front page list are truncated. You are one click\nfurther to the whole story.</li>\n<li>It seems quite arrogant to force the reader to come to your site.</li>\n</ul>\n<p>I just edited my articles to have an excerpt when it is quite long.</p>\n<p>What do you think ?</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-403185-1\" id=\"pnote-403185-1\" name=\"pnote-403185-1\">1</a>] It is <strong>very</strong> far from being a very\nfamous blog with hordes of readers :-p</p>\n</div>","","are excerpts good thing one thing really wondering should use excerpts the plus side the list easier read amp shorter parse you can then just click the link that are interesting you can manage see which posts are mostly read useful since the comments aren good indicator mostly all readers comes from google having little list and only then content dedicated page seems help being nicely referenced more specifics hence ususally more relevant topics the down side the rss flow and the front page list are truncated you are one click further the whole story seems quite arrogant force the reader come your site just edited articles have excerpt when quite long what you think notes very far from being very famous blog with hordes readers","","1","0","1","0","0","0","0"
"265456","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2008-08-03 11:17:00","Europe/Paris","2008-07-31 18:23:48","2009-05-18 10:07:16","","post","wiki","2008/08/03/Daisy-Chain-Setters-and-Handle-Optional-Parameters-Effectively","en","Daisy Chain Setters and Handle Optional Parameters Effectively","A side effect of [RAII in Java|/post/2008/07/27/RAII-in-Java-to-clean-your-code|en] is that all the parameters have to be set at construction time, since construction time is the time to acquire the resources. A quite common problem is the handling of optional parameters.","<p>A side effect of <a href=\"/post/2008/07/27/RAII-in-Java-to-clean-your-code\" hreflang=\"en\">RAII in Java</a> is that all the parameters have to be set at\nconstruction time, since construction time is the time to acquire the\nresources. A quite common problem is the handling of optional parameters.</p>","!!The ''Pure RAII'' way\r\n\r\nYou use many different constructor signatures. It's quite suboptimal if you have many parameters that have nothing in common except their type : you have to use the infamous ''__null__'', or have a special value that conveys a ''not specified'' meaning.\r\n\r\nCode usage is something like this : \r\n\r\n///\r\nMyFile fileRead = new MyFile(\"in.txt\", Flags.Read);\r\nMyFile fileTemp = new MyFile(\"tmp.txt\", \r\n    Flags.Write, Boolean.TRUE, Boolean.FALSE);\r\nMyFile fileWrite = new MyFile(\"out.txt\", \r\n    Flags.Write, null, Flags.Boolean.FALSE); \r\n///\r\n\r\n!!The ''JavaBean'' way\r\n\r\nThe JavaBean standard militates for a simple constructor : the default one. The client uses then setters to initialize the object. This isn't RAII anymore, but is a very common idiom in Java. But in my opinion the code produced (like the one below) is quite cluttered.\r\n\r\nIt produces code like this : \r\n\r\n///\r\nMyFile file = new MyFile();\r\nfile.setFileName(fileName);\r\nfile.setOpenFlags(openFlags);\r\nfile.setShouldLock(shouldLock);\r\nfile.setIsSync(isSync);\r\nfile.setMaxFileSize(maxFileSize);\r\nfile.setReadAheadSize(readAheadSize); \r\n///\r\n\r\n!!The ''StringBuffer'' way\r\n\r\nWith the same trick as the @@StringBuffer.append()@@, it is possible to chain all the setters just like this :\r\n\r\n///\r\nMyFile file = new MyFile()\r\n    .setFileName(fileName)\r\n    .setOpenFlags(openFlags)\r\n    .setShouldLock(shouldLock)\r\n    .setIsSync(isSync)\r\n    .setMaxFileSize(maxFileSize)\r\n    .setReadAheadSize(readAheadSize)\r\n  ; \r\n///\r\n\r\nIn my opinion this is more typo-proof, since you don't need to repeat the variable name each time. It may also been seen as easier to read since the whole initializing part is done in one block.\r\n\r\nThe main design point with this construct is that the setters must honor __exactly the same contract__ than the constructor does : \r\n* They __cannot__ return null (otherwise chaining them will throw an uninformative [NPE|http://java.sun.com/javase/6/docs/api/java/lang/NullPointerException.html|en]). Only exceptions are allowed to signal a failure while setting the value.\r\n* In case they throw an exception, they have to cleanup any non-memory resource, since the caller has no reference on the object anymore.\r\n\r\nThese rules are quite natural if you think of the setters as a extension to the constructor. The object is still responsible for the resources it manages, although the resources itself can change when calling setters (by setting a different filename for example).\r\n\r\n!!The ''Immutable'' way\r\n\r\nSince every setter returns a MyFile, you can use an [immutable|http://en.wikipedia.org/wiki/Immutable_object|en] design. It has many benefits, specifically [when multi threading|http://blog.pwkf.org/post/2007/12/03/Use-immutable-objects-to-avoid-synchronisation|en]. The concept is quite easy, and inspired from the String object. Each constructor creates a brand new object that copies every properties from the parent except the one that is changing. The downside is it might create many unnecessary objects but it is a good concept for factories that are seldom created, but used many times.\r\n\r\nFactories, can then create object that have already all their properties set with the factory default. It can then reuse pooled objets that have the same properties.  \r\n\r\nAn example for a factory can be : \r\n///\r\nprivate final static MyFileFactory fileFactory = \r\n    new MyFileFactory().setIsSync(true);\r\n...\r\n    MyFile file =fileFactory.create(\"filename.txt\");\r\n///\r\n\r\nThe @@MyFileFactory.create()@@ must honor the same contract than the MyFile constructor for the same reasons than the setters do.","<h4>The <em>Pure RAII</em> way</h4>\n<p>You use many different constructor signatures. It's quite suboptimal if you\nhave many parameters that have nothing in common except their type : you have\nto use the infamous <em><strong>null</strong></em>, or have a special value\nthat conveys a <em>not specified</em> meaning.</p>\n<p>Code usage is something like this :</p>\n<pre>\nMyFile fileRead = new MyFile(&quot;in.txt&quot;, Flags.Read);\nMyFile fileTemp = new MyFile(&quot;tmp.txt&quot;, \n    Flags.Write, Boolean.TRUE, Boolean.FALSE);\nMyFile fileWrite = new MyFile(&quot;out.txt&quot;, \n    Flags.Write, null, Flags.Boolean.FALSE); \n</pre>\n<h4>The <em>JavaBean</em> way</h4>\n<p>The JavaBean standard militates for a simple constructor : the default one.\nThe client uses then setters to initialize the object. This isn't RAII anymore,\nbut is a very common idiom in Java. But in my opinion the code produced (like\nthe one below) is quite cluttered.</p>\n<p>It produces code like this :</p>\n<pre>\nMyFile file = new MyFile();\nfile.setFileName(fileName);\nfile.setOpenFlags(openFlags);\nfile.setShouldLock(shouldLock);\nfile.setIsSync(isSync);\nfile.setMaxFileSize(maxFileSize);\nfile.setReadAheadSize(readAheadSize); \n</pre>\n<h4>The <em>StringBuffer</em> way</h4>\n<p>With the same trick as the <code>StringBuffer.append()</code>, it is\npossible to chain all the setters just like this :</p>\n<pre>\nMyFile file = new MyFile()\n    .setFileName(fileName)\n    .setOpenFlags(openFlags)\n    .setShouldLock(shouldLock)\n    .setIsSync(isSync)\n    .setMaxFileSize(maxFileSize)\n    .setReadAheadSize(readAheadSize)\n  ; \n</pre>\n<p>In my opinion this is more typo-proof, since you don't need to repeat the\nvariable name each time. It may also been seen as easier to read since the\nwhole initializing part is done in one block.</p>\n<p>The main design point with this construct is that the setters must honor\n<strong>exactly the same contract</strong> than the constructor does :</p>\n<ul>\n<li>They <strong>cannot</strong> return null (otherwise chaining them will\nthrow an uninformative <a href=\"http://java.sun.com/javase/6/docs/api/java/lang/NullPointerException.html\" hreflang=\"en\">NPE</a>). Only exceptions are allowed to signal a failure while\nsetting the value.</li>\n<li>In case they throw an exception, they have to cleanup any non-memory\nresource, since the caller has no reference on the object anymore.</li>\n</ul>\n<p>These rules are quite natural if you think of the setters as a extension to\nthe constructor. The object is still responsible for the resources it manages,\nalthough the resources itself can change when calling setters (by setting a\ndifferent filename for example).</p>\n<h4>The <em>Immutable</em> way</h4>\n<p>Since every setter returns a MyFile, you can use an <a href=\"http://en.wikipedia.org/wiki/Immutable_object\" hreflang=\"en\">immutable</a>\ndesign. It has many benefits, specifically <a href=\"http://blog.pwkf.org/post/2007/12/03/Use-immutable-objects-to-avoid-synchronisation\" hreflang=\"en\">when multi threading</a>. The concept is quite easy, and inspired\nfrom the String object. Each constructor creates a brand new object that copies\nevery properties from the parent except the one that is changing. The downside\nis it might create many unnecessary objects but it is a good concept for\nfactories that are seldom created, but used many times.</p>\n<p>Factories, can then create object that have already all their properties set\nwith the factory default. It can then reuse pooled objets that have the same\nproperties.</p>\n<p>An example for a factory can be :</p>\n<pre>\nprivate final static MyFileFactory fileFactory = \n    new MyFileFactory().setIsSync(true);\n...\n    MyFile file =fileFactory.create(&quot;filename.txt&quot;);\n</pre>\n<p>The <code>MyFileFactory.create()</code> must honor the same contract than\nthe MyFile constructor for the same reasons than the setters do.</p>","","daisy chain setters and handle optional parameters effectively side effect raii java that all the parameters have set construction time since construction time the time acquire the resources quite common problem the handling optional parameters the pure raii way you use many different constructor signatures quite suboptimal you have many parameters that have nothing common except their type you have use the infamous null have special value that conveys not specified meaning code usage something like this myfile fileread new myfile quot txt quot flags read myfile filetemp new myfile quot tmp txt quot flags write boolean true boolean false myfile filewrite new myfile quot out txt quot flags write null flags boolean false the javabean way the javabean standard militates for simple constructor the default one the client uses then setters initialize the object this isn raii anymore but very common idiom java but opinion the code produced like the one below quite cluttered produces code like this myfile file new myfile file setfilename filename file setopenflags openflags file setshouldlock shouldlock file setissync issync file setmaxfilesize maxfilesize file setreadaheadsize readaheadsize the stringbuffer way with the same trick the stringbuffer append possible chain all the setters just like this myfile file new myfile setfilename filename setopenflags openflags setshouldlock shouldlock setissync issync setmaxfilesize maxfilesize setreadaheadsize readaheadsize opinion this more typo proof since you don need repeat the variable name each time may also been seen easier read since the whole initializing part done one block the main design point with this construct that the setters must honor exactly the same contract than the constructor does they cannot return null otherwise chaining them will throw uninformative npe only exceptions are allowed signal failure while setting the value case they throw exception they have cleanup any non memory resource since the caller has reference the object anymore these rules are quite natural you think the setters extension the constructor the object still responsible for the resources manages although the resources itself can change when calling setters setting different filename for example the immutable way since every setter returns myfile you can use immutable design has many benefits specifically when multi threading the concept quite easy and inspired from the string object each constructor creates brand new object that copies every properties from the parent except the one that changing the downside might create many unnecessary objects but good concept for factories that are seldom created but used many times factories can then create object that have already all their properties set with the factory default can then reuse pooled objets that have the same properties example for factory can private final static myfilefactory filefactory new myfilefactory setissync true myfile file filefactory create quot filename txt quot the myfilefactory create must honor the same contract than the myfile constructor for the same reasons than the setters","a:0:{}","1","0","1","0","0","0","0"
"532312","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2010-07-12 14:11:00","Europe/Paris","2010-07-09 08:33:40","2011-04-07 11:40:38","","post","wiki","2010/07/Waiting-for-Munin-2.0-Native-SSH-transport","en","Waiting for Munin 2.0 - Native SSH transport","","","In the munin architecture, the munin-master has to connect to the munin-node via a very simple protocol and plain TCP. \r\n\r\nThis has several advantages : \r\n# __Very__ simple to manage & install\r\n# Optional SSL since 1.4 enabling secure communications\r\n# Quite simple firewall rules. \r\n\r\nIt has also some disadvantages : \r\n# A new listening service means a wider exposure\r\n# The SSL option might add some administrative overhead (certificates management, ...)\r\n# A native protocol isn't always covered by all firewall solutions\r\n# Some organisations only authorize a few protocols to simplify audits (ex: only SSH & HTTPS)\r\n\r\n!!!! Native SSH\r\n\r\nTheses down points may be solved by [encapsulation over SSH|http://munin-monitoring.org/wiki/MuninSSHTunneling|en], but it can be a tedious task to maintain if the number of hosts increases.\r\n\r\nTherefore 2.0 introduces the concept of a __native SSH__ transport. Its usage is dead simple : replace the address with an @@ssh://@@ URL-like one. \r\n\r\nThe node still has to be modified to communicate with @@stdin@@/@@stdout@@ instead of a network socket. For now, only [pmmn|/post/2008/11/04/A-Poor-Man-s-Munin-Node-to-Monitor-Hostile-UNIX-Servers] and [munin-async|/post/2010/06/Waiting-for-Munin-2.0-Performance-Asynchronous-updates] are able to provide such a node. \r\n\r\n!!! Configuration\r\n\r\nThe URL is quite self-explanatory as shown in the example below :\r\n\r\n///\r\n[old-style-host]\r\n    address host.example.com\r\n\r\n[new-style-host]\r\n    address ssh://munin-node-user@host.example.com/path/to/stdio-enabled-node --params\r\n///\r\n\r\n!!! Installation notes\r\n\r\nAuthentication should be done without password but via SSH keys. The connection is from @@munin-user@host-munin@@ to @@munin-node-user@remote-node@@.\r\n\r\nIf you use @@munin-async@@, the user on the remote node might only be a readonly one, since it only needs to read spooled data. This implies that you use @@--spoolfetch@@ and not @@--vectorfetch@@ that updates the spool repository.\r\n\r\n!!!! Upcoming HTTP(S) transport in 3.0\r\n\r\nAnd the sweetest part is that since all the work has been done for adding another transport, adding a CGI-based HTTP transport one is possible (and therefore done) for 3.0.","<p>In the munin architecture, the munin-master has to connect to the munin-node\nvia a very simple protocol and plain TCP.</p>\n<p>This has several advantages :</p>\n<ol>\n<li><strong>Very</strong> simple to manage &amp; install</li>\n<li>Optional SSL since 1.4 enabling secure communications</li>\n<li>Quite simple firewall rules.</li>\n</ol>\n<p>It has also some disadvantages :</p>\n<ol>\n<li>A new listening service means a wider exposure</li>\n<li>The SSL option might add some administrative overhead (certificates\nmanagement, ...)</li>\n<li>A native protocol isn't always covered by all firewall solutions</li>\n<li>Some organisations only authorize a few protocols to simplify audits (ex:\nonly SSH &amp; HTTPS)</li>\n</ol>\n<h2>Native SSH</h2>\n<p>Theses down points may be solved by <a href=\"http://munin-monitoring.org/wiki/MuninSSHTunneling\" hreflang=\"en\">encapsulation over SSH</a>, but it can be a tedious task to maintain if\nthe number of hosts increases.</p>\n<p>Therefore 2.0 introduces the concept of a <strong>native SSH</strong>\ntransport. Its usage is dead simple : replace the address with an\n<code>ssh://</code> URL-like one.</p>\n<p>The node still has to be modified to communicate with\n<code>stdin</code>/<code>stdout</code> instead of a network socket. For now,\nonly <a href=\"/post/2008/11/04/A-Poor-Man-s-Munin-Node-to-Monitor-Hostile-UNIX-Servers\">pmmn</a>\nand <a href=\"/post/2010/06/Waiting-for-Munin-2.0-Performance-Asynchronous-updates\">munin-async</a>\nare able to provide such a node.</p>\n<h3>Configuration</h3>\n<p>The URL is quite self-explanatory as shown in the example below :</p>\n<pre>\n[old-style-host]\n    address host.example.com\n\n[new-style-host]\n    address ssh://munin-node-user@host.example.com/path/to/stdio-enabled-node --params\n</pre>\n<h3>Installation notes</h3>\n<p>Authentication should be done without password but via SSH keys. The\nconnection is from <code>munin-user@host-munin</code> to\n<code>munin-node-user@remote-node</code>.</p>\n<p>If you use <code>munin-async</code>, the user on the remote node might only\nbe a readonly one, since it only needs to read spooled data. This implies that\nyou use <code>--spoolfetch</code> and not <code>--vectorfetch</code> that\nupdates the spool repository.</p>\n<h2>Upcoming HTTP(S) transport in 3.0</h2>\n<p>And the sweetest part is that since all the work has been done for adding\nanother transport, adding a CGI-based HTTP transport one is possible (and\ntherefore done) for 3.0.</p>","","waiting for munin native ssh transport the munin architecture the munin master has connect the munin node via very simple protocol and plain tcp this has several advantages very simple manage amp install optional ssl since enabling secure communications quite simple firewall rules has also some disadvantages new listening service means wider exposure the ssl option might add some administrative overhead certificates management native protocol isn always covered all firewall solutions some organisations only authorize few protocols simplify audits only ssh amp https native ssh theses down points may solved encapsulation over ssh but can tedious task maintain the number hosts increases therefore introduces the concept native ssh transport its usage dead simple replace the address with ssh url like one the node still has modified communicate with stdin stdout instead network socket for now only pmmn and munin async are able provide such node configuration the url quite self explanatory shown the example below old style host address host example com new style host address ssh munin node user host example com path stdio enabled node params installation notes authentication should done without password but via ssh keys the connection from munin user host munin munin node user remote node you use munin async the user the remote node might only readonly one since only needs read spooled data this implies that you use spoolfetch and not vectorfetch that updates the spool repository upcoming http transport and the sweetest part that since all the work has been done for adding another transport adding cgi based http transport one possible and therefore done for","a:1:{s:3:\"tag\";a:5:{i:0;s:4:\"http\";i:1;s:8:\"sysadmin\";i:2;s:3:\"ssh\";i:3;s:7:\"munin20\";i:4;s:5:\"munin\";}}","1","0","1","0","3","0","0"
"504880","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2010-04-01 08:00:00","Europe/Paris","2010-03-31 17:25:48","2010-04-01 07:11:23","","post","wiki","2010/04/Do-not-use-Excerpt-At-least-with-DotClear","en","Don't use Excerpt... At least with DotClear.","","","DotClear automatically generates a @@meta description@@ tag from the blog entry, but it doesn't take the excerpt into account.\r\n\r\nIt just takes the beginning of the article __content__. Since the excerpt is also shown at the beginning of the article, I cannot just write 2 times the same content.\r\n\r\n@@meta description@@ is quite interesting since it is usually used for the little snipped under a search result in usual search engines, so having the beginning of the post in here is very nice.\r\n\r\nThis fact annihilates the [good point of having excerpts|/post/2009/05/Are-Excerpts-a-Good-Thing].\r\n\r\nI'm now falling back to removing progressively all the excerpts on my posts...","<p>DotClear automatically generates a <code>meta description</code> tag from\nthe blog entry, but it doesn't take the excerpt into account.</p>\n<p>It just takes the beginning of the article <strong>content</strong>. Since\nthe excerpt is also shown at the beginning of the article, I cannot just write\n2 times the same content.</p>\n<p><code>meta description</code> is quite interesting since it is usually used\nfor the little snipped under a search result in usual search engines, so having\nthe beginning of the post in here is very nice.</p>\n<p>This fact annihilates the <a href=\"/post/2009/05/Are-Excerpts-a-Good-Thing\">good point of having\nexcerpts</a>.</p>\n<p>I'm now falling back to removing progressively all the excerpts on my\nposts...</p>","","don use excerpt least with dotclear dotclear automatically generates meta description tag from the blog entry but doesn take the excerpt into account just takes the beginning the article content since the excerpt also shown the beginning the article cannot just write times the same content meta description quite interesting since usually used for the little snipped under search result usual search engines having the beginning the post here very nice this fact annihilates the good point having excerpts now falling back removing progressively all the excerpts posts","","1","0","1","0","0","0","0"
"224934","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2008-03-28 20:22:00","Europe/Paris","2008-03-28 19:22:06","2009-06-03 07:04:57","","post","wiki","2008/03/28/Speedup-OpenOffice-with-LeapHeap","en","Speed Up OpenOffice with LeapHeap","","","Using a new heap manager like [Leap Heap|http://www.leapheap.com/] under windows takes OpenOffice to blazing speeds. Tried the same with FireFox with the same conclusions. \r\n\r\nWould it be possible to learn from this little add-in ?","<p>Using a new heap manager like <a href=\"http://www.leapheap.com/\">Leap\nHeap</a> under windows takes OpenOffice to blazing speeds. Tried the same with\nFireFox with the same conclusions.</p>\n<p>Would it be possible to learn from this little add-in ?</p>","","speed openoffice with leapheap using new heap manager like leap heap under windows takes openoffice blazing speeds tried the same with firefox with the same conclusions would possible learn from this little add","a:0:{}","1","0","1","0","0","0","0"
"718156","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","","2013-02-04 13:27:00","Europe/Paris","2013-02-04 12:27:31","2013-02-04 12:27:31","","post","wiki","2013/02/Graph-network-traffic-with-1-sec-accuracy-in-Munin","en","Graph network traffic with 1 sec accuracy in Munin","","","Graph network traffic with 1 sec accuracy in Munin","<p>Graph network traffic with 1 sec accuracy in Munin</p>","","graph network traffic with sec accuracy munin graph network traffic with sec accuracy munin","","-2","0","1","0","0","0","0"
"397655","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2009-05-06 11:15:00","Europe/Paris","2009-04-27 16:10:09","2009-05-18 18:05:11","","post","wiki","2009/05/Bringing-C-Const-to-Java","en","Bringing C++ Const to Java","''Constness'' is another C\++ idiom, like RAII that I talked about [earlier|/post/2008/07/27/RAII-in-Java-to-clean-your-code]. We can then write code that are side effects free : when a function is called with a @@const@@ argument, we are assured that this argument will not  be modified ''under the hood'' without us knowing$$Of course, C\++ being what it is, there always will be ways. But let's say it's not as tempting to do as otherwise.$$.\r\n\r\nIn Java, the commonly accepted way is to use the @@final@@ keyword. But it has a major drawback : the object cannot be ''redefined'', but can be ''modified'' by calling mutable members. You have to convert it to an immutable type. This is a simple task, but radically different ways exists.","<p><em>Constness</em> is another C++ idiom, like RAII that I talked about\n<a href=\"/post/2008/07/27/RAII-in-Java-to-clean-your-code\">earlier</a>. We can\nthen write code that are side effects free : when a function is called with a\n<code>const</code> argument, we are assured that this argument will not be\nmodified <em>under the hood</em> without us knowing<sup>[<a href=\"#pnote-397655-1\" id=\"rev-pnote-397655-1\" name=\"rev-pnote-397655-1\">1</a>]</sup>.</p>\n<p>In Java, the commonly accepted way is to use the <code>final</code> keyword.\nBut it has a major drawback : the object cannot be <em>redefined</em>, but can\nbe <em>modified</em> by calling mutable members. You have to convert it to an\nimmutable type. This is a simple task, but radically different ways exists.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-397655-1\" id=\"pnote-397655-1\" name=\"pnote-397655-1\">1</a>] Of course, C++ being what it is, there always will be\nways. But let's say it's not as tempting to do as otherwise.</p>\n</div>","In Java, the commonly accepted way is to use the @@final@@ keyword. But it has a major drawback : the object cannot be ''redefined'', but can be ''modified'' by calling mutable members. You have to convert it to an immutable type. This is a simple task, but radically different ways exists.\r\n\r\n!!!! Achieve Immutable in Java\r\n\r\nWe'll see some differents methods with a common example. Let's start with a very simple @@Point@@ class :\r\n\r\n///\r\npublic class Point {\r\n    private int x;\r\n    private int y;\r\n    void setX(int i) { this.x = i; }\r\n    void setY(int i) { this.y = i; }\r\n    int getX() { return this.x; }\r\n    int getY() { return this.y; }\r\n}\r\n///\r\n\r\n!!! The ''Java Collection way''\r\n\r\nIt is the easiest way. It is designed to be a drop-in, like in the Java Collection API. You just create a read-only class that derives your read-write class just like the @@PointReadOnly@@ that mimics the r/o List. It overrides all the methods either to delegate or to forbid calling by throwing a run-time exception.\r\n\r\n///\r\npublic class PointReadOnly extends Point {\r\n    private final Point inner;\r\n    public PointReadOnly(Point p) { this.inner = p }; \r\n\r\n    void setX(int i) { \r\n        throw new UnsupportedOperationException(\"PointReadOnly is read-only\"); \r\n    }\r\n    void setY(int i) {\r\n        throw new UnsupportedOperationException(\"PointReadOnly is read-only\");\r\n    }\r\n\r\n    int getX() { return inner.getX(); }\r\n    int getY() { return inner.getY(); }\r\n}\r\n///\r\n\r\n!! Advantages\r\n\r\nYou can use it __right-now__ : Your code can slowly evolve to progressively generate and integrate this new object without really knowing it's here. The misuses, that are changing the object, will be catched in the testing phase when exceptions are being thrown. The use of a good logging system in the setters is a nice addition.\r\n\r\n!! Disadvantages\r\n\r\nAny pedantic compiler should warn you that the setter parameter isn't read. I tried to be quite clever and find ways to ignore it. Just rewrite it as : \r\n\r\n///\r\n    void setX(int i) { \r\n        if (i == 0) { /* Do nothing but keep the compiler happy */ }\r\n        throw new UnsupportedOperationException(\"PointReadOnly is read-only\"); \r\n    }\r\n///\r\n\r\nThe performance penalty is minimal since any compiler should optimize the if-case away and even if not, __this function should not be called anyway__.\r\n\r\nBut [as a user on StackOverflow pointed|http://stackoverflow.com/questions/786740/how-to-avoid-the-unused-param-warning-when-overriding-a-method-in-java-1-4/786794#786794|en], that warning ''definitively'' indicates a code smell : __this hierarchy blatantly violates Liskov's principle of substitution__ since a class receiving an instance of a @@Point@@ expects @@setX()@@ to work, and may not handle this exception correctly. Obviously a read-writeable class is readable, but a readable class is not necessarly a read-writeable class. This may introduce subtle bugs that would only be catched at runtime, and therefore testing, as simple as it may be, has to be quite thorough in order not to be bitten at a inconvenient time. \r\n\r\nA good solution that respects Liskov's principle is to redesign the hierarchy, we'll talk about it just later. But right now, don't despair, you can have almost the best of the both world : simple, yet ''almost'' respecting Liskov's principle.\r\n\r\n!! Circus time : ride two horses at once\r\n\r\nActually what violates Liskov's principle is that the setters don't do the job they are entitled to do. Throwing an exception is a sure way to __ensure__ that the object won't be modified, but that is usually overkill$$It all depends on the indent, specific policies on specific usages.$$. If you just log the erroneous call, but still do what it should do, you have the warnings, but not the nasty consequences. Just take care to log the callstack also, in order to be able to fix it much more easily.\r\n\r\nThe new method becomes then : \r\n\r\n///\r\nvoid setX(int i) { \r\n    Log.warn(new UnsupportedOperationException(\"PointReadOnly is read-only\")); \r\n    return inner.setX(i);\r\n}\r\n///\r\n\r\nA notable side effect is that your class isn't really read-only anymore though, just complain-only.\r\n\r\n!!! The ''Object-Oriented way''\r\n\r\nA much better solution is to create a read-only super class that the read-write one would inherit. The read-only class only implements getters (or any non-changing method). The read-write complement them with the setters.\r\n\r\nIt would lead to code like : \r\n\r\n///\r\npublic class PointReadOnly {\r\n    private final Point inner;\r\n    public PointReadOnly(Point p) { this.inner = p }; \r\n\r\n    int getX() { return inner.getX(); }\r\n    int getY() { return inner.getY(); }\r\n}\r\n\r\npublic class Point extends PointReadOnly {\r\n    private int x;\r\n    private int y;\r\n    void setX(int i) { this.x = i; }\r\n    void setY(int i) { this.y = i; }\r\n    int getX() { return this.x; }\r\n    int getY() { return this.y; }\r\n}\r\n///\r\n\r\n!! Advantages\r\n\r\nNow __Liskov's principle is not violated anymore__ and code that expects a read-only class can seamlessly have a r/o or a r/w class. \r\n\r\nThe changing will also spread naturally all throughout the codebase, just like the C\++ @@const@@ modifier and failures to comply would be caught at __compile time__ and not at runtime.\r\n\r\nThe r/o is a nice wrapper for the read-only class, and a r/w can even be used in place of the r/o place.\r\n\r\n!! Disadvantages\r\n\r\nIt doesn't ''feel'' right :The r/o class is just a placeholder for a r/w class.\r\n\r\n!! Comments\r\n\r\nAn interface-base inheritance scheme can even be better. @@Point@@ as the r/o interface, @@PointMutable@@ as the r/w and @@PointBase@@ as the base implentation class. This way you can avoid the wrapper class. You also get the C\++ @@const_cast<>()@@ for free (by just downcasting as @@PointMutable@@).\r\n\r\n///\r\ninterface Point { \r\n	int getX(); \r\n	int getY(); \r\n}\r\ninterface PointMutable extends Point { \r\n	void setX(int i); \r\n	void setY(int i);\r\n}\r\nclass PointBase implements Point { \r\n    private int x;\r\n    private int y;\r\n    void setX(int i) { this.x = i; }\r\n    void setY(int i) { this.y = i; }\r\n    int getX() { return this.x; }\r\n    int getY() { return this.y; }\r\n}\r\n///\r\n\r\nI realize this does not answer your question about getting rid of the warnings. But warnings can either be suppressed, ignored, or addressed. An unused parameter is often a bad smell that indicates your method might not be doing what it's expected to do. Methods should only get essential parameters. If the parameter is not used, the parameter is not essential, and therefore something needs to be changed.\r\n\r\n!!! The ''@@String@@ way''\r\n\r\nAnother way is just to avoid the read-only and mutable API all together and go the functional way. Since it's a completely different paradigm, it doesn't exactly fits the bill here but it's a quite important way of [cutting the gordian knot|http://en.wikipedia.org/wiki/Gordian_Knot|en]. Its concepts are rooted in both functional programming and RAII. It is quite practical only if you have an efficient garbage collection system, since it has the tendency to create lots of temporary objets that are mostly created, used and forgot.\r\n\r\nDo do thing, you have to redesign the whole class, in order to be able to defined it only once, and when you call a modifying method, you end up with an other object, also immutable. The @@String@@, @@Integer@@ and @@Class@@ class make extensive usage of this.\r\n\r\nSo, our class will become : \r\n\r\n///\r\nclass final Point {\r\n    private final int x;\r\n    private final int y;\r\n    Point(int x, int y) { this.x = x; this.y = y; }\r\n    int getX() { return this.x; }\r\n    int getY() { return this.y; }\r\n\r\n    void setX(int i) { return new Point(i, this.y); }\r\n    void setY(int i) { return new Point(this.x, i); }\r\n}\r\n///\r\n\r\nAs you can see, every object is not modifiable, if a method recieves the object as an argument, it cannot modify it, since it would have a new object. The properties are final, and the class has usually to be made final in order not to be able to mutate the class by inheritance. \r\n\r\nThis enables programming without side-effect and therefore is quite interesting for multi-thread programming.\r\n\r\n!!!! Conclusion & thoughts\r\n\r\nThe @@UnsupportedOperationException@@ is a very practical idiom, that even the Java Collection API is based on it. It may not be very nice in theory, but in practice it has its usages. It should not be abused thought, and the ''OO''-way should be prefered when possible, usually when you are in control of all the code base. \r\n\r\nThe logging approch doesn't really solve the problem since it's only advisory, but can fit the bill when the code base is huge and you want to be able to understand what is going wrong.\r\n\r\nThe ''OO''-way is a nice compromise between the abstract beauty of the String-way and the quite hugly kludge of the ''Collection''-way, especially when used with the interfaces scheme : it spreads out as the @@const@@ param, and force the coder to think if a modifying is really needed.\r\n\r\nThe ''String''-way is very tempting, as a true create-and-forget way. If you could even enforce to only have one instance of each representation you'll have a free @@==@@ (almost) overloaded operator$$Just be aware of classloader issue with @@static@@. Hmmm that will be covered in another article.$$.\r\n\r\nSo we have at least 3 ways to learn and adapt idioms from C\++ in Java. The major issue is that it's not as itegrated as @@const@@ : every object needs custom code except maybe the ''String''-way.","<p>In Java, the commonly accepted way is to use the <code>final</code> keyword.\nBut it has a major drawback : the object cannot be <em>redefined</em>, but can\nbe <em>modified</em> by calling mutable members. You have to convert it to an\nimmutable type. This is a simple task, but radically different ways exists.</p>\n<h2>Achieve Immutable in Java</h2>\n<p>We'll see some differents methods with a common example. Let's start with a\nvery simple <code>Point</code> class :</p>\n<pre>\npublic class Point {\n    private int x;\n    private int y;\n    void setX(int i) { this.x = i; }\n    void setY(int i) { this.y = i; }\n    int getX() { return this.x; }\n    int getY() { return this.y; }\n}\n</pre>\n<h3>The <em>Java Collection way</em></h3>\n<p>It is the easiest way. It is designed to be a drop-in, like in the Java\nCollection API. You just create a read-only class that derives your read-write\nclass just like the <code>PointReadOnly</code> that mimics the r/o List. It\noverrides all the methods either to delegate or to forbid calling by throwing a\nrun-time exception.</p>\n<pre>\npublic class PointReadOnly extends Point {\n    private final Point inner;\n    public PointReadOnly(Point p) { this.inner = p }; \n\n    void setX(int i) { \n        throw new UnsupportedOperationException(&quot;PointReadOnly is read-only&quot;); \n    }\n    void setY(int i) {\n        throw new UnsupportedOperationException(&quot;PointReadOnly is read-only&quot;);\n    }\n\n    int getX() { return inner.getX(); }\n    int getY() { return inner.getY(); }\n}\n</pre>\n<h4>Advantages</h4>\n<p>You can use it <strong>right-now</strong> : Your code can slowly evolve to\nprogressively generate and integrate this new object without really knowing\nit's here. The misuses, that are changing the object, will be catched in the\ntesting phase when exceptions are being thrown. The use of a good logging\nsystem in the setters is a nice addition.</p>\n<h4>Disadvantages</h4>\n<p>Any pedantic compiler should warn you that the setter parameter isn't read.\nI tried to be quite clever and find ways to ignore it. Just rewrite it as :</p>\n<pre>\n    void setX(int i) { \n        if (i == 0) { /* Do nothing but keep the compiler happy */ }\n        throw new UnsupportedOperationException(&quot;PointReadOnly is read-only&quot;); \n    }\n</pre>\n<p>The performance penalty is minimal since any compiler should optimize the\nif-case away and even if not, <strong>this function should not be called\nanyway</strong>.</p>\n<p>But <a href=\"http://stackoverflow.com/questions/786740/how-to-avoid-the-unused-param-warning-when-overriding-a-method-in-java-1-4/786794#786794\" hreflang=\"en\">as a user on StackOverflow pointed</a>, that warning\n<em>definitively</em> indicates a code smell : <strong>this hierarchy blatantly\nviolates Liskov's principle of substitution</strong> since a class receiving an\ninstance of a <code>Point</code> expects <code>setX()</code> to work, and may\nnot handle this exception correctly. Obviously a read-writeable class is\nreadable, but a readable class is not necessarly a read-writeable class. This\nmay introduce subtle bugs that would only be catched at runtime, and therefore\ntesting, as simple as it may be, has to be quite thorough in order not to be\nbitten at a inconvenient time.</p>\n<p>A good solution that respects Liskov's principle is to redesign the\nhierarchy, we'll talk about it just later. But right now, don't despair, you\ncan have almost the best of the both world : simple, yet <em>almost</em>\nrespecting Liskov's principle.</p>\n<h4>Circus time : ride two horses at once</h4>\n<p>Actually what violates Liskov's principle is that the setters don't do the\njob they are entitled to do. Throwing an exception is a sure way to\n<strong>ensure</strong> that the object won't be modified, but that is usually\noverkill<sup>[<a href=\"#pnote-397655-1\" id=\"rev-pnote-397655-1\" name=\"rev-pnote-397655-1\">1</a>]</sup>. If you just log the erroneous call, but\nstill do what it should do, you have the warnings, but not the nasty\nconsequences. Just take care to log the callstack also, in order to be able to\nfix it much more easily.</p>\n<p>The new method becomes then :</p>\n<pre>\nvoid setX(int i) { \n    Log.warn(new UnsupportedOperationException(&quot;PointReadOnly is read-only&quot;)); \n    return inner.setX(i);\n}\n</pre>\n<p>A notable side effect is that your class isn't really read-only anymore\nthough, just complain-only.</p>\n<h3>The <em>Object-Oriented way</em></h3>\n<p>A much better solution is to create a read-only super class that the\nread-write one would inherit. The read-only class only implements getters (or\nany non-changing method). The read-write complement them with the setters.</p>\n<p>It would lead to code like :</p>\n<pre>\npublic class PointReadOnly {\n    private final Point inner;\n    public PointReadOnly(Point p) { this.inner = p }; \n\n    int getX() { return inner.getX(); }\n    int getY() { return inner.getY(); }\n}\n\npublic class Point extends PointReadOnly {\n    private int x;\n    private int y;\n    void setX(int i) { this.x = i; }\n    void setY(int i) { this.y = i; }\n    int getX() { return this.x; }\n    int getY() { return this.y; }\n}\n</pre>\n<h4>Advantages</h4>\n<p>Now <strong>Liskov's principle is not violated anymore</strong> and code\nthat expects a read-only class can seamlessly have a r/o or a r/w class.</p>\n<p>The changing will also spread naturally all throughout the codebase, just\nlike the C++ <code>const</code> modifier and failures to comply would be caught\nat <strong>compile time</strong> and not at runtime.</p>\n<p>The r/o is a nice wrapper for the read-only class, and a r/w can even be\nused in place of the r/o place.</p>\n<h4>Disadvantages</h4>\n<p>It doesn't <em>feel</em> right :The r/o class is just a placeholder for a\nr/w class.</p>\n<h4>Comments</h4>\n<p>An interface-base inheritance scheme can even be better. <code>Point</code>\nas the r/o interface, <code>PointMutable</code> as the r/w and\n<code>PointBase</code> as the base implentation class. This way you can avoid\nthe wrapper class. You also get the C++ <code>const_cast&lt;&gt;()</code> for\nfree (by just downcasting as <code>PointMutable</code>).</p>\n<pre>\ninterface Point { \n        int getX(); \n        int getY(); \n}\ninterface PointMutable extends Point { \n        void setX(int i); \n        void setY(int i);\n}\nclass PointBase implements Point { \n    private int x;\n    private int y;\n    void setX(int i) { this.x = i; }\n    void setY(int i) { this.y = i; }\n    int getX() { return this.x; }\n    int getY() { return this.y; }\n}\n</pre>\n<p>I realize this does not answer your question about getting rid of the\nwarnings. But warnings can either be suppressed, ignored, or addressed. An\nunused parameter is often a bad smell that indicates your method might not be\ndoing what it's expected to do. Methods should only get essential parameters.\nIf the parameter is not used, the parameter is not essential, and therefore\nsomething needs to be changed.</p>\n<h3>The <em><code>String</code> way</em></h3>\n<p>Another way is just to avoid the read-only and mutable API all together and\ngo the functional way. Since it's a completely different paradigm, it doesn't\nexactly fits the bill here but it's a quite important way of <a href=\"http://en.wikipedia.org/wiki/Gordian_Knot\" hreflang=\"en\">cutting the gordian\nknot</a>. Its concepts are rooted in both functional programming and RAII. It\nis quite practical only if you have an efficient garbage collection system,\nsince it has the tendency to create lots of temporary objets that are mostly\ncreated, used and forgot.</p>\n<p>Do do thing, you have to redesign the whole class, in order to be able to\ndefined it only once, and when you call a modifying method, you end up with an\nother object, also immutable. The <code>String</code>, <code>Integer</code> and\n<code>Class</code> class make extensive usage of this.</p>\n<p>So, our class will become :</p>\n<pre>\nclass final Point {\n    private final int x;\n    private final int y;\n    Point(int x, int y) { this.x = x; this.y = y; }\n    int getX() { return this.x; }\n    int getY() { return this.y; }\n\n    void setX(int i) { return new Point(i, this.y); }\n    void setY(int i) { return new Point(this.x, i); }\n}\n</pre>\n<p>As you can see, every object is not modifiable, if a method recieves the\nobject as an argument, it cannot modify it, since it would have a new object.\nThe properties are final, and the class has usually to be made final in order\nnot to be able to mutate the class by inheritance.</p>\n<p>This enables programming without side-effect and therefore is quite\ninteresting for multi-thread programming.</p>\n<h2>Conclusion &amp; thoughts</h2>\n<p>The <code>UnsupportedOperationException</code> is a very practical idiom,\nthat even the Java Collection API is based on it. It may not be very nice in\ntheory, but in practice it has its usages. It should not be abused thought, and\nthe <em>OO</em>-way should be prefered when possible, usually when you are in\ncontrol of all the code base.</p>\n<p>The logging approch doesn't really solve the problem since it's only\nadvisory, but can fit the bill when the code base is huge and you want to be\nable to understand what is going wrong.</p>\n<p>The <em>OO</em>-way is a nice compromise between the abstract beauty of the\nString-way and the quite hugly kludge of the <em>Collection</em>-way,\nespecially when used with the interfaces scheme : it spreads out as the\n<code>const</code> param, and force the coder to think if a modifying is really\nneeded.</p>\n<p>The <em>String</em>-way is very tempting, as a true create-and-forget way.\nIf you could even enforce to only have one instance of each representation\nyou'll have a free <code>==</code> (almost) overloaded operator<sup>[<a href=\"#pnote-397655-2\" id=\"rev-pnote-397655-2\" name=\"rev-pnote-397655-2\">2</a>]</sup>.</p>\n<p>So we have at least 3 ways to learn and adapt idioms from C++ in Java. The\nmajor issue is that it's not as itegrated as <code>const</code> : every object\nneeds custom code except maybe the <em>String</em>-way.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-397655-1\" id=\"pnote-397655-1\" name=\"pnote-397655-1\">1</a>] It all depends on the indent, specific policies on\nspecific usages.</p>\n<p>[<a href=\"#rev-pnote-397655-2\" id=\"pnote-397655-2\" name=\"pnote-397655-2\">2</a>] Just be aware of classloader issue with\n<code>static</code>. Hmmm that will be covered in another article.</p>\n</div>","","bringing const java constness another idiom like raii that talked about earlier can then write code that are side effects free when function called with const argument are assured that this argument will not modified under the hood without knowing java the commonly accepted way use the final keyword but has major drawback the object cannot redefined but can modified calling mutable members you have convert immutable type this simple task but radically different ways exists notes course being what there always will ways but let say not tempting otherwise java the commonly accepted way use the final keyword but has major drawback the object cannot redefined but can modified calling mutable members you have convert immutable type this simple task but radically different ways exists achieve immutable java see some differents methods with common example let start with very simple point class public class point private int private int void setx int this void sety int this int getx return this int gety return this the java collection way the easiest way designed drop like the java collection api you just create read only class that derives your read write class just like the pointreadonly that mimics the list overrides all the methods either delegate forbid calling throwing run time exception public class pointreadonly extends point private final point inner public pointreadonly point this inner void setx int throw new unsupportedoperationexception quot pointreadonly read only quot void sety int throw new unsupportedoperationexception quot pointreadonly read only quot int getx return inner getx int gety return inner gety advantages you can use right now your code can slowly evolve progressively generate and integrate this new object without really knowing here the misuses that are changing the object will catched the testing phase when exceptions are being thrown the use good logging system the setters nice addition disadvantages any pedantic compiler should warn you that the setter parameter isn read tried quite clever and find ways ignore just rewrite void setx int nothing but keep the compiler happy throw new unsupportedoperationexception quot pointreadonly read only quot the performance penalty minimal since any compiler should optimize the case away and even not this function should not called anyway but user stackoverflow pointed that warning definitively indicates code smell this hierarchy blatantly violates liskov principle substitution since class receiving instance point expects setx work and may not handle this exception correctly obviously read writeable class readable but readable class not necessarly read writeable class this may introduce subtle bugs that would only catched runtime and therefore testing simple may has quite thorough order not bitten inconvenient time good solution that respects liskov principle redesign the hierarchy talk about just later but right now don despair you can have almost the best the both world simple yet almost respecting liskov principle circus time ride two horses once actually what violates liskov principle that the setters don the job they are entitled throwing exception sure way ensure that the object won modified but that usually overkill you just log the erroneous call but still what should you have the warnings but not the nasty consequences just take care log the callstack also order able fix much more easily the new method becomes then void setx int log warn new unsupportedoperationexception quot pointreadonly read only quot return inner setx notable side effect that your class isn really read only anymore though just complain only the object oriented way much better solution create read only super class that the read write one would inherit the read only class only implements getters any non changing method the read write complement them with the setters would lead code like public class pointreadonly private final point inner public pointreadonly point this inner int getx return inner getx int gety return inner gety public class point extends pointreadonly private int private int void setx int this void sety int this int getx return this int gety return this advantages now liskov principle not violated anymore and code that expects read only class can seamlessly have class the changing will also spread naturally all throughout the codebase just like the const modifier and failures comply would caught compile time and not runtime the nice wrapper for the read only class and can even used place the place disadvantages doesn feel right the class just placeholder for class comments interface base inheritance scheme can even better point the interface pointmutable the and pointbase the base implentation class this way you can avoid the wrapper class you also get the const cast for free just downcasting pointmutable interface point int getx int gety interface pointmutable extends point void setx int void sety int class pointbase implements point private int private int void setx int this void sety int this int getx return this int gety return this realize this does not answer your question about getting rid the warnings but warnings can either suppressed ignored addressed unused parameter often bad smell that indicates your method might not doing what expected methods should only get essential parameters the parameter not used the parameter not essential and therefore something needs changed the string way another way just avoid the read only and mutable api all together and the functional way since completely different paradigm doesn exactly fits the bill here but quite important way cutting the gordian knot its concepts are rooted both functional programming and raii quite practical only you have efficient garbage collection system since has the tendency create lots temporary objets that are mostly created used and forgot thing you have redesign the whole class order able defined only once and when you call modifying method you end with other object also immutable the string integer and class class make extensive usage this our class will become class final point private final int private final int point int int this this int getx return this int gety return this void setx int return new point this void sety int return new point this you can see every object not modifiable method recieves the object argument cannot modify since would have new object the properties are final and the class has usually made final order not able mutate the class inheritance this enables programming without side effect and therefore quite interesting for multi thread programming conclusion amp thoughts the unsupportedoperationexception very practical idiom that even the java collection api based may not very nice theory but practice has its usages should not abused thought and the way should prefered when possible usually when you are control all the code base the logging approch doesn really solve the problem since only advisory but can fit the bill when the code base huge and you want able understand what going wrong the way nice compromise between the abstract beauty the string way and the quite hugly kludge the collection way especially when used with the interfaces scheme spreads out the const param and force the coder think modifying really needed the string way very tempting true create and forget way you could even enforce only have one instance each representation you have free almost overloaded operator have least ways learn and adapt idioms from java the major issue that not itegrated const every object needs custom code except maybe the string way notes all depends the indent specific policies specific usages just aware classloader issue with static hmmm that will covered another article","a:0:{}","1","1","1","0","0","0","0"
"340907","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2009-08-10 20:00:00","Europe/Paris","2009-03-19 10:18:29","2009-08-10 20:01:55","","post","wiki","2009/08/A-Simple-Dns-Server-for-a-SOHO-Network","en","A Simple Dns Server for a SOHO Network","","","I'm in search of a very simple DNS Server for a small network. It should be : \r\n\r\n* recursive & caching (can be used as a proxy) \r\n* very simple administration (parsing /etc/hosts would be perfect, raw DNS zones like BIND would be a little bit overkill) \r\n* quite lightweight (aka no dependency on an SQL engine like MySQL, such as MyDNS)\r\n* Seamless integration to Windows lookups (nmblookup) via proxying functions (DNS to/from NMB)","<p>I'm in search of a very simple DNS Server for a small network. It should be\n:</p>\n<ul>\n<li>recursive &amp; caching (can be used as a proxy)</li>\n<li>very simple administration (parsing /etc/hosts would be perfect, raw DNS\nzones like BIND would be a little bit overkill)</li>\n<li>quite lightweight (aka no dependency on an SQL engine like MySQL, such as\nMyDNS)</li>\n<li>Seamless integration to Windows lookups (nmblookup) via proxying functions\n(DNS to/from NMB)</li>\n</ul>","After a search i found out a promising  one :\r\nUse pdnsd, Main selling point :  /etc/hosts parsing","simple dns server for soho network search very simple dns server for small network should recursive amp caching can used proxy very simple administration parsing etc hosts would perfect raw dns zones like bind would little bit overkill quite lightweight aka dependency sql engine like mysql such mydns seamless integration windows lookups nmblookup via proxying functions dns from nmb","a:1:{s:3:\"tag\";a:1:{i:0;s:3:\"sql\";}}","1","0","1","0","2","0","0"
"406133","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","6848","2009-05-29 08:00:00","Europe/Paris","2009-05-29 07:45:46","2010-08-23 16:45:30","","post","wiki","2009/05/Synthetic-Style-for-Blog-Posts-:-Presentation-Style-Blogging","en","Synthetic Style for Blog Posts : Presentation Style Blogging","","","!!!! The good\r\n\r\n* With SMS, IM and now Twitter becoming more and more predominant : __ideas might be given as bullet points__ \r\n* A presentation is much more dense in meaning than a big blob of text, Less time is required to read the post and __be inspired__ by the content\r\n* Blogging is much less time consuming. As Jeff Artwood said [Quantity Always Trumps Quality|http://www.codinghorror.com/blog/archives/001160.html|en], it is __the commitment__ that __is important__ (hence the schedule).\r\n* You might divide a long article in several short and related posts.\r\n\r\n!!!! The bad\r\n\r\n* __Blog__ articles are usually a __medium-depth analysis__ of a problem whereas presentation are usually a written support of a more detailed oral presentation.\r\n* Presentation sentences are usually hard to understand since they are just headlines without the underlying context.\r\n* You can always do a ''fast-reading'' version by putting the __important sentences in bold__ for all those hasty readers.\r\n\r\n!!!! The ugly\r\n\r\n* __Longer don't always mean more interesting__. [Voltaire|http://en.wikipedia.org/wiki/Voltaire|en] said {{Perfection is attained by slow degrees; it requires the hand of time.}} and [Antoine de Saint-Exupery|http://en.wikipedia.org/wiki/Antoine_de_Saint-Exupery|en] completed {{Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.}}.\r\n\r\n!!!! Conclusion\r\n\r\nAs concluded by [Pascal|http://en.wikipedia.org/wiki/Blaise_Pascal|en], {{I made this letter longer than usual because I lack the time to make it shorter}}, a __good and terse__ article is therefore much __more difficult__ to achieve. Therefore ''Presentation Style Blogging'' may be a false good idea.","<h2>The good</h2>\n<ul>\n<li>With SMS, IM and now Twitter becoming more and more predominant :\n<strong>ideas might be given as bullet points</strong></li>\n<li>A presentation is much more dense in meaning than a big blob of text, Less\ntime is required to read the post and <strong>be inspired</strong> by the\ncontent</li>\n<li>Blogging is much less time consuming. As Jeff Artwood said <a href=\"http://www.codinghorror.com/blog/archives/001160.html\" hreflang=\"en\">Quantity\nAlways Trumps Quality</a>, it is <strong>the commitment</strong> that\n<strong>is important</strong> (hence the schedule).</li>\n<li>You might divide a long article in several short and related posts.</li>\n</ul>\n<h2>The bad</h2>\n<ul>\n<li><strong>Blog</strong> articles are usually a <strong>medium-depth\nanalysis</strong> of a problem whereas presentation are usually a written\nsupport of a more detailed oral presentation.</li>\n<li>Presentation sentences are usually hard to understand since they are just\nheadlines without the underlying context.</li>\n<li>You can always do a <em>fast-reading</em> version by putting the\n<strong>important sentences in bold</strong> for all those hasty readers.</li>\n</ul>\n<h2>The ugly</h2>\n<ul>\n<li><strong>Longer don't always mean more interesting</strong>. <a href=\"http://en.wikipedia.org/wiki/Voltaire\" hreflang=\"en\">Voltaire</a> said\n<q>Perfection is attained by slow degrees; it requires the hand of time.</q>\nand <a href=\"http://en.wikipedia.org/wiki/Antoine_de_Saint-Exupery\" hreflang=\"en\">Antoine de Saint-Exupery</a> completed <q>Perfection is achieved, not when\nthere is nothing more to add, but when there is nothing left to take\naway.</q>.</li>\n</ul>\n<h2>Conclusion</h2>\n<p>As concluded by <a href=\"http://en.wikipedia.org/wiki/Blaise_Pascal\" hreflang=\"en\">Pascal</a>, <q>I made this letter longer than usual because I\nlack the time to make it shorter</q>, a <strong>good and terse</strong> article\nis therefore much <strong>more difficult</strong> to achieve. Therefore\n<em>Presentation Style Blogging</em> may be a false good idea.</p>","","synthetic style for blog posts presentation style blogging the good with sms and now twitter becoming more and more predominant ideas might given bullet points presentation much more dense meaning than big blob text less time required read the post and inspired the content blogging much less time consuming jeff artwood said quantity always trumps quality the commitment that important hence the schedule you might divide long article several short and related posts the bad blog articles are usually medium depth analysis problem whereas presentation are usually written support more detailed oral presentation presentation sentences are usually hard understand since they are just headlines without the underlying context you can always fast reading version putting the important sentences bold for all those hasty readers the ugly longer don always mean more interesting voltaire said perfection attained slow degrees requires the hand time and antoine saint exupery completed perfection achieved not when there nothing more add but when there nothing left take away conclusion concluded pascal made this letter longer than usual because lack the time make shorter good and terse article therefore much more difficult achieve therefore presentation style blogging may false good idea","","1","0","1","0","0","0","0"
"467153","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38367","2009-12-11 23:00:00","Europe/Paris","2009-12-11 11:52:33","2009-12-12 10:44:17","","post","wiki","2009/12/Avoid-the-Preprocessor-Use-Compile-Time-Polymorphism-for-Cross-platform-Development","en","Avoid the Preprocessor : Use ''Compile-Time Polymorphism'' for Cross-platform Development","When writing portable cross-platform code, don't litter your code with preprocessor macros, use ''compile-time polymorphism'' instead.\r\n\r\nA flexible build system will enable you to use advanced OOP-like compile-time polymorphism. That way you can hide all the specifics of the different platform behind an interface firewall. It is the usual way that most cross-platform toolkits and frameworks (such as QT, GTK or wxWidgets) are designed.","<p>When writing portable cross-platform code, don't litter your code with\npreprocessor macros, use <em>compile-time polymorphism</em> instead.</p>\n<p>A flexible build system will enable you to use advanced OOP-like\ncompile-time polymorphism. That way you can hide all the specifics of the\ndifferent platform behind an interface firewall. It is the usual way that most\ncross-platform toolkits and frameworks (such as QT, GTK or wxWidgets) are\ndesigned.</p>","We'll take a very simple file I/O subsystem here as an example (@@open@@, @@close@@). We'll also only take Linux and Windows, since that's usually the 2 most common platforms people want to develop for$$actually when targeting Linux, you usually target all Unix-like systems since they already have POSIX as a common abstraction$$.\r\n\r\n!!!! The usual abstraction with the preprocessor\r\n\r\nUsually preprocessor @@#ifdef@@s are used to compile specific parts for the underlying OS. \r\n\r\nThe file @@fileiosys.cpp@@ looks$$The code is not real, it has been sweetened$$ like the one below :\r\n\r\n///\r\nfileiosys_filedesc open(char* filename) {\r\n#ifdefine __WIN32__\r\n    return OpenFile(\r\n        filename, \r\n        GENERIC_READ | GENERIC_WRITE, \r\n        FILE_SHARE_READ | FILE_SHARE_WRITE, \r\n        NULL,\r\n    );  \r\n#else // UNIX\r\n    return open(filename, O_READ | O_WRITE, 0666);\r\n#endif\r\n}\r\n\r\nint close (fileiosys_filedesc fd) {\r\n#ifdefine __WIN32__\r\n    return CloseHandle(fd) ? 0 : -1;\r\n#else // UNIX\r\n    return close(fd);\r\n#endif\r\n}\r\n///\r\n\r\nThe header @@fileiosys.h@@ looks then like : \r\n\r\n///\r\n// Define a custom file descriptor\r\n#ifdefine __WIN32__\r\n    #define fileiosys_filedesc HANDLE\r\n#else // UNIX\r\n    #define fileiosys_filedesc int\r\n#endif\r\n\r\nfileiosys_filedesc open(char* filename); \r\nint close(fileiosys_filedesc fd); \r\n///\r\n\r\n!!!! Avoid the preprocessor Compile-time polymorphism\r\n\r\nAs the previous little example, the code isn't really easy to read. You have to always think which environnement you are in. All the specifics are multiplexed in the same file, and the programmer has to always demultiplex it in real time each time he reads the code. \r\n\r\nThe last part of the interface file is quite simple to read since it's already abstracted away. Now let's completely demultiplex the implementation in several implementation files.\r\n\r\n!!! The header file is the common interface\r\n\r\nThe interface is the most important part of the design. It should be high-level enough to mask the differences between the plateforms you want to support, but not too high-level, otherwise you'll end up duplicating to much code. \r\n\r\nSo, for our file I/O subsystem, we'll just abstract the usual syscalls @@open@@, @@close@@ in the same way as before.  \r\n\r\nThe parameters that are passed through the interface are also very important. You cannot usually leak a platform-specific structure. So here all the file descriptors are just an opaque handle, represented by a pointer to a structure defined only as a forward declaration in the header. This pattern, sometimes called the [pimpl idiom|http://en.wikipedia.org/wiki/Opaque_pointer|en], enables use to really share the representation while implementing it differently.\r\n\r\nThe header file is preserved as @@fileiosys.h@@, whereas the implementation is in the files @@linux/fileiosys.cpp@@ and @@win32/fileiosys.cpp@@   \r\n\r\n!! @@fileiosys.h@@\r\n\r\n///\r\n// Define the forward declaration\r\nstruct fis_filedesc;\r\nfis_filedesc* open(char* filename); \r\nint close(fileiosys_filedesc* fd); \r\n///\r\n\r\n!! @@win32/fileiosys.cpp@@\r\n///\r\nstruct fis_filedesc {\r\n    HANDLE handle;\r\n};\r\n\r\nfis_filedesc* open(char* filename) {\r\n    fis_filedesc* fd = new fis_filedesc();\r\n    fd->handle = OpenFile(\r\n        filename, \r\n        GENERIC_READ | GENERIC_WRITE, \r\n        FILE_SHARE_READ | FILE_SHARE_WRITE, \r\n        NULL,\r\n    );  \r\n\r\n    return fd;\r\n}\r\n\r\nint close (fis_filedesc* fd) {\r\n    int retval = CloseHandle(fd->handle) ? 0 : -1;\r\n    delete(fd);\r\n    return retval;\r\n}\r\n///\r\n\r\n\r\n!! @@linux/fileiosys.cpp@@\r\n///\r\nstruct fis_filedesc {\r\n    int file_descriptor;\r\n};\r\n\r\nstatic fis_filedesc[32];\r\n\r\nfis_filedesc* open(char* filename) {\r\n    fis_filedesc* fd = new fis_filedesc();\r\n    fd->file_descriptor = open(filename, O_READ | O_WRITE, 0666);\r\n    return fd;\r\n}\r\n\r\nint close (fis_filedesc* fd) {\r\n    int retval = close(fd);\r\n    delete(fd);\r\n    return retval;\r\n}\r\n///\r\n\r\n!! The build system : @@Makefile@@\r\n\r\nThe makefile should take into account the different platforms, and only compile the needed implementation file. All the gluing magic will then be done at linking time instead of preprocessor time.\r\n\r\n!!!! Conclusion\r\n\r\nThe interest of have multiple implementation files is obvious. It is __much__ more straightforward to read and only marginally harder to write. But since most of the time code is read and not written, the choose is quite a no-brainer.\r\n\r\nThe nicest part is that all this is possible even without the expensive run-time polymorphism and RTTI, since the choose is done at compile-time.","<p>We'll take a very simple file I/O subsystem here as an example\n(<code>open</code>, <code>close</code>). We'll also only take Linux and\nWindows, since that's usually the 2 most common platforms people want to\ndevelop for<sup>[<a href=\"#pnote-467153-1\" id=\"rev-pnote-467153-1\" name=\"rev-pnote-467153-1\">1</a>]</sup>.</p>\n<h2>The usual abstraction with the preprocessor</h2>\n<p>Usually preprocessor <code>#ifdef</code>s are used to compile specific parts\nfor the underlying OS.</p>\n<p>The file <code>fileiosys.cpp</code> looks<sup>[<a href=\"#pnote-467153-2\" id=\"rev-pnote-467153-2\" name=\"rev-pnote-467153-2\">2</a>]</sup> like the one below\n:</p>\n<pre>\nfileiosys_filedesc open(char* filename) {\n#ifdefine __WIN32__\n    return OpenFile(\n        filename, \n        GENERIC_READ | GENERIC_WRITE, \n        FILE_SHARE_READ | FILE_SHARE_WRITE, \n        NULL,\n    );  \n#else // UNIX\n    return open(filename, O_READ | O_WRITE, 0666);\n#endif\n}\n\nint close (fileiosys_filedesc fd) {\n#ifdefine __WIN32__\n    return CloseHandle(fd) ? 0 : -1;\n#else // UNIX\n    return close(fd);\n#endif\n}\n</pre>\n<p>The header <code>fileiosys.h</code> looks then like :</p>\n<pre>\n// Define a custom file descriptor\n#ifdefine __WIN32__\n    #define fileiosys_filedesc HANDLE\n#else // UNIX\n    #define fileiosys_filedesc int\n#endif\n\nfileiosys_filedesc open(char* filename); \nint close(fileiosys_filedesc fd); \n</pre>\n<h2>Avoid the preprocessor Compile-time polymorphism</h2>\n<p>As the previous little example, the code isn't really easy to read. You have\nto always think which environnement you are in. All the specifics are\nmultiplexed in the same file, and the programmer has to always demultiplex it\nin real time each time he reads the code.</p>\n<p>The last part of the interface file is quite simple to read since it's\nalready abstracted away. Now let's completely demultiplex the implementation in\nseveral implementation files.</p>\n<h3>The header file is the common interface</h3>\n<p>The interface is the most important part of the design. It should be\nhigh-level enough to mask the differences between the plateforms you want to\nsupport, but not too high-level, otherwise you'll end up duplicating to much\ncode.</p>\n<p>So, for our file I/O subsystem, we'll just abstract the usual syscalls\n<code>open</code>, <code>close</code> in the same way as before.</p>\n<p>The parameters that are passed through the interface are also very\nimportant. You cannot usually leak a platform-specific structure. So here all\nthe file descriptors are just an opaque handle, represented by a pointer to a\nstructure defined only as a forward declaration in the header. This pattern,\nsometimes called the <a href=\"http://en.wikipedia.org/wiki/Opaque_pointer\" hreflang=\"en\">pimpl idiom</a>, enables use to really share the representation\nwhile implementing it differently.</p>\n<p>The header file is preserved as <code>fileiosys.h</code>, whereas the\nimplementation is in the files <code>linux/fileiosys.cpp</code> and\n<code>win32/fileiosys.cpp</code></p>\n<h4><code>fileiosys.h</code></h4>\n<pre>\n// Define the forward declaration\nstruct fis_filedesc;\nfis_filedesc* open(char* filename); \nint close(fileiosys_filedesc* fd); \n</pre>\n<h4><code>win32/fileiosys.cpp</code></h4>\n<pre>\nstruct fis_filedesc {\n    HANDLE handle;\n};\n\nfis_filedesc* open(char* filename) {\n    fis_filedesc* fd = new fis_filedesc();\n    fd-&gt;handle = OpenFile(\n        filename, \n        GENERIC_READ | GENERIC_WRITE, \n        FILE_SHARE_READ | FILE_SHARE_WRITE, \n        NULL,\n    );  \n\n    return fd;\n}\n\nint close (fis_filedesc* fd) {\n    int retval = CloseHandle(fd-&gt;handle) ? 0 : -1;\n    delete(fd);\n    return retval;\n}\n</pre>\n<h4><code>linux/fileiosys.cpp</code></h4>\n<pre>\nstruct fis_filedesc {\n    int file_descriptor;\n};\n\nstatic fis_filedesc[32];\n\nfis_filedesc* open(char* filename) {\n    fis_filedesc* fd = new fis_filedesc();\n    fd-&gt;file_descriptor = open(filename, O_READ | O_WRITE, 0666);\n    return fd;\n}\n\nint close (fis_filedesc* fd) {\n    int retval = close(fd);\n    delete(fd);\n    return retval;\n}\n</pre>\n<h4>The build system : <code>Makefile</code></h4>\n<p>The makefile should take into account the different platforms, and only\ncompile the needed implementation file. All the gluing magic will then be done\nat linking time instead of preprocessor time.</p>\n<h2>Conclusion</h2>\n<p>The interest of have multiple implementation files is obvious. It is\n<strong>much</strong> more straightforward to read and only marginally harder\nto write. But since most of the time code is read and not written, the choose\nis quite a no-brainer.</p>\n<p>The nicest part is that all this is possible even without the expensive\nrun-time polymorphism and RTTI, since the choose is done at compile-time.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-467153-1\" id=\"pnote-467153-1\" name=\"pnote-467153-1\">1</a>] actually when targeting Linux, you usually target all\nUnix-like systems since they already have POSIX as a common abstraction</p>\n<p>[<a href=\"#rev-pnote-467153-2\" id=\"pnote-467153-2\" name=\"pnote-467153-2\">2</a>] The code is not real, it has been sweetened</p>\n</div>","","avoid the preprocessor use compile time polymorphism for cross platform development when writing portable cross platform code don litter your code with preprocessor macros use compile time polymorphism instead flexible build system will enable you use advanced oop like compile time polymorphism that way you can hide all the specifics the different platform behind interface firewall the usual way that most cross platform toolkits and frameworks such gtk wxwidgets are designed take very simple file subsystem here example open close also only take linux and windows since that usually the most common platforms people want develop for the usual abstraction with the preprocessor usually preprocessor ifdefs are used compile specific parts for the underlying the file fileiosys cpp looks like the one below fileiosys filedesc open char filename ifdefine win32 return openfile filename generic read generic write file share read file share write null else unix return open filename read write 0666 endif int close fileiosys filedesc ifdefine win32 return closehandle else unix return close endif the header fileiosys looks then like define custom file descriptor ifdefine win32 define fileiosys filedesc handle else unix define fileiosys filedesc int endif fileiosys filedesc open char filename int close fileiosys filedesc avoid the preprocessor compile time polymorphism the previous little example the code isn really easy read you have always think which environnement you are all the specifics are multiplexed the same file and the programmer has always demultiplex real time each time reads the code the last part the interface file quite simple read since already abstracted away now let completely demultiplex the implementation several implementation files the header file the common interface the interface the most important part the design should high level enough mask the differences between the plateforms you want support but not too high level otherwise you end duplicating much code for our file subsystem just abstract the usual syscalls open close the same way before the parameters that are passed through the interface are also very important you cannot usually leak platform specific structure here all the file descriptors are just opaque handle represented pointer structure defined only forward declaration the header this pattern sometimes called the pimpl idiom enables use really share the representation while implementing differently the header file preserved fileiosys whereas the implementation the files linux fileiosys cpp and win32 fileiosys cpp fileiosys define the forward declaration struct fis filedesc fis filedesc open char filename int close fileiosys filedesc win32 fileiosys cpp struct fis filedesc handle handle fis filedesc open char filename fis filedesc new fis filedesc handle openfile filename generic read generic write file share read file share write null return int close fis filedesc int retval closehandle handle delete return retval linux fileiosys cpp struct fis filedesc int file descriptor static fis filedesc fis filedesc open char filename fis filedesc new fis filedesc file descriptor open filename read write 0666 return int close fis filedesc int retval close delete return retval the build system makefile the makefile should take into account the different platforms and only compile the needed implementation file all the gluing magic will then done linking time instead preprocessor time conclusion the interest have multiple implementation files obvious much more straightforward read and only marginally harder write but since most the time code read and not written the choose quite brainer the nicest part that all this possible even without the expensive run time polymorphism and rtti since the choose done compile time notes actually when targeting linux you usually target all unix like systems since they already have posix common abstraction the code not real has been sweetened","","1","0","1","0","0","0","0"
"406940","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","12386","2009-06-01 17:38:00","Europe/Paris","2009-06-01 15:38:29","2009-06-01 16:02:32","","post","wiki","2009/06/Equality-in-Java-is-a-Hot-Topic-but-a-Hazardous-one","en","Equality in Java is a Hot Topic, but a Hazardous one.","","","It seems that comparing two objects isn't as a simple task to do as it seems at first. \r\n\r\nMy concept for [smart comparison|/post/2009/05/Compare-Efficiently-in-Java-%3A-Embrace-Smart-Comparison] still holds, but is completed perfectly with the [How to Write an Equality Method in Java|http://www.artima.com/lejava/articles/equality.html|en] article posted on Artima soon after mine$$But as much as I wished it to be, it is not related :-)$$.\r\n\r\n2 main points that you have to be careful to (Others are also detailed in this article) :\r\n* @@hasCode()@@ has to be redefined usually since otherwise the [equality contract|http://www.geocities.com/technofundo/tech/java/equalhash.html|en] on @@Object.hascode()@@ would be broken.\r\n* the @@equals()@@ has to take an Object as parameter since overloading in Java is resolved by the compile-time type of the argument, not the run-time type.\r\n\r\nI just got bitten by the second one, but not much (yet) since I mostly rely on compile-time overloading. My purpose here was only to compare objects either to constants or to simple variables. But in the generalization I would certainly have overlooked this and be bitten much more deeply.","<p>It seems that comparing two objects isn't as a simple task to do as it seems\nat first.</p>\n<p>My concept for <a href=\"/post/2009/05/Compare-Efficiently-in-Java-%3A-Embrace-Smart-Comparison\">smart\ncomparison</a> still holds, but is completed perfectly with the <a href=\"http://www.artima.com/lejava/articles/equality.html\" hreflang=\"en\">How to\nWrite an Equality Method in Java</a> article posted on Artima soon after\nmine<sup>[<a href=\"#pnote-406940-1\" id=\"rev-pnote-406940-1\" name=\"rev-pnote-406940-1\">1</a>]</sup>.</p>\n<p>2 main points that you have to be careful to (Others are also detailed in\nthis article) :</p>\n<ul>\n<li><code>hasCode()</code> has to be redefined usually since otherwise the\n<a href=\"http://www.geocities.com/technofundo/tech/java/equalhash.html\" hreflang=\"en\">equality contract</a> on <code>Object.hascode()</code> would be\nbroken.</li>\n<li>the <code>equals()</code> has to take an Object as parameter since\noverloading in Java is resolved by the compile-time type of the argument, not\nthe run-time type.</li>\n</ul>\n<p>I just got bitten by the second one, but not much (yet) since I mostly rely\non compile-time overloading. My purpose here was only to compare objects either\nto constants or to simple variables. But in the generalization I would\ncertainly have overlooked this and be bitten much more deeply.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-406940-1\" id=\"pnote-406940-1\" name=\"pnote-406940-1\">1</a>] But as much as I wished it to be, it is not related\n:-)</p>\n</div>","","equality java hot topic but hazardous one seems that comparing two objects isn simple task seems first concept for smart comparison still holds but completed perfectly with the how write equality method java article posted artima soon after mine main points that you have careful others are also detailed this article hascode has redefined usually since otherwise the equality contract object hascode would broken the equals has take object parameter since overloading java resolved the compile time type the argument not the run time type just got bitten the second one but not much yet since mostly rely compile time overloading purpose here was only compare objects either constants simple variables but the generalization would certainly have overlooked this and bitten much more deeply notes but much wished not related","","1","0","1","0","0","0","0"
"402145","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","33233","2009-06-02 08:00:00","Europe/Paris","2009-05-14 15:35:15","2009-07-16 12:52:55","","post","wiki","2009/05/Efficient-Denormalization-with-Views","en","Databases: Efficient Denormalization with Views","Everyone is unanimous that __database normalization__ is considered a __Good Thing__. \r\n\r\nBut it usually comes with a cost : __writing queries can be very tedious__ since you always have to join many tables together to be able to retrieve useful human data from all those reference tables.","<p>Everyone is unanimous that <strong>database normalization</strong> is\nconsidered a <strong>Good Thing</strong>.</p>\n<p>But it usually comes with a cost : <strong>writing queries can be very\ntedious</strong> since you always have to join many tables together to be able\nto retrieve useful human data from all those reference tables.</p>","On the other side, to __denormalize__ is sometimes seen as a way to : \r\n* __optimize development__ : you do not need to write (and debug) complex queries since all the data is nicely located in the same table\r\n* __optimize performance__ : the data has a better locality (no need to fetch or compute data from elsewhere). You can even pre-compute [order totals|http://database-programmer.blogspot.com/2008/11/keeping-denormalized-values-correct.html|en]$$That article also explains why __denormalization maintenance must stay at the database level__ with a very interesting metaphor$$.\r\n\r\nDenormalize correctly is quite difficult since __one change__ snowballs in __multiple updates__ to keep the redundant data coherent. It is therefore usually done on the __application side__ with ready-to-use ORM frameworks. But managing it at the application level comes with an ugly cost : it has to be __reimplemented over and over__ each time the application switches technology, version or when a new application is connecting to the database.\r\n\r\nIn almost every programming language, code reuse is encouraged. Here, at the database level, we can also apply the same principles, and use views to transparently present a denormalized API (read ''tables'') on a more normalized schema.\r\n\r\nThe main points are :\r\n\r\n* Views are the main code reuse vector at the database level. Current database usually correctly optimize (by rewrite and recombination) the simple queries so that performance is on par with a hand crafted query that doesn't use the views\r\n* Sometime even faster since the data is nicely organized, so there is much less data to transfer, and I/O bandwidth is a usual suspect.\r\n* The application part doesn't even need to know that normalization happens under-hood since [updates to views|http://www.ibm.com/developerworks/db2/library/techarticle/0210rielau/0210rielau.html|en] are possible in many modern RDMs with the [@@INSTEAD OF@@ trigger|http://publib.boulder.ibm.com/infocenter/ids9help/topic/com.ibm.sqlt.doc/sqltmst333.htm|en] (or something [equivalent|http://www.postgresql.org/docs/current/static/rules-update.html|en]).\r\n* Theses updates can be created with [Materialized views|http://en.wikipedia.org/wiki/Materialized_view|en] are a step even further on the denormalizing road, since it provides the common benefits of denormalize without the implementation caveats. We can even hand-craft these ''Materialized views'' directly in an aggregation table in order to have the space benefits of normalisation and the performance benefits of denormalization.\r\n\r\nSo, nothing stops you from normalizing at will, and denormalizing when needed. Where to put the cursor is yours to decide, but moving it afterwards is finally easier that what is commonly admited.","<p>On the other side, to <strong>denormalize</strong> is sometimes seen as a\nway to :</p>\n<ul>\n<li><strong>optimize development</strong> : you do not need to write (and\ndebug) complex queries since all the data is nicely located in the same\ntable</li>\n<li><strong>optimize performance</strong> : the data has a better locality (no\nneed to fetch or compute data from elsewhere). You can even pre-compute\n<a href=\"http://database-programmer.blogspot.com/2008/11/keeping-denormalized-values-correct.html\" hreflang=\"en\">order totals</a><sup>[<a href=\"#pnote-402145-1\" id=\"rev-pnote-402145-1\" name=\"rev-pnote-402145-1\">1</a>]</sup>.</li>\n</ul>\n<p>Denormalize correctly is quite difficult since <strong>one change</strong>\nsnowballs in <strong>multiple updates</strong> to keep the redundant data\ncoherent. It is therefore usually done on the <strong>application side</strong>\nwith ready-to-use ORM frameworks. But managing it at the application level\ncomes with an ugly cost : it has to be <strong>reimplemented over and\nover</strong> each time the application switches technology, version or when a\nnew application is connecting to the database.</p>\n<p>In almost every programming language, code reuse is encouraged. Here, at the\ndatabase level, we can also apply the same principles, and use views to\ntransparently present a denormalized API (read <em>tables</em>) on a more\nnormalized schema.</p>\n<p>The main points are :</p>\n<ul>\n<li>Views are the main code reuse vector at the database level. Current\ndatabase usually correctly optimize (by rewrite and recombination) the simple\nqueries so that performance is on par with a hand crafted query that doesn't\nuse the views</li>\n<li>Sometime even faster since the data is nicely organized, so there is much\nless data to transfer, and I/O bandwidth is a usual suspect.</li>\n<li>The application part doesn't even need to know that normalization happens\nunder-hood since <a href=\"http://www.ibm.com/developerworks/db2/library/techarticle/0210rielau/0210rielau.html\" hreflang=\"en\">updates to views</a> are possible in many modern RDMs with the\n<a href=\"http://publib.boulder.ibm.com/infocenter/ids9help/topic/com.ibm.sqlt.doc/sqltmst333.htm\" hreflang=\"en\">@@INSTEAD OF@@ trigger</a> (or something <a href=\"http://www.postgresql.org/docs/current/static/rules-update.html\" hreflang=\"en\">equivalent</a>).</li>\n<li>Theses updates can be created with <a href=\"http://en.wikipedia.org/wiki/Materialized_view\" hreflang=\"en\">Materialized\nviews</a> are a step even further on the denormalizing road, since it provides\nthe common benefits of denormalize without the implementation caveats. We can\neven hand-craft these <em>Materialized views</em> directly in an aggregation\ntable in order to have the space benefits of normalisation and the performance\nbenefits of denormalization.</li>\n</ul>\n<p>So, nothing stops you from normalizing at will, and denormalizing when\nneeded. Where to put the cursor is yours to decide, but moving it afterwards is\nfinally easier that what is commonly admited.</p>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-402145-1\" id=\"pnote-402145-1\" name=\"pnote-402145-1\">1</a>] That article also explains why <strong>denormalization\nmaintenance must stay at the database level</strong> with a very interesting\nmetaphor</p>\n</div>","","databases efficient denormalization with views everyone unanimous that database normalization considered good thing but usually comes with cost writing queries can very tedious since you always have join many tables together able retrieve useful human data from all those reference tables the other side denormalize sometimes seen way optimize development you not need write and debug complex queries since all the data nicely located the same table optimize performance the data has better locality need fetch compute data from elsewhere you can even pre compute order totals denormalize correctly quite difficult since one change snowballs multiple updates keep the redundant data coherent therefore usually done the application side with ready use orm frameworks but managing the application level comes with ugly cost has reimplemented over and over each time the application switches technology version when new application connecting the database almost every programming language code reuse encouraged here the database level can also apply the same principles and use views transparently present denormalized api read tables more normalized schema the main points are views are the main code reuse vector the database level current database usually correctly optimize rewrite and recombination the simple queries that performance par with hand crafted query that doesn use the views sometime even faster since the data nicely organized there much less data transfer and bandwidth usual suspect the application part doesn even need know that normalization happens under hood since updates views are possible many modern rdms with the instead trigger something equivalent theses updates can created with materialized views are step even further the denormalizing road since provides the common benefits denormalize without the implementation caveats can even hand craft these materialized views directly aggregation table order have the space benefits normalisation and the performance benefits denormalization nothing stops you from normalizing will and denormalizing when needed where put the cursor yours decide but moving afterwards finally easier that what commonly admited notes that article also explains why denormalization maintenance must stay the database level with very interesting metaphor","a:0:{}","1","1","1","0","4","0","0"
"462735","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2009-11-26 22:00:00","Europe/Paris","2009-11-27 00:19:04","2010-08-22 14:22:30","","post","wiki","2009/11/Native-SSH-transport-for-Munin","en","Native SSH transport for Munin","///html\r\n<p style=\"color:red\">\r\n<b>Update</b>: This page is obsolete, since it has been merged in the <a hred=\"/post/2010/07/Waiting-for-Munin-2.0-Native-SSH-transport\">2.0 version of Munin</a>. The syntax is a little bit different, but the idea is the same.\r\n</p>\r\n///\r\n\r\nI [blogged|/post/2008/11/04/A-Poor-Man-s-Munin-Node-to-Monitor-Hostile-UNIX-Servers] about the [munin monitoring system|http://munin.projects.linpro.no/|en] a while ago. \r\n\r\nThe fact the Munin team did quite a remarkable job in cleaning up the 1.2 code for the 1.4 release enabled me to add a native SSH transport for Munin, and made be able to get rid of all SSH tunnels.","<p style=\"color:red\"><b>Update</b>: This page is obsolete, since it has been\nmerged in the <a>2.0 version of Munin</a>. The syntax is a little bit\ndifferent, but the idea is the same.</p>\n<p>I <a href=\"/post/2008/11/04/A-Poor-Man-s-Munin-Node-to-Monitor-Hostile-UNIX-Servers\">blogged</a>\nabout the <a href=\"http://munin.projects.linpro.no/\" hreflang=\"en\">munin\nmonitoring system</a> a while ago.</p>\n<p>The fact the Munin team did quite a remarkable job in cleaning up the 1.2\ncode for the 1.4 release enabled me to add a native SSH transport for Munin,\nand made be able to get rid of all SSH tunnels.</p>","Actually the tunnel won't disappear, but they will be launched only when needed and, most importantly configured in @@munin.conf@@.\r\n\r\nThe [native ssh for munin patch|/public/direct_ssh_transport.diff|en] should applies quite cleanly on revision 3101 of the svn trunk.\r\n\r\nIts use is quite straightforward : in @@/etc/munin/munin.conf@@, you just migrate address to the new configuration directive @@remote_connection_cmd@@ that take the whole @@ssh@@ command to launch a stdio munin-node such as pmmn. \r\n\r\nIf we take the examples from the previous post, it becomes clear it's __much__ easier to configure.\r\n\r\n!!!munin.conf snippet - Inetd version\r\n///\r\n[server1]\r\n    address localhost\r\n    port 7001\r\n[server2]\r\n    address localhost\r\n    port 7002\r\n///\r\n\r\n!!!munin.conf snippet - Native SSH transport version\r\n\r\nRight now, the @@address@@ directive is still mandatory, but ignored when connecting.\r\n\r\n///\r\n[server1]\r\n    address dummy\r\n    remote_connection_cmd /usr/bin/ssh -- supusr@server1 /home/suprusr/pmmn/pmmn.pl\r\n[server2]\r\n    address dummy\r\n    remote_connection_cmd /usr/bin/ssh -- supusr@server2 /home/suprusr/pmmn/pmmn.pl\r\n///\r\n\r\n!!!Warning\r\n\r\nBeware that the @@ssh@@ process will now be launched by the @@munin@@ user, so you have to update the key-based SSH authentication accordingly.","<p>Actually the tunnel won't disappear, but they will be launched only when\nneeded and, most importantly configured in <code>munin.conf</code>.</p>\n<p>The <a href=\"/public/direct_ssh_transport.diff\" hreflang=\"en\">native ssh for\nmunin patch</a> should applies quite cleanly on revision 3101 of the svn\ntrunk.</p>\n<p>Its use is quite straightforward : in <code>/etc/munin/munin.conf</code>,\nyou just migrate address to the new configuration directive\n<code>remote_connection_cmd</code> that take the whole <code>ssh</code> command\nto launch a stdio munin-node such as pmmn.</p>\n<p>If we take the examples from the previous post, it becomes clear it's\n<strong>much</strong> easier to configure.</p>\n<h3>munin.conf snippet - Inetd version</h3>\n<pre>\n[server1]\n    address localhost\n    port 7001\n[server2]\n    address localhost\n    port 7002\n</pre>\n<h3>munin.conf snippet - Native SSH transport version</h3>\n<p>Right now, the <code>address</code> directive is still mandatory, but\nignored when connecting.</p>\n<pre>\n[server1]\n    address dummy\n    remote_connection_cmd /usr/bin/ssh -- supusr@server1 /home/suprusr/pmmn/pmmn.pl\n[server2]\n    address dummy\n    remote_connection_cmd /usr/bin/ssh -- supusr@server2 /home/suprusr/pmmn/pmmn.pl\n</pre>\n<h3>Warning</h3>\n<p>Beware that the <code>ssh</code> process will now be launched by the\n<code>munin</code> user, so you have to update the key-based SSH authentication\naccordingly.</p>","","native ssh transport for munin update this page obsolete since has been merged the version munin the syntax little bit different but the idea the same blogged about the munin monitoring system while ago the fact the munin team did quite remarkable job cleaning the code for the release enabled add native ssh transport for munin and made able get rid all ssh tunnels actually the tunnel won disappear but they will launched only when needed and most importantly configured munin conf the native ssh for munin patch should applies quite cleanly revision 3101 the svn trunk its use quite straightforward etc munin munin conf you just migrate address the new configuration directive remote connection cmd that take the whole ssh command launch stdio munin node such pmmn take the examples from the previous post becomes clear much easier configure munin conf snippet inetd version server1 address localhost port 7001 server2 address localhost port 7002 munin conf snippet native ssh transport version right now the address directive still mandatory but ignored when connecting server1 address dummy remote connection cmd usr bin ssh supusr server1 home suprusr pmmn pmmn server2 address dummy remote connection cmd usr bin ssh supusr server2 home suprusr pmmn pmmn warning beware that the ssh process will now launched the munin user you have update the key based ssh authentication accordingly","a:1:{s:3:\"tag\";a:2:{i:0;s:3:\"ssh\";i:1;s:5:\"munin\";}}","1","0","1","0","0","0","0"
"525792","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2010-06-16 23:25:00","Europe/Paris","2010-06-09 21:00:54","2010-06-22 08:26:59","","post","wiki","2010/06/Waiting-for-Munin-2.0-Performance-FastCGI","en","Waiting for Munin 2.0 - Performance - FastCGI","","","1.2 has CGI, it is slow, unsupported, but [it does exist|http://munin-monitoring.org/wiki/CgiHowto|en].\r\n\r\n1.4 has even an[ experimental FastCGI|http://munin-monitoring.org/wiki/CgiHowto#FastCGI|en] install mode.\r\n\r\nQuoting from this page : \r\n\r\n> This is more a proof of concept than a recommended - it's slow. \r\n> Also we do not test it before every release\r\n\r\nIn 2.0 lots of work has been done to take this experimental CGI mode into a supported one. It might even be the primary way of using munin since, when an install has a certain size, CGI becomes mandatory. \r\n\r\nThat's because @@munin-graph@@ doesn't have time to finish its job when the next one is launched, and the new one doesn't run. It is not as dramatic as a missed @@munin-update@@ execution, since the graphs will still be generated on the later round, but there will be random graph lags and it will put quite some stress on the CPU & I/O subsystem. This will slow @@munin-update@@ down since it also uses the I/O subsystem much, and that's to be avoided at all costs. \r\n\r\nMainstream CGI has some consequences :\r\n# Only the FastCGI wrapper remained : the plain CGI one is dropped. \r\n#* The CPAN module @@CGI::Fast@@ is compatible when launched as a normal CGI. \r\n#* Almost all HTTP servers support plain CGI,  and with the [cgi-fcgi|http://www.fastcgi.com/devkit/doc/cgi-fcgi.1|en] wrapper from the FastCGI devkit (Debian package @@libfcgi@@), you can have the best of both worlds (a custom HTTP server & FastCGI). I even posted on how to have a working [thttpd with FastCGI|/post/2010/06/CGI-on-steroids-with-FastCGI%2C-but-on-a-CGI-only-server-The-FastCGI-wrapper].\r\n# The old process limit mechanism is dropped also. The FastCGI server configuration is a much better way to control it. The old code was based on System V semaphores and was not 100% reliable.\r\n# A caching system has to be implemented, in order for each graph to be generated only once for its lifetime.\r\n# The CGI process is launched with the HTTP server user. Since it doesn't only read now, but also writes log files and images files, there is an extra step when installing it. But it's already described in the [Munin CGI|http://munin-monitoring.org/wiki/CgiHowto|en] page given previously.\r\n# Since the process is launched only once, for now it read only once the config. So if some part of the config change, the FastCGI container MUST be restarted.\r\n\r\n!!!! Some benchmarks\r\n\r\nNow, the sweet part : I'm putting up some micro-benchmarks. \r\n\r\nThey should be taken with caution as every benchmark should be, but I think the general idea is conveyed. For the sake of simplicity I'm only doing 1 request in parallel and disabled IMS caching.\r\n\r\n!!! Basic 1.2 CGI \r\n\r\n///\r\n$ httperf --num-conns 10  --add-header='Cache-Control: no-cache\n' \\r\n    --uri  /cgi-bin/munin-cgi-graph/localdomain/localhost.localdomain/cpu-day.png\r\n\r\nTotal: connections 10 requests 10 replies 10 test-duration 27.939 s\r\n\r\nConnection rate: 0.4 conn/s (2793.9 ms/conn, <=1 concurrent connections)\r\nConnection time [ms]: min 1653.9 avg 2793.9 max 5217.0 median 1912.5 stddev 1487.8\r\nConnection time [ms]: connect 0.0\r\nConnection length [replies/conn]: 1.000\r\n\r\nRequest rate: 0.4 req/s (2793.9 ms/req)\r\nRequest size [B]: 131.0\r\n///\r\n\r\n!!! 1.4 FastCGI\r\n\r\nThe munin-fastcgi-graph is only loaded once, but the munin-graph is reloaded each time.\r\n\r\n///\r\n$ httperf --num-conns 10  --add-header='Cache-Control: no-cache\n' \\r\n    --uri  /cgi-bin/munin-fastcgi-graph/localdomain/localhost.localdomain/cpu-day.png\r\n\r\nTotal: connections 10 requests 10 replies 10 test-duration 13.807 s\r\n\r\nConnection rate: 0.7 conn/s (1380.7 ms/conn, <=1 concurrent connections)\r\nConnection time [ms]: min 1141.3 avg 1380.7 max 1636.1 median 1381.5 stddev 173.7\r\nConnection time [ms]: connect 0.0\r\nConnection length [replies/conn]: 1.000\r\n\r\nRequest rate: 0.7 req/s (1380.7 ms/req)\r\n///\r\n\r\nThe response time is cut almost in half. That's expected, since only the top half of the processing isn't reloaded.\r\n\r\n!!! 2.0 FastCGI\r\n\r\nHere everything is loaded once. \r\n\r\n///\r\n$ httperf --num-conns 10  --add-header='Cache-Control: no-cache\n' \\r\n    --uri  /cgi-bin/munin-cgi-graph-2.0/localdomain/localhost.localdomain/cpu-day.png\r\n\r\nTotal: connections 10 requests 10 replies 10 test-duration 1.668 s\r\n\r\nConnection rate: 6.0 conn/s (166.8 ms/conn, <=1 concurrent connections)\r\nConnection time [ms]: min 123.0 avg 166.8 max 513.4 median 127.5 stddev 121.9\r\nConnection time [ms]: connect 0.0\r\nConnection length [replies/conn]: 1.000\r\n\r\nRequest rate: 6.0 req/s (166.8 ms/req)\r\n///\r\n\r\nNow response time is cut almost by a ten factor ! That's quite good news, since it goes 20 times faster that the original CGI.","<p>1.2 has CGI, it is slow, unsupported, but <a href=\"http://munin-monitoring.org/wiki/CgiHowto\" hreflang=\"en\">it does\nexist</a>.</p>\n<p>1.4 has even an <a href=\"http://munin-monitoring.org/wiki/CgiHowto#FastCGI\" hreflang=\"en\">experimental FastCGI</a> install mode.</p>\n<p>Quoting from this page :</p>\n<blockquote>\n<p>This is more a proof of concept than a recommended - it's slow. Also we do\nnot test it before every release</p>\n</blockquote>\n<p>In 2.0 lots of work has been done to take this experimental CGI mode into a\nsupported one. It might even be the primary way of using munin since, when an\ninstall has a certain size, CGI becomes mandatory.</p>\n<p>That's because <code>munin-graph</code> doesn't have time to finish its job\nwhen the next one is launched, and the new one doesn't run. It is not as\ndramatic as a missed <code>munin-update</code> execution, since the graphs will\nstill be generated on the later round, but there will be random graph lags and\nit will put quite some stress on the CPU &amp; I/O subsystem. This will slow\n<code>munin-update</code> down since it also uses the I/O subsystem much, and\nthat's to be avoided at all costs.</p>\n<p>Mainstream CGI has some consequences :</p>\n<ol>\n<li>Only the FastCGI wrapper remained : the plain CGI one is dropped.\n<ul>\n<li>The CPAN module <code>CGI::Fast</code> is compatible when launched as a\nnormal CGI.</li>\n<li>Almost all HTTP servers support plain CGI, and with the <a href=\"http://www.fastcgi.com/devkit/doc/cgi-fcgi.1\" hreflang=\"en\">cgi-fcgi</a>\nwrapper from the FastCGI devkit (Debian package <code>libfcgi</code>), you can\nhave the best of both worlds (a custom HTTP server &amp; FastCGI). I even\nposted on how to have a working <a href=\"/post/2010/06/CGI-on-steroids-with-FastCGI%2C-but-on-a-CGI-only-server-The-FastCGI-wrapper\">\nthttpd with FastCGI</a>.</li>\n</ul>\n</li>\n<li>The old process limit mechanism is dropped also. The FastCGI server\nconfiguration is a much better way to control it. The old code was based on\nSystem V semaphores and was not 100% reliable.</li>\n<li>A caching system has to be implemented, in order for each graph to be\ngenerated only once for its lifetime.</li>\n<li>The CGI process is launched with the HTTP server user. Since it doesn't\nonly read now, but also writes log files and images files, there is an extra\nstep when installing it. But it's already described in the <a href=\"http://munin-monitoring.org/wiki/CgiHowto\" hreflang=\"en\">Munin CGI</a> page\ngiven previously.</li>\n<li>Since the process is launched only once, for now it read only once the\nconfig. So if some part of the config change, the FastCGI container MUST be\nrestarted.</li>\n</ol>\n<h2>Some benchmarks</h2>\n<p>Now, the sweet part : I'm putting up some micro-benchmarks.</p>\n<p>They should be taken with caution as every benchmark should be, but I think\nthe general idea is conveyed. For the sake of simplicity I'm only doing 1\nrequest in parallel and disabled IMS caching.</p>\n<h3>Basic 1.2 CGI</h3>\n<pre>\n$ httperf --num-conns 10  --add-header='Cache-Control: no-cache\n' \\n    --uri  /cgi-bin/munin-cgi-graph/localdomain/localhost.localdomain/cpu-day.png\n\nTotal: connections 10 requests 10 replies 10 test-duration 27.939 s\n\nConnection rate: 0.4 conn/s (2793.9 ms/conn, &lt;=1 concurrent connections)\nConnection time [ms]: min 1653.9 avg 2793.9 max 5217.0 median 1912.5 stddev 1487.8\nConnection time [ms]: connect 0.0\nConnection length [replies/conn]: 1.000\n\nRequest rate: 0.4 req/s (2793.9 ms/req)\nRequest size [B]: 131.0\n</pre>\n<h3>1.4 FastCGI</h3>\n<p>The munin-fastcgi-graph is only loaded once, but the munin-graph is reloaded\neach time.</p>\n<pre>\n$ httperf --num-conns 10  --add-header='Cache-Control: no-cache\n' \\n    --uri  /cgi-bin/munin-fastcgi-graph/localdomain/localhost.localdomain/cpu-day.png\n\nTotal: connections 10 requests 10 replies 10 test-duration 13.807 s\n\nConnection rate: 0.7 conn/s (1380.7 ms/conn, &lt;=1 concurrent connections)\nConnection time [ms]: min 1141.3 avg 1380.7 max 1636.1 median 1381.5 stddev 173.7\nConnection time [ms]: connect 0.0\nConnection length [replies/conn]: 1.000\n\nRequest rate: 0.7 req/s (1380.7 ms/req)\n</pre>\n<p>The response time is cut almost in half. That's expected, since only the top\nhalf of the processing isn't reloaded.</p>\n<h3>2.0 FastCGI</h3>\n<p>Here everything is loaded once.</p>\n<pre>\n$ httperf --num-conns 10  --add-header='Cache-Control: no-cache\n' \\n    --uri  /cgi-bin/munin-cgi-graph-2.0/localdomain/localhost.localdomain/cpu-day.png\n\nTotal: connections 10 requests 10 replies 10 test-duration 1.668 s\n\nConnection rate: 6.0 conn/s (166.8 ms/conn, &lt;=1 concurrent connections)\nConnection time [ms]: min 123.0 avg 166.8 max 513.4 median 127.5 stddev 121.9\nConnection time [ms]: connect 0.0\nConnection length [replies/conn]: 1.000\n\nRequest rate: 6.0 req/s (166.8 ms/req)\n</pre>\n<p>Now response time is cut almost by a ten factor ! That's quite good news,\nsince it goes 20 times faster that the original CGI.</p>","","waiting for munin performance fastcgi has cgi slow unsupported but does exist has even experimental fastcgi install mode quoting from this page this more proof concept than recommended slow also not test before every release lots work has been done take this experimental cgi mode into supported one might even the primary way using munin since when install has certain size cgi becomes mandatory that because munin graph doesn have time finish its job when the next one launched and the new one doesn run not dramatic missed munin update execution since the graphs will still generated the later round but there will random graph lags and will put quite some stress the cpu amp subsystem this will slow munin update down since also uses the subsystem much and that avoided all costs mainstream cgi has some consequences only the fastcgi wrapper remained the plain cgi one dropped the cpan module cgi fast compatible when launched normal cgi almost all http servers support plain cgi and with the cgi fcgi wrapper from the fastcgi devkit debian package libfcgi you can have the best both worlds custom http server amp fastcgi even posted how have working thttpd with fastcgi the old process limit mechanism dropped also the fastcgi server configuration much better way control the old code was based system semaphores and was not 100 reliable caching system has implemented order for each graph generated only once for its lifetime the cgi process launched with the http server user since doesn only read now but also writes log files and images files there extra step when installing but already described the munin cgi page given previously since the process launched only once for now read only once the config some part the config change the fastcgi container must restarted some benchmarks now the sweet part putting some micro benchmarks they should taken with caution every benchmark should but think the general idea conveyed for the sake simplicity only doing request parallel and disabled ims caching basic cgi httperf num conns add header cache control cache uri cgi bin munin cgi graph localdomain localhost localdomain cpu day png total connections requests replies test duration 939 connection rate conn 2793 conn concurrent connections connection time min 1653 avg 2793 max 5217 median 1912 stddev 1487 connection time connect connection length replies conn 000 request rate req 2793 req request size 131 fastcgi the munin fastcgi graph only loaded once but the munin graph reloaded each time httperf num conns add header cache control cache uri cgi bin munin fastcgi graph localdomain localhost localdomain cpu day png total connections requests replies test duration 807 connection rate conn 1380 conn concurrent connections connection time min 1141 avg 1380 max 1636 median 1381 stddev 173 connection time connect connection length replies conn 000 request rate req 1380 req the response time cut almost half that expected since only the top half the processing isn reloaded fastcgi here everything loaded once httperf num conns add header cache control cache uri cgi bin munin cgi graph localdomain localhost localdomain cpu day png total connections requests replies test duration 668 connection rate conn 166 conn concurrent connections connection time min 123 avg 166 max 513 median 127 stddev 121 connection time connect connection length replies conn 000 request rate req 166 req now response time cut almost ten factor that quite good news since goes times faster that the original cgi","a:1:{s:3:\"tag\";a:2:{i:0;s:7:\"munin20\";i:1;s:5:\"munin\";}}","1","0","1","0","0","0","0"
"440747","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","28205","2009-09-11 08:00:00","Europe/Paris","2009-09-11 12:58:20","2010-10-21 14:40:21","","post","wiki","2009/09/Sychronize-clock-between-hosts-with-SSH","en","Sychronize clock between hosts with SSH","","","[NTP|http://en.wikipedia.org/wiki/Network_Time_Protocol|en] is very handy for server clock synchronisation, but it can be cumbersome to deploy. \r\n\r\nSometimes you just need to do a one-shot clock synchronisation, so you use the standard @@date@@ command. But there isn't a flag to easily copy a setting to another. \r\n\r\n!!!! From a remote host\r\n\r\nQuite easy :\r\n\r\n///\r\n# date `ssh remoteuser@remotehost date +%m%d%H%M%Y.%S`\r\n///\r\n\r\n!!!! To a remote host\r\n\r\nIt's also very easy$$Yes, I __do__ know that logging remotely as root is a security pitfall...$$ : \r\n\r\n///\r\n# ssh root@remotehost date `date +%m%d%H%M%Y.%S`\r\n///","<p><a href=\"http://en.wikipedia.org/wiki/Network_Time_Protocol\" hreflang=\"en\">NTP</a> is very handy for server clock synchronisation, but it can be\ncumbersome to deploy.</p>\n<p>Sometimes you just need to do a one-shot clock synchronisation, so you use\nthe standard <code>date</code> command. But there isn't a flag to easily copy a\nsetting to another.</p>\n<h2>From a remote host</h2>\n<p>Quite easy :</p>\n<pre>\n# date `ssh remoteuser@remotehost date +%m%d%H%M%Y.%S`\n</pre>\n<h2>To a remote host</h2>\n<p>It's also very easy<sup>[<a href=\"#pnote-440747-1\" id=\"rev-pnote-440747-1\" name=\"rev-pnote-440747-1\">1</a>]</sup> :</p>\n<pre>\n# ssh root@remotehost date `date +%m%d%H%M%Y.%S`\n</pre>\n<div class=\"footnotes\">\n<h4>Notes</h4>\n<p>[<a href=\"#rev-pnote-440747-1\" id=\"pnote-440747-1\" name=\"pnote-440747-1\">1</a>] Yes, I <strong>do</strong> know that logging remotely\nas root is a security pitfall...</p>\n</div>","","sychronize clock between hosts with ssh ntp very handy for server clock synchronisation but can cumbersome deploy sometimes you just need one shot clock synchronisation you use the standard date command but there isn flag easily copy setting another from remote host quite easy date ssh remoteuser remotehost date remote host also very easy ssh root remotehost date date notes yes know that logging remotely root security pitfall","a:1:{s:3:\"tag\";a:1:{i:0;s:3:\"ssh\";}}","1","0","1","0","0","0","0"
"283294","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2008-11-04 20:00:00","Europe/Paris","2008-10-04 11:05:20","2012-03-07 17:07:53","","post","wiki","2008/11/04/A-Poor-Man-s-Munin-Node-to-Monitor-Hostile-UNIX-Servers","en","A Poor Man's Munin Node to Monitor \"Hostile\" UNIX Servers","","","[Munin|http://munin.projects.linpro.no/|en] is a nice monitoring system. Simple but quite effective. It's main selling point is the UNIX-esque simplicity of the architecture. You can just [create a new plugin|http://sysmonblog.co.uk/?p=17|en] in a matter of minutes to monitor whatever you can imagine.\r\n\r\nThere is even a [comprehensive collection|https://github.com/munin-monitoring/contrib/|en] of plugins ready to use (admittedly of various quality). \r\n\r\n!!!! Various platform are supported\r\n\r\nUsually the main issue is the ''MuninNode'', an agent (daemon) that runs on the server to be monitored, since it is responsible of translating the request of the munin server (the one with the  graphs) to the various plugins, build-ins or external. The ease of installing this agent depends on the OS and the access you have on the server : \r\n\r\n!!! Windows\r\n\r\nFor Windows you can install [munin-node-win32|http://www.jory.info/serendipity/index.php?/categories/4-Munin-Node-for-Windows|en]. \r\n\r\n!!! Unix (with root access)\r\n\r\nFor Unix when you are root, usually there is a package ready to install in your distribution, or from the source.\r\n\r\n!!! Others (Hostile servers)\r\n\r\nOn ''hostile'' servers, you don't usually have a root access and no easy acces to a compiler.\r\n\r\nI wrote [pmmn (Poor Man's Munin Node)|https://github.com/munin-monitoring/contrib/tree/master/tools/pmmn/|en], a little vanilla Perl script that emulates the core functionality of the real munin-node script, but without having to install many Perl CPAN modules.\r\n\r\nIt has also a nice functionality : it is possible to communicate via stdin/stdout instead of a TCP port. This way it is very easy to monitor hosts that are behind a firewall without opening (and monitoring) many tunnels. \r\n\r\n!!!! Installation of __pmmn__ \r\n\r\nSuppose you have access to the server via a supervision user (let's say ''supusr''). Installation of __pmmn__ is quite easy : just copy the files somewhere on the disk where you have access, for example (''/home/supusr'').\r\n\r\n!!! TCP installation\r\n\r\nJust launch the server with __-p 4949__ and declare it in the __munin.conf__ file on your munin-server. This solution is quite equivalent to a regular munin-node installation. \r\n\r\n!!! Tunnel installation\r\n\r\nSame as the TCP, but you have to [create a TCP tunnel via SSH|http://munin.projects.linpro.no/wiki/MuninSSHTunneling|en] to be able to reach the munin-node.\r\n\r\n!!! Inetd+SSH installation\r\n\r\nIt is a mix between [port forwarding via inetd|http://munin.projects.linpro.no/wiki/MuninInetdBouncing|en] and the  Tunnel-based previously discussed.\r\n\r\nYou first have to [established a key-based SSH authentication|http://www.laubenheimer.net/ssh-keys.shtml|en] without passphrase (you will not be there to type it) from ''supusr'' on the inet server (usually the munin-node one) to the user ''supusr'' on the server to be monitored.\r\n\r\nFor example, to monitor ''server1'' and ''server2'', in the file /etc/inetd.conf, you have to add lines :\r\n\r\n///\r\n7001   stream  tcp     nowait  supusr /usr/bin/ssh -- supusr@server1 /home/suprusr/pmmn/pmmn.pl\r\n7002   stream  tcp     nowait  supusr /usr/bin/ssh -- supusr@server2 /home/suprusr/pmmn/pmmn.pl\r\n///\r\n\r\nThen, in the ''munin.conf'' file of the MuninServer, you just have to declare the new nodes : \r\n///\r\n[server1]\r\n    address localhost\r\n    port 7001\r\n[server2]\r\n    address localhost\r\n    port 7002\r\n///\r\n\r\nThe MuninServer will now set up a stdin/stdout SSH tunnel transparently and launch the __pmmn__ server when needed. You are now free to write plugins like if a real munin-node where installed.  \r\nThe only restriction is that y  ou don't have a root access, so you are limited in the information you may collect.","<p><a href=\"http://munin.projects.linpro.no/\" hreflang=\"en\">Munin</a> is a nice\nmonitoring system. Simple but quite effective. It's main selling point is the\nUNIX-esque simplicity of the architecture. You can just <a href=\"http://sysmonblog.co.uk/?p=17\" hreflang=\"en\">create a new plugin</a> in a\nmatter of minutes to monitor whatever you can imagine.</p>\n<p>There is even a <a href=\"https://github.com/munin-monitoring/contrib/\" hreflang=\"en\">comprehensive collection</a> of plugins ready to use (admittedly\nof various quality).</p>\n<h2>Various platform are supported</h2>\n<p>Usually the main issue is the <em>MuninNode</em>, an agent (daemon) that\nruns on the server to be monitored, since it is responsible of translating the\nrequest of the munin server (the one with the graphs) to the various plugins,\nbuild-ins or external. The ease of installing this agent depends on the OS and\nthe access you have on the server :</p>\n<h3>Windows</h3>\n<p>For Windows you can install <a href=\"http://www.jory.info/serendipity/index.php?/categories/4-Munin-Node-for-Windows\" hreflang=\"en\">munin-node-win32</a>.</p>\n<h3>Unix (with root access)</h3>\n<p>For Unix when you are root, usually there is a package ready to install in\nyour distribution, or from the source.</p>\n<h3>Others (Hostile servers)</h3>\n<p>On <em>hostile</em> servers, you don't usually have a root access and no\neasy acces to a compiler.</p>\n<p>I wrote <a href=\"https://github.com/munin-monitoring/contrib/tree/master/tools/pmmn/\" hreflang=\"en\">pmmn (Poor Man's Munin Node)</a>, a little vanilla Perl script that\nemulates the core functionality of the real munin-node script, but without\nhaving to install many Perl CPAN modules.</p>\n<p>It has also a nice functionality : it is possible to communicate via\nstdin/stdout instead of a TCP port. This way it is very easy to monitor hosts\nthat are behind a firewall without opening (and monitoring) many tunnels.</p>\n<h2>Installation of <strong>pmmn</strong></h2>\n<p>Suppose you have access to the server via a supervision user (let's say\n<em>supusr</em>). Installation of <strong>pmmn</strong> is quite easy : just\ncopy the files somewhere on the disk where you have access, for example\n(<em>/home/supusr</em>).</p>\n<h3>TCP installation</h3>\n<p>Just launch the server with <strong>-p 4949</strong> and declare it in the\n<strong>munin.conf</strong> file on your munin-server. This solution is quite\nequivalent to a regular munin-node installation.</p>\n<h3>Tunnel installation</h3>\n<p>Same as the TCP, but you have to <a href=\"http://munin.projects.linpro.no/wiki/MuninSSHTunneling\" hreflang=\"en\">create a\nTCP tunnel via SSH</a> to be able to reach the munin-node.</p>\n<h3>Inetd+SSH installation</h3>\n<p>It is a mix between <a href=\"http://munin.projects.linpro.no/wiki/MuninInetdBouncing\" hreflang=\"en\">port\nforwarding via inetd</a> and the Tunnel-based previously discussed.</p>\n<p>You first have to <a href=\"http://www.laubenheimer.net/ssh-keys.shtml\" hreflang=\"en\">established a key-based SSH authentication</a> without passphrase\n(you will not be there to type it) from <em>supusr</em> on the inet server\n(usually the munin-node one) to the user <em>supusr</em> on the server to be\nmonitored.</p>\n<p>For example, to monitor <em>server1</em> and <em>server2</em>, in the file\n/etc/inetd.conf, you have to add lines :</p>\n<pre>\n7001   stream  tcp     nowait  supusr /usr/bin/ssh -- supusr@server1 /home/suprusr/pmmn/pmmn.pl\n7002   stream  tcp     nowait  supusr /usr/bin/ssh -- supusr@server2 /home/suprusr/pmmn/pmmn.pl\n</pre>\n<p>Then, in the <em>munin.conf</em> file of the MuninServer, you just have to\ndeclare the new nodes :</p>\n<pre>\n[server1]\n    address localhost\n    port 7001\n[server2]\n    address localhost\n    port 7002\n</pre>\n<p>The MuninServer will now set up a stdin/stdout SSH tunnel transparently and\nlaunch the <strong>pmmn</strong> server when needed. You are now free to write\nplugins like if a real munin-node where installed. The only restriction is that\ny ou don't have a root access, so you are limited in the information you may\ncollect.</p>","","poor man munin node monitor hostile unix servers munin nice monitoring system simple but quite effective main selling point the unix esque simplicity the architecture you can just create new plugin matter minutes monitor whatever you can imagine there even comprehensive collection plugins ready use admittedly various quality various platform are supported usually the main issue the muninnode agent daemon that runs the server monitored since responsible translating the request the munin server the one with the graphs the various plugins build ins external the ease installing this agent depends the and the access you have the server windows for windows you can install munin node win32 unix with root access for unix when you are root usually there package ready install your distribution from the source others hostile servers hostile servers you don usually have root access and easy acces compiler wrote pmmn poor man munin node little vanilla perl script that emulates the core functionality the real munin node script but without having install many perl cpan modules has also nice functionality possible communicate via stdin stdout instead tcp port this way very easy monitor hosts that are behind firewall without opening and monitoring many tunnels installation pmmn suppose you have access the server via supervision user let say supusr installation pmmn quite easy just copy the files somewhere the disk where you have access for example home supusr tcp installation just launch the server with 4949 and declare the munin conf file your munin server this solution quite equivalent regular munin node installation tunnel installation same the tcp but you have create tcp tunnel via ssh able reach the munin node inetd ssh installation mix between port forwarding via inetd and the tunnel based previously discussed you first have established key based ssh authentication without passphrase you will not there type from supusr the inet server usually the munin node one the user supusr the server monitored for example monitor server1 and server2 the file etc inetd conf you have add lines 7001 stream tcp nowait supusr usr bin ssh supusr server1 home suprusr pmmn pmmn 7002 stream tcp nowait supusr usr bin ssh supusr server2 home suprusr pmmn pmmn then the munin conf file the muninserver you just have declare the new nodes server1 address localhost port 7001 server2 address localhost port 7002 the muninserver will now set stdin stdout ssh tunnel transparently and launch the pmmn server when needed you are now free write plugins like real munin node where installed the only restriction that don have root access you are limited the information you may collect","a:1:{s:3:\"tag\";a:2:{i:0;s:3:\"ssh\";i:1;s:5:\"munin\";}}","1","0","1","0","2","0","0"
"540984","d0cea1ba0b96cc1f1d745b224ff8099a","SS4181-GANDI","38368","2010-08-23 18:47:00","Europe/Paris","2010-08-22 14:44:39","2010-08-23 16:52:03","","post","wiki","2010/08/Waiting-for-Munin-2.0-Keep-more-data-with-custom-data-retention-plans","en","Waiting for Munin 2.0 - Keep more data with custom data retention plans","","","!!!! RRD is Munin's backbone.\r\n\r\nMunin keeps its data in an [RRD database|http://oss.oetiker.ch/rrdtool/]. It's a wonderful piece of software, designed for this very purpose : keep an history of numeric data.\r\n\r\nAll you need is to tell RRD for how long and the precision you want to keep your data. RRD manages then all the underlying work : pruning old data, averaging to decrease precision if needed, ... \r\n\r\nMunin automatically creates the RRD databases it needs. \r\n\r\n!!!! 1.2 - Only one set\r\n\r\nIn 1.2, every database creation was done with the same temporal & precision parameters. Since the output parameters were constant (day, week, month, year graphs), there were little need to have a different set of parameters.\r\n\r\n!!!! 1.4 - 2 sets : normal & huge\r\n\r\nIn 1.4, various users showed their need to have different graphing outputs, and began to hack around Munin's fixed graphing. It became rapidly obvious that the 1.2 preset wasn't a fit for everyone. \r\n\r\nTherefore a @@huge@@ dataset was available to be able to extend the finest precision (5min) to the whole Munin timeframe. This comes at a price though : more space is required, and the graph generation is slower, specially when generating the yearly one, since more data has to be read and analysed.\r\n\r\nThe switch is done for the whole munin installation by changing the system-wide @@graph_data_size@@, although already created rrd databases aren't changed. \r\nIt is then even possible for a user to pre-customize the rrd file. Munin will then happily uses them transparently thanks to the RRD layer.\r\n\r\n!!! Manual overriding\r\n\r\nAltering the RRD files after it is created is possible, but not as simple. Standard export & import from RRD take the structure with it. So data has to be moved around with special tools. [rrdmove|http://code.google.com/p/pmptools/source/browse/trunk/rrd/rrdmove] is my attempt to create such a tool. It copies data between 2 already existing RRD files, even asking RRD to interpolate the data when needed. \r\n\r\n!!!! 2.0 - Full control\r\n\r\nStarting with 2.0, the parameter @@graph_data_size@@ is per service. It also has a special mode : @@custom@@. [Its format|http://munin-monitoring.org/wiki/format-graph_data_size] is very simple : \r\n\r\n/// \r\ngraph_data_size custom FULL_NB, MULTIPLIER_1 MULTIPLIER_1_NB, ... MULTIPLIER_NMULTIPLIER_N_NB\r\ngraph_data_size custom 300, 15 1600, 30 3000\r\n///\r\n\r\nThe first number is the number of data at full resolution. Then usually it comes gradually decreasing resolution.\r\n\r\nA decreasing resolution has 2 usages : \r\n\r\n* Limit the space consumption : keeping full resolution for the whole period (default : 5min for 2 years) is sometime too precise.\r\n* Increase performance : RRD will choose the best fitting resolution to generate its graphs. Already aggregated data is faster to compute.","<h2>RRD is Munin's backbone.</h2>\n<p>Munin keeps its data in an <a href=\"http://oss.oetiker.ch/rrdtool/\">RRD\ndatabase</a>. It's a wonderful piece of software, designed for this very\npurpose : keep an history of numeric data.</p>\n<p>All you need is to tell RRD for how long and the precision you want to keep\nyour data. RRD manages then all the underlying work : pruning old data,\naveraging to decrease precision if needed, ...</p>\n<p>Munin automatically creates the RRD databases it needs.</p>\n<h2>1.2 - Only one set</h2>\n<p>In 1.2, every database creation was done with the same temporal &amp;\nprecision parameters. Since the output parameters were constant (day, week,\nmonth, year graphs), there were little need to have a different set of\nparameters.</p>\n<h2>1.4 - 2 sets : normal &amp; huge</h2>\n<p>In 1.4, various users showed their need to have different graphing outputs,\nand began to hack around Munin's fixed graphing. It became rapidly obvious that\nthe 1.2 preset wasn't a fit for everyone.</p>\n<p>Therefore a <code>huge</code> dataset was available to be able to extend the\nfinest precision (5min) to the whole Munin timeframe. This comes at a price\nthough : more space is required, and the graph generation is slower, specially\nwhen generating the yearly one, since more data has to be read and\nanalysed.</p>\n<p>The switch is done for the whole munin installation by changing the\nsystem-wide <code>graph_data_size</code>, although already created rrd\ndatabases aren't changed. It is then even possible for a user to pre-customize\nthe rrd file. Munin will then happily uses them transparently thanks to the RRD\nlayer.</p>\n<h3>Manual overriding</h3>\n<p>Altering the RRD files after it is created is possible, but not as simple.\nStandard export &amp; import from RRD take the structure with it. So data has\nto be moved around with special tools. <a href=\"http://code.google.com/p/pmptools/source/browse/trunk/rrd/rrdmove\">rrdmove</a>\nis my attempt to create such a tool. It copies data between 2 already existing\nRRD files, even asking RRD to interpolate the data when needed.</p>\n<h2>2.0 - Full control</h2>\n<p>Starting with 2.0, the parameter <code>graph_data_size</code> is per\nservice. It also has a special mode : <code>custom</code>. <a href=\"http://munin-monitoring.org/wiki/format-graph_data_size\">Its format</a> is\nvery simple :</p>\n<pre>\n \ngraph_data_size custom FULL_NB, MULTIPLIER_1 MULTIPLIER_1_NB, ... MULTIPLIER_NMULTIPLIER_N_NB\ngraph_data_size custom 300, 15 1600, 30 3000\n</pre>\n<p>The first number is the number of data at full resolution. Then usually it\ncomes gradually decreasing resolution.</p>\n<p>A decreasing resolution has 2 usages :</p>\n<ul>\n<li>Limit the space consumption : keeping full resolution for the whole period\n(default : 5min for 2 years) is sometime too precise.</li>\n<li>Increase performance : RRD will choose the best fitting resolution to\ngenerate its graphs. Already aggregated data is faster to compute.</li>\n</ul>","","waiting for munin keep more data with custom data retention plans rrd munin backbone munin keeps its data rrd database wonderful piece software designed for this very purpose keep history numeric data all you need tell rrd for how long and the precision you want keep your data rrd manages then all the underlying work pruning old data averaging decrease precision needed munin automatically creates the rrd databases needs only one set every database creation was done with the same temporal amp precision parameters since the output parameters were constant day week month year graphs there were little need have different set parameters sets normal amp huge various users showed their need have different graphing outputs and began hack around munin fixed graphing became rapidly obvious that the preset wasn fit for everyone therefore huge dataset was available able extend the finest precision 5min the whole munin timeframe this comes price though more space required and the graph generation slower specially when generating the yearly one since more data has read and analysed the switch done for the whole munin installation changing the system wide graph data size although already created rrd databases aren changed then even possible for user pre customize the rrd file munin will then happily uses them transparently thanks the rrd layer manual overriding altering the rrd files after created possible but not simple standard export amp import from rrd take the structure with data has moved around with special tools rrdmove attempt create such tool copies data between already existing rrd files even asking rrd interpolate the data when needed full control starting with the parameter graph data size per service also has special mode custom its format very simple graph data size custom full multiplier multiplier multiplier nmultiplier graph data size custom 300 1600 3000 the first number the number data full resolution then usually comes gradually decreasing resolution decreasing resolution has usages limit the space consumption keeping full resolution for the whole period default 5min for years sometime too precise increase performance rrd will choose the best fitting resolution generate its graphs already aggregated data faster compute","a:1:{s:3:\"tag\";a:3:{i:0;s:8:\"sysadmin\";i:1;s:7:\"munin20\";i:2;s:5:\"munin\";}}","1","0","1","0","4","0","0"

[media media_id,user_id,media_path,media_title,media_file,media_meta,media_dt,media_creadt,media_upddt,media_private,media_dir]
"108675","SS4181-GANDI","d0cea1ba0b96cc1f1d745b224ff8099a","custom_style.css","custom_style.css","","2007-09-19 04:39:00","2007-10-12 08:45:19.642013","2007-10-12 08:45:19.642013","0","."

[comment comment_id,post_id,comment_dt,comment_tz,comment_upddt,comment_author,comment_email,comment_site,comment_content,comment_words,comment_ip,comment_status,comment_spam_status,comment_trackback,comment_spam_filter]
"8378819","243917","2008-12-15 21:49:05","Europe/Paris","2008-12-15 20:49:05","Krysztof von Murphy","christophe@courtois.cc","http://www.courtois.cc/blogeclectique/","<p>Mistakes in isolation are the worst when you don't know much about the\nproblem/the language/the algorithm/the business, you go full speed into the\nwall.<br />\nOn the contrary, a Master in isolation WILL achieve high productivity (that is,\nuntil the next paradigm change, that he won’t see from his ivory tower).</p>\n<p>Another fact is that you really learn when you burn yourself. Of course,\nletting others criticize your work is learning their experience without getting\nburnt yourself. But if you always follow the advices from the others, you may\navoid valuable lessons: knowing WHO are the real gurus whose advice really\ncount; when knowing that YOU are right and all the others aren’t; and\nrecognizing that they were right indeed, but you had to try.</p>","mistakes isolation are the worst when you don know much about the problem the language the algorithm the business you full speed into the wall the contrary master isolation will achieve high productivity that until the next paradigm change that won’t see from his ivory tower another fact that you really learn when you burn yourself course letting others criticize your work learning their experience without getting burnt yourself but you always follow the advices from the others you may avoid valuable lessons knowing who are the real gurus whose advice really count when knowing that you are right and all the others aren’t and recognizing that they were right indeed but you had try","82.231.211.69","1","0","0",""
"8205664","243917","2008-06-25 15:51:52","Europe/Paris","2008-06-25 13:51:52","pieroxy","pieroxy@gmail.com","","<p>You say &quot;I actually prefer to have negative comments than positive ones...\nat least when they are well argumented&quot;</p>\n<p>You are (I think) mixing up &quot;negative comment&quot; and &quot;criticism&quot;.</p>\n<p>A negative comment that is well argumented is called a constructive\ncriticism. That's a criticism you can build on, by opposition to destructive\none that don't give you anything to improve.</p>\n<p>I think everyone agrees to the fact that constructive criticism is good!</p>","you say quot actually prefer have negative comments than positive ones least when they are well argumented quot you are think mixing quot negative comment quot and quot criticism quot negative comment that well argumented called constructive criticism that criticism you can build opposition destructive one that don give you anything improve think everyone agrees the fact that constructive criticism good","194.51.44.193","1","0","0",""
"8886789","283294","2010-06-12 12:14:44","Europe/Paris","2010-06-26 07:01:38","Steve Schnepp","steve.schnepp@pwkf.org","http://blog.pwkf.org/","<p>Yes, you just have to install the files in a directory. But since it doesn't\nlisten on port 4949 (since port opening is usually restricted on\nhostile hosts)</p>\n<p>So you have to either :</p>\n<ul>\n<li>create an SSH pipe like the described inetd+ssh </li>\n<li>use the <a href=\"/post/2010/06/Waiting-for-Munin-2.0-Introduction\">upcoming\nmunin 2.0 SSH-transport feature</a>. </li>\n</ul>","yes you just have install the files directory but since doesn listen port 4949 since port opening usually restricted hostile hosts you have either create ssh pipe like the described inetd ssh use the upcoming munin ssh transport feature","89.224.151.36","1","0","0",""
"8458261","381574","2009-04-09 22:12:00","Europe/Paris","2009-04-09 20:12:00","Krysztof von Murphy","christophe@courtois.cc","","<p>Tom Kyte would say something along the lines of “if the answer is a trigger,\nthen you didn’t understand the question”, and I hate things that happen\nmagically under the hood too, but I can’t think of any other maintenable ways\nof adding stamp dates in an application. I had to deal with it in Oracle\nApplications, all tables had creation_date, update_login, with associated\nlogins and so on (but no history): it was a pain to add all these columns in\nevery bit of code.</p>\n<p>Anyway, the ddj article is very good. I have this recurring problem in my\ncurrent job, it’s nice to read something clean about the different options.</p>","tom kyte would say something along the lines “if the answer trigger then you didn’t understand the question” and hate things that happen magically under the hood too but can’t think any other maintenable ways adding stamp dates application had deal with oracle applications all tables had creation date update login with associated logins and but history was pain add all these columns every bit code anyway the ddj article very good have this recurring problem current job it’s nice read something clean about the different options","82.231.211.69","1","0","0",""
"8458024","391283","2009-04-09 15:50:20","Europe/Paris","2009-04-09 13:50:20","Krysztof von Murphy","christophe@courtois.cc","http://www.courtois.cc/blogeclectique/","<p>I've got blog entries that were &quot;in progress&quot; for years. Or still are.</p>\n<p>A day or a week precision is not important. It should be the date at the\n(first) publication. Before that, the URL is not supposed to be published and\nits URL can change.</p>","got blog entries that were quot progress quot for years still are day week precision not important should the date the first publication before that the url not supposed published and its url can change","129.184.84.11","1","0","0",""
"8458028","391283","2009-04-09 16:04:13","Europe/Paris","2009-04-09 14:21:49","Steve Schnepp","steve.schnepp@pwkf.org","","True, and solved typically by software. Make me wonder why blogging engines\ndon't do it auto-magically... Actually, in mine, I can just erase the first\ngenerated one, change it's modification date to now via the provided little\ncalendar widget, and the URL will be regenerated with the fresh, updated, date.\nUnfortunately this translates itself to many manual steps that could be easily\nautomated in the engine (may be on a opt-in/out basis), and therefore not\nforgotten.","true and solved typically software make wonder why blogging engines don auto magically actually mine can just erase the first generated one change modification date now via the provided little calendar widget and the url will regenerated with the fresh updated date unfortunately this translates itself many manual steps that could easily automated the engine may opt out basis and therefore not forgotten","83.206.127.225","1","0","0",""
"8512835","402145","2009-06-15 18:24:05","Europe/Paris","2009-06-15 16:24:05","Krysztof von Murphy","christophe@courtois.cc","http://www.courtois.cc/blogeclectique/","<p>Que dire sinon que tu parles d'or ? Je soufre actuellement (et le client\npaye très cher au final) à cause d'une application dénormalisée à mort faite\npar des gens qui doivent raisonner encore en fichiers plats et en Cobol.</p>\n<p>Ajoutons que stocker contraintes, vues utilitaires, même logique\nfonctionnelle en base permet de garantir la cohérence contre toutes les\nattaques contre la cohérence de la base (du point de vue du DBA) : développeurs\nindisciplinés, doc pas claire, erreurs humaines, sous-traitance,\nstagiaires...</p>\n<p>La dénormalisation doit être réservée aux cas extrêmes, et comme tu dis,\nentre les vues simples et les vues matérialisées, on doit pouvoir simplifier le\ndéveloppement. Attention, ces vues à mon sens ne doivent pas contenir de\nlogique fonctionnelle précise sinon on duplique ce qui existe ailleurs dans\nl'appli (sauf à réutiliser, disons, des fonctions stockées fournies par cet\nappli) ; je n'aime que les vues qui font des jointures naturelles (le long des\nclés étrangères, comme n'importe qui a le réflexe de faire) sauf à coder pour\nune application précise.</p>\n<p>À propos des jointures que l'on évite : c'est encore plus intéressant avec\nles clés naturelles puisqu'on a souvent là deux ou trois colonnes par index\nsinon plus, ce qui fait autant que l'on n'a pas à écrire en en oubliant. (Une\ndes raisons pour lesquelles je préfère les clés arbitraires.)</p>","que dire sinon que parles soufre actuellement client paye très cher final cause une application dénormalisée mort faite par des gens qui doivent raisonner encore fichiers plats cobol ajoutons que stocker contraintes vues utilitaires même logique fonctionnelle base permet garantir cohérence contre toutes les attaques contre cohérence base point vue dba développeurs indisciplinés doc pas claire erreurs humaines sous traitance stagiaires dénormalisation doit être réservée aux cas extrêmes comme dis entre les vues simples les vues matérialisées doit pouvoir simplifier développement attention ces vues mon sens doivent pas contenir logique fonctionnelle précise sinon duplique qui existe ailleurs dans appli sauf réutiliser disons des fonctions stockées fournies par cet appli aime que les vues qui font des jointures naturelles long des clés étrangères comme importe qui réflexe faire sauf coder pour une application précise propos des jointures que évite est encore plus intéressant avec les clés naturelles puisqu souvent deux trois colonnes par index sinon plus qui fait autant que pas écrire oubliant une des raisons pour lesquelles préfère les clés arbitraires","82.231.211.69","1","0","0",""
"8512878","402145","2009-06-15 18:40:52","Europe/Paris","2009-06-15 16:40:52","Krysztof von Murphy","christophe@courtois.cc","http://www.courtois.cc/blogeclectique/","<p>J'ajoute aussi :</p>\n<p>Il y a un domaine où on dénormalise à fond délibérément : les systèmes\ndécisionnels (datawarehouse). Ce sont des systèmes séparés alimentés par les\ndiverses applications d'une entreprise. Les perfs à la lecture passent devant\nles soucis de volumétrie ou de temps de mise à jour. Mais ladite mise à jour se\nfait en batch nocturne ou ouikénal.</p>\n<p>On utilise rarement du code applicatif classique mais des outils spécialisés\n(ETL) qui se chargent de malaxer les données depuis plusieurs applications,\ncalculer un milliard d'indicateurs, etc. Et avoir des vues déjà toute prêtes du\ncôté de l'application dont on pompe les données est effectivement une\nbénédiction pour le développeur qui n'a pas à recoder les jointures dans son\noutil (surtout si lesdites jointures sont subtiles...).</p>\n<p>Ces systèmes décisionnels (et les autres systèmes qui ont besoin de regarder\n« sous le capot » au niveau SQL) sont une des raisons qui doivent\ndécourager de mettre de la logique (contraintes, cohérence statique, pas les\nflux et process) ailleurs que dans la base. À l'inverse, il existe des plugins\n(coûteux) pour les ETL courant pour lire par exemple dans SAP, l'archétype de\nl'application qui interdit que quiconque lise dans la base sans passer par lui\n(et ça donne ça :<a href=\"http://www.courtois.cc/blogeclectique/index.php?post/2006/10/14/246-des-millions-de-lignes-a-travers-le-millefeuille\" title=\"http://www.courtois.cc/blogeclectique/index.php?post/2006/10/14/246-des-millions-de-lignes-a-travers-le-millefeuille\">http://www.courtois.cc/blogeclectiq...</a>)</p>\n<p>Par contre, le code applicatif dans la base c'est pas glorieux pour\nl'indépendance envers les fournisseurs de BdD, il faut savoir ce qui est\nimportant pour son application, et coder au PPCD si on tient à avoir des\ndonnées « libres » - d'où des compromis dans les fonctionnalités\nutilisables.</p>","ajoute aussi domaine dénormalise fond délibérément les systèmes décisionnels datawarehouse sont des systèmes séparés alimentés par les diverses applications une entreprise les perfs lecture passent devant les soucis volumétrie temps mise jour mais ladite mise jour fait batch nocturne ouikénal utilise rarement code applicatif classique mais des outils spécialisés etl qui chargent malaxer les données depuis plusieurs applications calculer milliard indicateurs etc avoir des vues déjà toute prêtes côté application dont pompe les données est effectivement une bénédiction pour développeur qui pas recoder les jointures dans son outil surtout lesdites jointures sont subtiles ces systèmes décisionnels les autres systèmes qui ont besoin regarder sous capot niveau sql sont une des raisons qui doivent décourager mettre logique contraintes cohérence statique pas les flux process ailleurs que dans base inverse existe des plugins coûteux pour les etl courant pour lire par exemple dans sap archétype application qui interdit que quiconque lise dans base sans passer par lui donne http www courtois blogeclectiq par contre code applicatif dans base est pas glorieux pour indépendance envers les fournisseurs bdd faut savoir qui est important pour son application coder ppcd tient avoir des données libres des compromis dans les fonctionnalités utilisables","82.231.211.69","1","0","0",""
"8513296","402145","2009-06-16 08:53:39","Europe/Paris","2009-06-16 06:54:02","Steve Schnepp","steve.schnepp@pwkf.org","http://blog.pwkf.org/","<p>Merci :-)</p>\n<p>For an english translation of these interesting comments, just use <a href=\"http://translate.google.com/translate?js=n&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fblog.pwkf.org%2Fpost%2F2009%2F05%2FEfficient-Denormalization-with-Views&amp;sl=fr&amp;tl=en&amp;history_state0=\" hreflang=\"en\">Google translate</a> services.</p>","merci for english translation these interesting comments just use google translate services","89.224.151.36","1","0","0",""
"8514575","402145","2009-06-17 07:53:38","Europe/Paris","2009-06-17 05:53:38","Krysztof von Murphy","christophe@courtois.cc","http://www.courtois.cc/blogeclectique/","<p>Sorry, I didn’t even see I was replying in another language :-))))</p>","sorry didn’t even see was replying another language","82.231.211.69","1","0","0",""
"8565087","395563","2009-08-21 14:00:47","Europe/Paris","2009-08-21 12:00:47","Krystof von Murphy","christophe@courtois.cc","http://www.courtois.cc/","<p>Whatever the choice of the developer, that’as better than these &quot;checked&quot;\nexceptions:</p>\n<pre>\nBEGIN\n (SQL code...)\nEXCEPTIONS \n WHEN OTHERS THEN NULL ;\nEND ;\n</pre>\n<p>(People should be shot for that.)</p>","whatever the choice the developer that’as better than these quot checked quot exceptions begin sql code exceptions when others then null end people should shot for that","212.23.165.253","1","0","0",""
"8565103","395563","2009-08-21 14:21:18","Europe/Paris","2009-08-21 12:21:18","Steve Schnepp","steve.schnepp@pwkf.org","http://blog.pwkf.org/","<p>Yeah, same here in Java :</p>\n<pre>\n  try {\n     processThing();\n  } catch (Exception e) {\n     // Do nothing\n  }\n</pre>\n<p>Don't ever underestimate the power of laziness in the same team as ignorance\n:-)</p>","yeah same here java try processthing catch exception nothing don ever underestimate the power laziness the same team ignorance","194.51.44.193","1","0","0",""
"8565110","340907","2009-08-21 14:31:35","Europe/Paris","2009-08-21 12:31:35","Steve Schnepp","steve.schnepp@pwkf.org","http://blog.pwkf.org/","<p>The things I discovered so far are :</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Zeroconf\">ZeroConf</a> based\nsolutions, mostly using UPnP</li>\n<li>Have a small server like <a href=\"http://en.wikipedia.org/wiki/Pdnsd\">Pdnsd</a>.</li>\n</ul>","the things discovered far are zeroconf based solutions mostly using upnp have small server like pdnsd","83.206.127.225","1","0","0",""
"8565085","340907","2009-08-21 13:51:37","Europe/Paris","2009-08-21 11:51:37","Krystof von Murphy","christophe@courtois.cc","http://www.courtois.cc/","<p>Me too.</p>\n<p>A problem: it means a dedicated server, always on, and another point of\nfailure (server down, no network). For a home network, I’d like to see it\nintegrated into the ADSL box (which is a DHCP server already). For the moment,\nI live with manual hosts files.</p>\n<p>Does a distributed DNS exist, where any Linux/Win/Mac on the network can be\n&quot;the&quot; DNS ?</p>","too problem means dedicated server always and another point failure server down network for home network i’d like see integrated into the adsl box which dhcp server already for the moment live with manual hosts files does distributed dns exist where any linux win mac the network can quot the quot dns","212.23.165.253","1","0","0",""
"8642867","459477","2009-11-27 19:51:40","Europe/Paris","2009-11-27 18:51:40","Steve Schnepp","steve.schnepp@pwkf.org","http://blog.pwkf.org/","<p>Thx.</p>\n<p>About i18n, I'm wondering if it's the local part or the encoding that cause\nthis massive slowdown...<br /></p>","thx about i18n wondering the local part the encoding that cause this massive slowdown","89.224.151.36","1","0","0",""
"8642085","459477","2009-11-27 10:53:24","Europe/Paris","2009-11-27 09:53:24","Andreas Schamanek","schamane@fam.tuwien.ac.at","http://wox.at/as/_/","<p>Indeed a very interesting finding. Thanks for sharing.<br />\nOne cannot but repeat again and again how important it is to set locales\nproperly with every piece of code. Personally, I am no fan of i18n the command\nline.</p>","indeed very interesting finding thanks for sharing one cannot but repeat again and again how important set locales properly with every piece code personally fan i18n the command line","62.178.30.209","1","0","0",""
"9178616","527747","2011-05-01 21:02:17","Europe/Paris","2011-05-01 19:02:17","Mike Robinson","miker@sundialservices.com","","<p>To me, the overwhelming advantage of FastCGI vs. mod_perl comes when the\n&quot;web pages&quot; on a site begin to vary from trivial, fast-running ones, to reports\nthat might take hundreds of megabytes and hundreds of wall-clock seconds to\ncomplete. Suddenly, &quot;an 'httpd' process instance&quot; has ballooned from being a\nlean-and-mean process, in the eyes of the operating system, to a sluggish\nthousand-pound gorilla. Operating systems react slowly and awkwardly and\ninefficiently to such radical changes that come without warning. (They also\nrecover poorly, when the &quot;thousand pound gorilla&quot; that it moved heaven-and-hell\nto accommodate, suddenly becomes a docile mouse again ... and then, oh my,\nANOTHER 'httpd' instance has just turned into a gorilla!)</p>\n<p>&quot;Be nice to the operating system, and it will be nice to you.&quot; If not (and\nif you happen to remember this old TV commercial...) &quot;it's not NICE to fool\nwith Mother Nature...&quot; The OS will accommodate you, because that's its job, but\nyou will pay a price.</p>\n<p>FastCGI moves the actual processing-work an arm's length away from the\n'httpd' processes, allowing 'httpd' to continue to be fast-and-light at all\ntimes, and to continually present consistent run-time characteristics no matter\nwhat.</p>\n<p>Depending on the FastCGI container (application server...) that you use (or\nbuild...), you can distribute the requests that the web-server is sending,\namong a &quot;suitable&quot; mixture of CPUs and service-processes. If you want to build\nqueues, to use batch-processing software and so on, that also is much easier to\ndo with the FastCGI way of doing things.</p>\n<p>In short, 'httpd' is now able to remain as what it is best at being: a\nuser-interface of sorts, able to field thousands of requests per second, but no\nlonger -itself- burdened by the responsibility of actually servicing those\nrequests. &quot;It's just the delivery boy.&quot; Meanwhile, the FastCGI stream is\nserviced by a -team- of more-or-less specialist processes that may be running\non any number of back-end computers. The results are predictable, debuggable,\nand scalable.</p>","the overwhelming advantage fastcgi mod perl comes when the quot web pages quot site begin vary from trivial fast running ones reports that might take hundreds megabytes and hundreds wall clock seconds complete suddenly quot httpd process instance quot has ballooned from being lean and mean process the eyes the operating system sluggish thousand pound gorilla operating systems react slowly and awkwardly and inefficiently such radical changes that come without warning they also recover poorly when the quot thousand pound gorilla quot that moved heaven and hell accommodate suddenly becomes docile mouse again and then another httpd instance has just turned into gorilla quot nice the operating system and will nice you quot not and you happen remember this old commercial quot not nice fool with mother nature quot the will accommodate you because that its job but you will pay price fastcgi moves the actual processing work arm length away from the httpd processes allowing httpd continue fast and light all times and continually present consistent run time characteristics matter what depending the fastcgi container application server that you use build you can distribute the requests that the web server sending among quot suitable quot mixture cpus and service processes you want build queues use batch processing software and that also much easier with the fastcgi way doing things short httpd now able remain what best being user interface sorts able field thousands requests per second but longer itself burdened the responsibility actually servicing those requests quot just the delivery boy quot meanwhile the fastcgi stream serviced team more less specialist processes that may running any number back end computers the results are predictable debuggable and scalable","67.161.224.178","1","0","0",""
"9508558","526512","2011-12-05 14:13:00","Europe/Paris","2011-12-05 13:13:00","Steve Schnepp","steve.schnepp@gmail.com","","<p>@Özgür: via the munin-async-server's little brother : munin-async-client. It\nspoolfetchs from the spool files back to the master.</p>\n<p>It doesn't need to run on the same host : you can even periodically rsync\nthe spool files at a lower interval than 5 min.</p>","özgür via the munin async server little brother munin async client spoolfetchs from the spool files back the master doesn need run the same host you can even periodically rsync the spool files lower interval than min","89.224.151.36","1","0","0",""
"9508471","526512","2011-12-05 11:12:19","Europe/Paris","2011-12-05 10:12:19","Özgür Kuru","ozgur@ozgurkuru.net","http://www.ozgurkuru.net","<p>munin-async-server is connecting to local munin-node and fetch data after\nthat store data in spool files. every thing okay. But How master server get\ndata from nodes?.</p>","munin async server connecting local munin node and fetch data after that store data spool files every thing okay but how master server get data from nodes","212.156.62.158","1","0","0",""
"8908586","526512","2010-07-02 03:10:35","Europe/Paris","2010-07-02 01:10:35","Edward Groenendaal","eddyg@myreflection.org","","<p>These are very exciting changes that I'm looking forward to, I think the\nproxy node is a good compromise on the simplicity vs functionality front. The\nonly reason our company is using munin is the simplicity of the plugins,\nallowing easy and quick additions.</p>","these are very exciting changes that looking forward think the proxy node good compromise the simplicity functionality front the only reason our company using munin the simplicity the plugins allowing easy and quick additions","202.137.165.217","1","0","0",""
"9156691","532312","2011-04-06 19:21:57","Europe/Paris","2011-04-06 17:21:57","Tenzer","munin@tenzer.dk","http://tenzer.dk/","<p>Hi, I just want to let you know that the sample SSH address you have in this\npost will not work. You should leave out the colon (:) from the string,\notherwise the perl URI module will not be able to parse the hostname corretly,\nand this leaves Munin trying to connect to &quot;host.example.com:&quot; which is not a\nvalid hostname.</p>\n<p>I have submitted a patch which enables you to specify the port number of the\nremote SSH server on this ticket: <a href=\"http://munin-monitoring.org/ticket/1063.\" title=\"http://munin-monitoring.org/ticket/1063.\">http://munin-monitoring.org/ticket/...</a></p>\n<p>Otherwise good job on the blog, I couldn't really find much other\ninformation on how to set this up :)</p>","just want let you know that the sample ssh address you have this post will not work you should leave out the colon from the string otherwise the perl uri module will not able parse the hostname corretly and this leaves munin trying connect quot host example com quot which not valid hostname have submitted patch which enables you specify the port number the remote ssh server this ticket http munin monitoring org ticket otherwise good job the blog couldn really find much other information how set this","130.226.210.2","1","0","0",""
"9157102","532312","2011-04-07 13:40:24","Europe/Paris","2011-04-07 11:41:13","Steve Schnepp","steve.schnepp@pwkf.org","http://blog.pwkf.org/","Thanks for your remark : I edited the post to have a correct code example. I\nalso integrated (r4153) the patch you posted in trunk.","thanks for your remark edited the post have correct code example also integrated r4153 the patch you posted trunk","83.206.127.225","1","0","0",""
"9257744","532312","2011-07-16 00:42:06","Europe/Paris","2011-07-15 22:42:06","Matt Kaufman","matt@mkfmn.com","http://www.mkfmn.com","<p>Perfect! I wasn't too familiar with munin's internal communication and\ndidn't know if I could use it like this.....</p>\n<p>I was actually just going to use netcat myself too, to do it. Hehe! However,\nI hadn't gotten to checking its messagings internally yet. Yay!</p>","perfect wasn too familiar with munin internal communication and didn know could use like this was actually just going use netcat myself too hehe however hadn gotten checking its messagings internally yet yay","68.173.139.53","1","0","0",""
"9394300","540984","2011-10-07 00:04:16","Europe/Paris","2011-10-06 22:04:16","Cosimo","cosimo@cpan.org","http://my.opera.com/cstrep/blog/","<p>Is also a feature of 2.0 the possibility to annotate graphs to mark specific\ntimestamps or with a text note?</p>","also feature the possibility annotate graphs mark specific timestamps with text note","84.215.40.84","1","0","0",""
"9455233","540984","2011-11-08 11:07:29","Europe/Paris","2011-11-08 10:07:29","Steve Schnepp","steve.schnepp@gmail.com","","<p>@Cosimo: not really. It would require a database to store the text notes.\nAnd for now, munin tries to be as light-weight to install and use.</p>","cosimo not really would require database store the text notes and for now munin tries light weight install and use","77.242.201.53","1","0","0",""
"9214711","540984","2011-06-09 20:59:17","Europe/Paris","2011-06-09 18:59:17","Steve Schnepp","steve.schnepp@gmail.com","","<p>Absolutely :-)</p>","absolutely","89.224.151.36","1","0","0",""
"9214012","540984","2011-06-09 13:19:25","Europe/Paris","2011-06-09 11:19:25","scott","scott@omnisys.com","","<p>does &quot;especially useful when zooming&quot; mean that zooming is a feature of\n2.0?</p>","does quot especially useful when zooming quot mean that zooming feature","166.154.19.8","1","0","0",""
"11968046","721204","2013-02-27 22:19:54","Europe/Paris","2013-02-27 21:19:54","Krysztof von Murphy","christophe@courtois.cc","http://www.courtois.cc/murphy/","<p>Tiens, je savais pas que tu avais pris tant de galon sur un outil qui me\nsert pas mal à titre pero depuis longtemps ;o) Merci !</p>","tiens savais pas que avais pris tant galon sur outil qui sert pas mal titre pero depuis longtemps merci","82.228.9.193","1","0","0",""

[meta meta_id,meta_type,post_id]
"sql","tag","117264"
"database","tag","117264"
"sql","tag","156496"
"ssh","tag","283294"
"munin","tag","283294"
"munin","tag","267373"
"sql","tag","309272"
"database","tag","309272"
"sql","tag","391276"
"database","tag","391276"
"sql","tag","401164"
"database","tag","401164"
"sql","tag","406128"
"database","tag","406128"
"sql","tag","407344"
"database","tag","407344"
"sql","tag","415129"
"database","tag","415129"
"sql","tag","340907"
"ssh","tag","440747"
"munin","tag","459093"
"sysadmin","tag","459093"
"sql","tag","459477"
"perl","tag","459477"
"locales","tag","459477"
"ssh","tag","462735"
"munin","tag","462735"
"garbage-collection","tag","466295"
"c-plus-plus","tag","466295"
"api","tag","475523"
"design","tag","475523"
"performance","tag","475523"
"munin","tag","525488"
"munin20","tag","525488"
"munin","tag","525782"
"munin20","tag","525782"
"munin","tag","525792"
"munin20","tag","525792"
"design","tag","527747"
"performance","tag","527747"
"perl","tag","527747"
"munin","tag","526512"
"munin20","tag","526512"
"performance","tag","526512"
"munin","tag","532312"
"munin20","tag","532312"
"ssh","tag","532312"
"sysadmin","tag","532312"
"http","tag","532312"
"munin","tag","540984"
"munin20","tag","540984"
"sysadmin","tag","540984"
"perl","tag","614219"
"fail","tag","614219"
"rrd","tag","614814"
"munin","tag","614814"
"performance","tag","614814"
"64","tag","631278"
"soldat","tag","631278"
"sysadmin","tag","631278"
"munin","tag","638286"
"tutorial","tag","638286"
"sysadmin","tag","638286"
"munin","tag","717685"
"sysadmin","tag","717685"
"munin","tag","721204"
"packaging","tag","721204"
"rebase","tag","727986"
"git","tag","727986"
"backup","tag","727986"
"munin","tag","729285"
"sysadmin","tag","729285"
"ecosystem","tag","729285"
"c","tag","729310"
"munin","tag","729310"
"design","tag","729310"
"performance","tag","729310"
"sysadmin","tag","729310"
"munin","tag","765452"
"tutorial","tag","765452"
